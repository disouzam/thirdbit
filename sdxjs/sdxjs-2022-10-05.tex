\documentclass[krantzl]{krantz}

% Language rules.
\usepackage[english]{babel}

% Font encoding.
\usepackage[T1]{fontenc}

% Disable ligatures. https://tex.stackexchange.com/a/36702
\usepackage{microtype}
\DisableLigatures{}

% Bibliography.
\usepackage[backend=biber,style=alphabetic,sorting=nyt,maxbibnames=99]{biblatex}
\addbibresource{bibliography.bib}

% https://tex.stackexchange.com/questions/8428/use-bibtex-key-as-the-cite-key
\DeclareFieldFormat{labelalpha}{\thefield{entrykey}}
\DeclareFieldFormat{extraalpha}{}

\setlength{\biblabelsep}{\labelsep}% <-- adjust this to your liking, the standard is 2\labelsep
\defbibenvironment{bibliography}
  {\list
     {\printtext[labelalphawidth]{%
        \printfield{prefixnumber}%
        \printfield{labelalpha}%
        \printfield{extraalpha}}}
     {\setlength{\labelsep}{\biblabelsep}%
      \setlength{\leftmargin}{24pt}%\labelsep
      \setlength{\itemsep}{\bibitemsep}%
      \setlength{\parsep}{\bibparsep}}%
      \renewcommand*{\makelabel}[1]{\bf##1\hss}}
  {\endlist}
  {\item}
\bibparsep3pt

% Include the bibliography in the table of contents.
\usepackage[nottoc,numbib]{tocbibind}

% Build an index.
\usepackage{makeidx}
\makeindex

% Show page boundaries.
% \usepackage[showframe]{geometry}
% \usepackage{geometry}
\usepackage{textcomp}

% Some special symbols.
\usepackage{amssymb}
\usepackage{textgreek}

% Format code listings.
% https://tex.stackexchange.com/questions/263032/why-is-listings-frame-width-a-little-larger-then-textwidth
\usepackage{listings}
\lstset{
  basicstyle=\fontsize{8}{10}\ttfamily,
  upquote=true,
  xleftmargin=3.4pt,
  xrightmargin=3.4pt,
  extendedchars=true,
  literate={✓}{{\checkmark}}1%
  {ô}{{\^o}}1%
  {ü}{{\"u}}1%
  {…}{{\ldots}}1%
  {\ }{\ }1%Non-breaking space, unicode c2a0
  {┐}{\textSFiii}1%
  {└}{\textSFii}1%
  {┴}{\textSFvii}1%
  {┬}{\textSFvi}1%
  {├}{\textSFviii}1%
  {─}{\textSFx}1%
  {│}{\textSFxi}1%
  {┼}{\textSFv}1%
}

% Mark keystrokes.
\usepackage{keystroke}

% Include images.
\usepackage{graphicx}

% Make description items cross-referenceable
\def\namedlabel#1#2{\begingroup
    #2%
    \def\@currentlabel{#2}%
    \phantomsection\label{#1}\endgroup
}
\makeatother

% Use numbers for nested lists all the way down and adjust indent.
\usepackage{enumitem}
\setlist[enumerate,1]{label=\arabic*., ref=\arabic*, leftmargin=*}
\setlist[enumerate,2]{label=\arabic*., ref=\arabic*, leftmargin=*}
\setlist[enumerate,3]{label=\arabic*., ref=\arabic*, leftmargin=*}

% Adjust indent for bullet lists.
\setlist[itemize,1]{label=\textbullet, leftmargin=*}
\setlist[itemize,2]{label=\textbullet, leftmargin=*}
\setlist[itemize,3]{label=\textbullet, leftmargin=*}

% Allow multi-page tables.
\usepackage{longtable}

% Center captions.
\usepackage[center]{caption}

% Mark table headers.
\newcommand{\tablehead}[1]{\underline{#1}}

% Figures.
\newcommand{\figimg}[4]{\begin{figure}%
\centering%
\includegraphics[width=\textwidth]{#2}%
\caption{#3}%
\label{#1}%
\end{figure}}

\newcommand{\figpdf}[4]{\begin{figure}%
\centering%
\includegraphics[scale={#4}]{#2}%
\caption{#3}%
\label{#1}%
\end{figure}}

\newcommand{\figpdfhere}[4]{\begin{figure}[h]%
\centering%
\includegraphics[scale={#4}]{#2}%
\caption{#3}%
\label{#1}%
\end{figure}}

% Cross-references.
\newcommand{\appref}[1]{Appendix~\ref{#1}}
\newcommand{\chapref}[1]{Chapter~\ref{#1}}
\newcommand{\figref}[1]{Figure~\ref{#1}}
\newcommand{\secref}[1]{Section~\ref{#1}}
\newcommand{\tblref}[1]{Table~\ref{#1}}

% Glossary items and references.
\newcommand{\glossref}[1]{\textbf{#1}}
\newcommand{\glosskey}[1]{\textbf{#1}}

% Asides
\usepackage[framemethod=default]{mdframed}
\usepackage{footnote}

% Temp box
\usepackage{xcolor}
\newmdenv[skipabove=7pt,
skipbelow=7pt,
rightline=true,
leftline=true,
topline=true,
bottomline=true,
innerleftmargin=-5pt,
innerrightmargin=-5pt,
innertopmargin=-5pt,
leftmargin=0cm,
rightmargin=0cm,
linewidth=.5pt,
innerbottommargin=0pt,
backgroundcolor=black!5]{tBox}

\newtoggle{inbox}
\togglefalse{inbox}

\pretocmd{\footnote}{\iftoggle{inbox}{\stepcounter{footnote}}{\relax}}{}{}

\newcommand{\callouttitle}[1]{\begin{center}{#1}\end{center}\vspace{\baselineskip}}
\newenvironment{callout}{\savenotes\begin{tBox}\begin{quotation}\toggletrue{inbox}\renewcommand{\thempfootnote}{\arabic{footnote}}}{\end{quotation}\vspace{\baselineskip}\end{tBox}\togglefalse{inbox}\spewnotes}
\newenvironment{hint}{\begin{mdframed}\begin{quotation}}{\end{quotation}\end{mdframed}}
\newlength{\tempindent}
\newenvironment{unindented}{%
  \setlength{\tempindent}{\parindent}%
  \setlength{\parindent}{0pt}%
}{%
  \setlength{\parindent}{\tempindent}%
}

% Unicode characters.
\usepackage[utf8]{inputenc}
\usepackage{pmboxdraw}
\usepackage{newunicodechar}
\newunicodechar{√}{$\sqrt{}$}
\newunicodechar{✓}{\checkmark}

% Don't indent footnotes.
\usepackage[hang,flushmargin,bottom]{footmisc}

% URLs as footnotes.
% Always load 'hyperref' last (see link below for explanation).
% https://tex.stackexchange.com/questions/16268/warning-with-footnotes-namehfootnote-xx-has-been-referenced-but-does-not-exi
\newcommand{\hreffoot}[2]{{#1}\footnote{\href{#2}{#2}}}
\usepackage[hidelinks]{hyperref}

% Adapted from Nemilov.cls.
\def\dedication#1{\thispagestyle{empty}\par\vspace*{9pc}\hfil{\large \textbf{\emph{Dedication}}}\hfil\par\vspace*{9pt}%
\hfil {\vrule height.5pt width 7pc}\hfil\par\vspace*{16pt}%
\vbox{\centering {#1}}}

\begin{document}

\include{config}
\maketitle

% Start numbering pages at 5 as per editor's request.
\frontmatter\setcounter{page}{5}

\input{dedication}

\tableofcontents

\mainmatter

\chapter{Introduction}\label{introduction}


The best way to learn design is to study examples \cite{Schon1984,Petre2016},
and some of the best examples of software design come from
the tools programmers use in their own work.
In these lessons we build small versions of things like file backup systems,
testing frameworks,
regular expression matchers,
and browser layout engines
both to demystify them
and to give some insights into how experienced programmers think.
We draw inspiration from \cite{Brown2011,Brown2012,Brown2016},
\hreffoot{Mary Rose Cook’s}{https://maryrosecook.com/} \hreffoot{Gitlet}{http://gitlet.maryrosecook.com/},
and the books that introduced the Unix philosophy to an entire generation of programmers
\cite{Kernighan1979,Kernighan1981,Kernighan1983,Kernighan1988}.


All of the written material in this project can be freely reused
under the terms of the \hreffoot{Creative Commons - Attribution - NonCommercial license}{https://creativecommons.org/licenses/by-nc/4.0/},
while all of the software is made available under the terms of
the \hreffoot{Hippocratic License}{https://firstdonoharm.dev/}.
All proceeds from this project will go to support the \hreffoot{Red Door Family Shelter}{https://www.reddoorshelter.ca/}.

\section{Who is our audience?}\label{introduction-audience}


Every lesson should be written with specific learners in mind.
These three \hreffoot{personas}{https://teachtogether.tech/en/index.html\#s:process-personas} describe ours:

\begin{itemize}

\item 

Aïsha started writing VB macros for Excel in an accounting course and never looked back.
    After spending three years doing front-end JavaScript work
    she now wants to learn how to build back-end applications.
    This material will fill in some gaps in her programming knowledge
    and teach her some common design patterns.



\item 

Rupinder is studying computer science at college.
    He has learned a lot about the theory of algorithms,
    and while he uses Git and unit testing tools in his assignments,
    he doesn’t feel he understands how they work.
    This material will give him a better understanding of those tools
    and of how to design new ones.



\item 

Yim builds mobile apps for a living
    but also teaches two college courses:
    one on full-stack web development using JavaScript and Node
    and another titled “Software Design”.
    They are happy with the former,
    but frustrated that so many books about the latter subject talk about it in the abstract
    and use examples that their students can’t relate to.
    This material will fill those gaps
    and give them starting points for a wide variety of course assignments.



\end{itemize}


Like these three personas, readers should be able to:

\begin{itemize}

\item 

Write JavaScript programs using loops, arrays, functions, and classes.



\item 

Create static web pages using HTML and CSS.



\item 

Install Node on their computer
    and run programs with it from the command line.



\item 

Use \hreffoot{Git}{https://git-scm.com/} to save and share files.
    (It’s OK not to know \hreffoot{the more obscure commands}{https://git-man-page-generator.lokaltog.net/}.)



\item 

Explain what a tree is and how to process one recursively.
    (This is the most complicated data structure and algorithm we \emph{don’t} explain.)



\end{itemize}


This book can be read on its own or used as a classroom resource.
If you are looking for a project to do in a software design course,
adding a tool to those covered here would be fun as well as educational.
Please see \chapref{conclusion} for more details.

\section{What tools and ideas do we cover?}\label{introduction-contents}


Programmers have invented \hreffoot{a lot of tools}{https://en.wikipedia.org/wiki/Programming\_tool} to make their lives easier.
This volume focuses on a few that individual developers use while writing software;
we hope future volumes
will explore those used in the applications that programmers build.


\appref{glossary} defines the terms we introduce in these lessons,
which in turn define their scope:

\begin{itemize}

\item 

How to process a program like any other piece of text.



\item 

How to turn a program into a data structure that can be analyzed and modified.



\item 

What design patterns are and which ones are used most often.



\item 

How programs are executed and how we can control and inspect their execution.



\item 

How we can analyze programs’ performance in order to make sensible design tradeoffs.



\item 

How to find and run code modules on the fly.



\end{itemize}


\newpage

\section{How are these lessons laid out?}\label{introduction-layout}


We display JavaScript source code like this:


\begin{lstlisting}[frame=tblr]
for (const thing in collection) {
  console.log(thing)
}
\end{lstlisting}



\noindent Unix shell commands are shown like this:


\begin{lstlisting}[frame=shadowbox]
for filename in *.dat
do
    cut -d , -f 10 $filename
done
\end{lstlisting}



\noindent and data and output like this:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
Package,Releases
0,1
0-0,0
0-0-1,1
00print-lol,2
00smalinux,0
01changer,0
\end{lstlisting}



We occasionally wrap lines in source code in unnatural ways to make listings fit the printed page,
and sometimes use \texttt{...} to show where lines have been omitted.
Where we need to break lines of output for the same reason,
we end all but the last line with a single backslash \texttt{{\textbackslash}}.
The full listings are all available in \hreffoot{our Git repository}{https://github.com/gvwilson/sdxjs/}
and \hreffoot{on our website}{https://stjs.tech/}.


Finally,
we write functions as \texttt{functionName} rather than \texttt{functionName()};
the latter is more common,
but people don’t use \texttt{objectName\{\}} for objects or \texttt{arrayName[]} for arrays,
and the empty parentheses makes it hard to tell
whether we’re talking about “the function itself” or “a call to the function with no parameters”.

\section{How did we get here?}\label{introduction-history}


In the early 2000s,
the University of Toronto\index{University of Toronto} asked \hreffoot{Greg Wilson}{https://third-bit.com/}\index{Wilson, Greg}
to teach an undergraduate course on software architecture.
After delivering the course three times he told the university they should cancel it:
between them,
the dozen textbooks he had purchased with the phrase “software architecture” in their titles
devoted a total of less than 30 pages to describing the designs of actual systems.


\newpage


Frustrated by that,
he and \hreffoot{Andy Oram}{http://www.praxagora.com/}\index{Oram, Andy} persuaded some well-known programmers to contribute a chapter each
to a collection called \emph{Beautiful Code} \cite{Oram2007},
which went on to win the Jolt Award in 2007.
Entries in the book described everything from figuring out whether three points are on a line
to core components of Linux
and the software for the Mars Rover,
but the breadth that made them fun to read
also meant they weren’t particularly useful for teaching.


To fix that,
Greg Wilson, \hreffoot{Amy Brown}{https://www.amyrhodabrown.com/}\index{Brown, Amy},
\hreffoot{Tavish Armstrong}{http://tavisharmstrong.com/}\index{Armstrong, Tavish},
and \hreffoot{Mike DiBernardo}{https://mikedebo.com/}\index{DiBernardo, Mike}
edited a four-book series between 2011 and 2016 called \emph{\hreffoot{The Architecture of Open Source Applications}{https://aosabook.org/}}.
In the first two volumes,
the creators of fifty open source projects described their systems’ designs;
the third book explored the performance of those systems,
while in the fourth volume contributors built scale models of common tools
as a way of demonstrating how those tools worked.
These books were closer to what an instructor would need for an undergraduate class on software design,
but still not quite right:
the intended audience would probably not be familiar with many of the problem domains,
and since each author used the programming language of their choice,
much of the code would be hard to understand.


\emph{Software Tools in JavaScript} is meant to address these shortcomings:
all of the code is written in one language,
and the examples are all tools that programmers use daily.
Most of the programs are less than 60 lines long and the longest is less than 200;
we believe each chapter can be covered in class in 1-2 hours,
while the exercises range in difficulty from a few minutes to a couple of days.

\section{How can people use and contribute to this material?}\label{introduction-use}


All of the written material on this site is made available under the Creative
Commons - Attribution - NonCommercial 4.0 International license (CC-BY-NC-4.0),
while the software is made available under the Hippocratic License.  The first
allows you to use and remix this material for non-commercial purposes, as-is or
in adapted form, provided you cite its original source; the second allows you to
use and remix the software on this site provided you do not violate
international agreements governing human rights. Please see \appref{license}
for details.


If you would like to improve what we have or add new material, please see the
Code of Conduct in \appref{conduct} and the contributor guidelines in
\appref{contributing}.  If you have questions or would like to use this material in
a course, please file an issue in \hreffoot{our GitHub repository}{https://github.com/gvwilson/sdxjs/} or send us email.

\section{Who helped us?}\label{introduction-help}


I am grateful to the creators of \hreffoot{diagrams.net}{https://www.diagrams.net/},
\hreffoot{Emacs}{https://www.gnu.org/software/emacs/},
\hreffoot{ESLint}{https://eslint.org/},
\hreffoot{Glosario}{https://github.com/carpentries/glosario},
\hreffoot{GNU Make}{https://www.gnu.org/software/make/},
\hreffoot{LaTeX}{https://www.latex-project.org/},
\hreffoot{Node}{https://nodejs.org/en/},
\hreffoot{NPM}{https://www.npmjs.com/},
\hreffoot{Standard JS}{https://standardjs.com/},
\hreffoot{SVG Screenshot}{https://chrome.google.com/webstore/detail/svg-screenshot/nfakpcpmhhilkdpphcjgnokknpbpdllg},
\hreffoot{WAVE}{https://wave.webaim.org/},
and all the other open source tools used in creating these lessons:
if we all give a little,
we all get a lot.
I would also like to thank Darren McElligott, Evan Schultz, and Juanan Pereira
for their feedback;
any errors, omissions, or misunderstandings that remain are entirely my fault.

\chapter{Systems Programming}\label{systems-programming}


\noindent 
  Terms defined: \glossref{anonymous function}, \glossref{asynchronous}, \glossref{Boolean}, \glossref{callback function}, \glossref{cognitive load}, \glossref{command-line argument}, \glossref{console}, \glossref{current working directory}, \glossref{destructuring assignment}, \glossref{edge case}, \glossref{filename extension}, \glossref{filesystem}, \glossref{filter}, \glossref{globbing}, \glossref{idiomatic}, \glossref{log message}, \glossref{path (in filesystem)}, \glossref{protocol}, \glossref{scope}, \glossref{single-threaded}, \glossref{string interpolation}



The biggest difference between JavaScript and most other programming languages
is that many operations in JavaScript are \glossref{asynchronous}\index{asynchronous execution}\index{execution!asynchronous}.
Its designers didn’t want browsers to freeze while waiting for data to arrive or for users to click on things,
so operations that might be slow are implemented by describing now what to do later.
And since anything that touches the hard drive is slow from a processor’s point of view,
\hreffoot{Node}{https://nodejs.org/en/} implements \glossref{filesystem}\index{filesystem operations} operations the same way.

\begin{callout}


\subsubsection*{How slow is slow?}


\cite{Gregg2020} used the analogy in \tblref{systems-programming-times}
to show how long it takes a computer to do different things
if we imagine that one CPU cycle is equivalent to one second.

\end{callout}

\begin{table}
\begin{tabular}{lll}
\textbf{\underline{Operation}} & \textbf{\underline{Actual Time}} & \textbf{\underline{Would Be...}} \\
1 CPU cycle & 0.3 nsec & 1 sec \\
Main memory access & 120 nsec & 6 min \\
Solid-state disk I/O & 50-150 {\textmu}sec & 2-6 days \\
Rotational disk I/O & 1-10 msec & 1-12 months \\
Internet: San Francisco to New York & 40 msec & 4 years \\
Internet: San Francisco to Australia & 183 msec & 19 years \\
Physical system reboot & 5 min & 32,000 years \\
\end{tabular}
\caption{Computer operation times at human scale.}
\label{systems-programming-times}
\end{table}



Early JavaScript programs used \glossref{callback functions}\index{callback function} to describe asynchronous operations,
but as we’re about to see,
callbacks can be hard to understand even in small programs.
In 2015,
the language’s developers standardized a higher-level tool called promises
to make callbacks easier to manage,
and more recently they have added new keywords called \texttt{async} and \texttt{await} to make it easier still.
We need to understand all three layers in order to debug things when they go wrong,
so this chapter explores callbacks,
while \chapref{async-programming} shows how promises and \texttt{async}/\texttt{await} work.
This chapter also shows how to read and write files and directories with Node’s standard libraries,
because we’re going to be doing that a lot.


\newpage

\section{How can we list a directory?}\label{systems-programming-ls}


To start,
let’s try listing the contents of a directory the way we would in \hreffoot{Python}{https://www.python.org/}\index{Python}
or \hreffoot{Java}{https://en.wikipedia.org/wiki/Java\_(programming\_language)}\index{Java}:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'

const srcDir = process.argv[2]
const results = fs.readdir(srcDir)
for (const name of results) {
  console.log(name)
}
\end{lstlisting}



\noindent We use \texttt{import \emph{module} from ‘source’}\index{import module} to load the library \texttt{\emph{source}}
and assign its contents to \texttt{\emph{module}}.
After that,
we can refer to things in the library using \texttt{\emph{module.component}}
just as we refer to things in any other object.
We can use whatever name we want for the module,
which allows us to give short nicknames to libraries with long names;
we will take advantage of this in future chapters.

\begin{callout}


\subsubsection*{\texttt{require} versus \texttt{import}}


In 2015, a new version of JavaScript called ES6 introduced
the keyword \texttt{import}\index{import vs. require}\index{require vs. import} for importing modules.
It improves on the older \texttt{require} function in several ways,
but Node still uses \texttt{require} by default.
To tell it to use \texttt{import},
we have added \texttt{"type": "module"} at the top level of our Node \texttt{package.json} file.

\end{callout}


Our little program uses the \hreffoot{\texttt{fs}}{https://nodejs.org/api/fs.html} library
which contains functions to create directories, read or delete files, etc.
(Its name is short for “filesystem”.)
We tell the program what to list using \glossref{command-line arguments}\index{command-line argument},
which Node automatically stores in an array called \texttt{process.argv}\index{process.argv}.
The name of the program used to run our code is stored \texttt{process.argv[0]} (which in this case is \texttt{node}),
while \texttt{process.argv[1]} is the name of our program (in this case \texttt{list-dir-wrong.js}).
The rest of \texttt{process.argv} holds whatever arguments we gave at the command line when we ran the program,
so \texttt{process.argv[2]} is the first argument after the name of our program (\figref{systems-programming-process-argv}).

\figpdf{systems-programming-process-argv}{./systems-programming/process-argv.pdf}{How Node stores command-line arguments in \texttt{process.argv}.}{0.6}


\newpage


If we run this program with the name of a directory as its argument,
\texttt{fs.readdir} returns the names of the things in that directory as an array of strings.
The program uses \texttt{for (const name of results)} to loop over the contents of that array.
We could use \texttt{let} instead of \texttt{const},
but it’s good practice to declare things as \texttt{const}\index{const declaration!advantages of} wherever possible
so that anyone reading the program knows the variable isn’t actually going to vary—doing
this reduces the \glossref{cognitive load}\index{cognitive load} on people reading the program.
Finally,
\texttt{console.log}\index{console.log} is JavaScript’s equivalent of other languages’ \texttt{print} command;
its strange name comes from the fact that
its original purpose was to create \glossref{log messages} in the browser \glossref{console}.


Unfortunately,
our program doesn’t work:


\begin{lstlisting}[frame=shadowbox]
node list-dir-wrong.js .
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
node:internal/process/esm_loader:74
    internalBinding('errors').triggerUncaughtException(
                              ^

TypeError [ERR_INVALID_CALLBACK]: Callback must be a function. Received \
undefined
    at makeCallback (node:fs:181:3)
    at Object.readdir (node:fs:1030:14)
    at /u/stjs/systems-programming/list-dir-wrong.js:4:20
    at ModuleJob.run (node:internal/modules/esm/module_job:154:23)
    at async Loader.import (node:internal/modules/esm/loader:177:24)
    at async Object.loadESM (node:internal/process/esm_loader:68:5) {
  code: 'ERR_INVALID_CALLBACK'
}
\end{lstlisting}



\noindent The error message comes from something we didn’t write whose source we would struggle to read.
If we look for the name of our file (\texttt{list-dir-wrong.js})
we see the error occurred on line 4;
everything above that is inside \texttt{fs.readdir},
while everything below it is Node loading and running our program.


The problem is that \texttt{fs.readdir} doesn’t return anything.
Instead,
its documentation says that it needs a callback function
that tells it what to do when data is available,
so we need to explore those in order to make our program work.

\begin{callout}


\subsubsection*{A theorem}

\begin{enumerate}

\item Every program contains at least one bug.

\item Every program can be made one line shorter.

\item Therefore, every program can be reduced to a single statement which is wrong.

\end{enumerate}


\noindent — variously attributed

\end{callout}

\section{What is a callback function?}\label{systems-programming-callback}


JavaScript uses a \glossref{single-threaded}\index{single-threaded execution}\index{execution!single-threaded} programming model:
as the introduction to this lesson said,
it splits operations like file I/O into “please do this” and “do this when data is available”.
\texttt{fs.readdir} is the first part,
but we need to write a function that specifies the second part.


\newpage


JavaScript saves a reference to this function
and calls with a specific set of parameters when our data is ready
(\figref{systems-programming-callbacks}).
Those parameters defined a standard \glossref{protocol}\index{protocol!API as}\index{API!as protocol}
for connecting to libraries,
just like the USB standard allows us to plug hardware devices together.

\figpdf{systems-programming-callbacks}{./systems-programming/callbacks.pdf}{How JavaScript runs callback functions.}{0.6}


This corrected program gives \texttt{fs.readdir} a callback function called \texttt{listContents}:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'

const listContents = (err, files) => {
  console.log('running callback')
  if (err) {
    console.error(err)
  } else {
    for (const name of files) {
      console.log(name)
    }
  }
}

const srcDir = process.argv[2]
fs.readdir(srcDir, listContents)
console.log('last line of program')
\end{lstlisting}



\noindent Node callbacks\index{callback function!conventions for}
always get an error (if there is any) as their first argument
and the result of a successful function call as their second.
The function can tell the difference by checking to see if the error argument is \texttt{null}.
If it is, the function lists the directory’s contents with \texttt{console.log},
otherwise, it uses \texttt{console.error} to display the error message.
Let’s run the program with the \glossref{current working directory}
(written as ‘.’)
as an argument:


\begin{lstlisting}[frame=shadowbox]
node list-dir-function-defined.js .
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
last line of program
running callback
Makefile
copy-file-filtered.js
copy-file-unfiltered.js
...
x-trace-anonymous
x-trace-anonymous.md
x-trace-callback
x-trace-callback.md
x-where-is-node.md
\end{lstlisting}



Nothing that follows will make sense if we don’t understand
the order in which Node executes the statements in this program
(\figref{systems-programming-execution-order}):

\begin{enumerate}

\item 

Execute the first line to load the \texttt{fs} library.



\item 

Define a function of two parameters and assign it to \texttt{listContents}.
    (Remember, a function is just another kind of data.)



\item 

Get the name of the directory from the command-line arguments.



\item 

Call \texttt{fs.readdir} to start a filesystem operation,
    telling it what directory we want to read and what function to call when data is available.



\item 

Print a message to show we’re at the end of the file.



\item 

Wait until the filesystem operation finishes (this step is invisible).



\item 

Run the callback function, which prints the directory listing.



\end{enumerate}

\figpdf{systems-programming-execution-order}{./systems-programming/execution-order.pdf}{When JavaScript runs callback functions.}{0.6}

\section{What are anonymous functions?}\label{systems-programming-anonymous}


Most JavaScript programmers wouldn’t define the function \texttt{listContents}
and then pass it as a callback.
Instead,
since the callback is only used in one place,
it is more \glossref{idiomatic}
to define it where it is needed
as an \glossref{anonymous function}\index{anonymous function}\index{function!anonymous}.
This makes it easier to see what’s going to happen when the operation completes,
though it means the order of execution is quite different from the order of reading
(\figref{systems-programming-anonymous-functions}).
Using an anonymous function gives us the final version of our program:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'

const srcDir = process.argv[2]
fs.readdir(srcDir, (err, files) => {
  if (err) {
    console.error(err)
  } else {
    for (const name of files) {
      console.log(name)
    }
  }
})
\end{lstlisting}


\figpdf{systems-programming-anonymous-functions}{./systems-programming/anonymous-functions.pdf}{How and when JavaScript creates and runs anonymous callback functions.}{0.6}

\begin{callout}


\subsubsection*{Functions are data}


As we noted above,
a function is just another kind of data\index{code!as data}.
Instead of being made up of numbers, characters, or pixels, it is made up of instructions,
but these are stored in memory like anything else.
Defining a function on the fly is no different from defining an array in-place using \texttt{[1, 3, 5]},
and passing a function as an argument to another function is no different from passing an array.
We are going to rely on this insight over and over again in the coming lessons.

\end{callout}

\section{How can we select a set of files?}\label{systems-programming-fileset}


Suppose we want to copy some files instead of listing a directory’s contents.
Depending on the situation
we might want to copy only those files given on the command line
or all files except some explicitly excluded.
What we \emph{don’t} want to have to do is list the files one by one;
instead,
we want to be able to write patterns like \texttt{*.js}.


To find files that match patterns like that,
we can use the \hreffoot{\texttt{glob}}{https://www.npmjs.com/package/glob} module.
(To \glossref{glob}\index{globbing} (short for “global”) is an old Unix term for matching a set of files by name.)
The \texttt{glob} module provides a function that takes a pattern and a callback
and does something with every filename that matched the pattern:


\begin{lstlisting}[frame=tblr]
import glob from 'glob'

glob('**/*.*', (err, files) => {
  if (err) {
    console.log(err)
  } else {
    for (const filename of files) {
      console.log(filename)
    }
  }
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
copy-file-filtered.js
copy-file-filtered.js.bck
copy-file-unfiltered.js
copy-file-unfiltered.js.bck
copy-file-unfiltered.out
...
x-trace-anonymous.md
x-trace-anonymous/trace.js
x-trace-callback.md
x-trace-callback/trace.js
x-where-is-node.md
\end{lstlisting}



The leading \texttt{**} means “recurse into subdirectories”,
while \texttt{*.*} means “any characters followed by ‘.’ followed by any characters”
(\figref{systems-programming-globbing}).
Names that don’t match \texttt{*.*} won’t be included,
and by default,
neither are names that start with a ‘.’ character.
This is another old Unix convention:
files and directories whose names have a leading ‘.’
usually contain configuration information for various programs,
so most commands will leave them alone unless told to do otherwise.

\figpdf{systems-programming-globbing}{./systems-programming/globbing.pdf}{Using \texttt{glob} patterns to match filenames.}{0.6}


This program works,
but we probably don’t want to copy editor backup files whose names end with \texttt{.bck}.
We can get rid of them by \glossref{filtering}\index{globbing!filtering results} the list that \texttt{glob} returns:


\newpage


\begin{lstlisting}[frame=tblr]
import glob from 'glob'

glob('**/*.*', (err, files) => {
  if (err) {
    console.log(err)
  } else {
    files = files.filter((f) => { return !f.endsWith('.bck') })
    for (const filename of files) {
      console.log(filename)
    }
  }
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
copy-file-filtered.js
copy-file-unfiltered.js
copy-file-unfiltered.out
copy-file-unfiltered.sh
copy-file-unfiltered.txt
...
x-trace-anonymous.md
x-trace-anonymous/trace.js
x-trace-callback.md
x-trace-callback/trace.js
x-where-is-node.md
\end{lstlisting}



\texttt{Array.filter}\index{Array.filter} creates a new array
containing all the items of the original array that pass a test
(\figref{systems-programming-array-filter}).
The test is specified as a callback function
that \texttt{Array.filter} calls once once for each item.
This function must return a \glossref{Boolean}
that tells \texttt{Array.filter} whether to keep the item in the new array or not.
\texttt{Array.filter} does not modify the original array,
so we can filter our original list of filenames several times if we want to.

\figpdf{systems-programming-array-filter}{./systems-programming/array-filter.pdf}{Selecting array elements using \texttt{Array.filter}.}{0.6}


We can make our globbing program more idiomatic by
removing the parentheses around the single parameter
and writing just the expression we want the function to return:


\begin{lstlisting}[frame=tblr]
import glob from 'glob'

glob('**/*.*', (err, files) => {
  if (err) {
    console.log(err)
  } else {
    files = files.filter(f => !f.endsWith('.bck'))
    for (const filename of files) {
      console.log(filename)
    }
  }
})
\end{lstlisting}



However,
it turns out that \texttt{glob} will filter for us.
According to its documentation,
the function takes an \texttt{options} object full of key-value settings
that control its behavior.
This is another common pattern in Node libraries:
rather than accepting a large number of rarely-used parameters,
a function can take a single object full of settings.


If we use this,
our program becomes:


\begin{lstlisting}[frame=tblr]
import glob from 'glob'

glob('**/*.*', { ignore: '*.bck' }, (err, files) => {
  if (err) {
    console.log(err)
  } else {
    for (const filename of files) {
      console.log(filename)
    }
  }
})
\end{lstlisting}



\noindent Notice that we don’t quote the key in the \texttt{options} object.
The keys in objects are almost always strings,
and if a string is simple enough that it won’t confuse the parser,
we don’t need to put quotes around it.
Here,
“simple enough” means “looks like it could be a variable name”,
or equivalently “contains only letters, digits, and the underscore”.

\begin{callout}


\subsubsection*{No one knows everything}


We combined \texttt{glob.glob} and \texttt{Array.filter} in our functions for more than a year
before someone pointed out the \texttt{ignore} option for \texttt{glob.glob}.
This shows:

\begin{enumerate}

\item 

Life is short,
    so most of us find a way to solve the problem in front of us
    and re-use it rather than looking for something better.



\item 

Code reviews aren’t just about finding bugs:
    they are also the most effective way to transfer knowledge between programmers.
    Even if someone is much more experienced than you,
    there’s a good chance you might have stumbled over a better way to do something
    than the one they’re using (see point \#1 above).



\end{enumerate}

\end{callout}


To finish off our globbing program,
let’s specify a source directory on the command line and include that in the pattern:


\begin{lstlisting}[frame=tblr]
import glob from 'glob'

const srcDir = process.argv[2]

glob(`${srcDir}/**/*.*`, { ignore: '*.bck' }, (err, files) => {
  if (err) {
    console.log(err)
  } else {
    for (const filename of files) {
      console.log(filename)
    }
  }
})
\end{lstlisting}



\noindent This program uses \glossref{string interpolation}\index{string interpolation}
to insert the value of \texttt{srcDir} into a string.
The template string is written in back quotes,
and JavaScript converts every expression written as \texttt{\$\{expression\}} to text.
We could create the pattern by concatenating strings using
\texttt{srcDir + {\textquotesingle}/**/*.*{\textquotesingle}},
but most programmers find interpolation easier to read.

\section{How can we copy a set of files?}\label{systems-programming-copy}


If we want to copy a set of files instead of just listing them
we need a way to create the \glossref{paths} of the files we are going to create.
If our program takes a second argument that specifies the desired output directory,
we can construct the full output path by replacing the name of the source directory with that path:


\begin{lstlisting}[frame=tblr]
import glob from 'glob'

const [srcDir, dstDir] = process.argv.slice(2)

glob(`${srcDir}/**/*.*`, { ignore: '*.bck' }, (err, files) => {
  if (err) {
    console.log(err)
  } else {
    for (const srcName of files) {
      const dstName = srcName.replace(srcDir, dstDir)
      console.log(srcName, dstName)
    }
  }
})
\end{lstlisting}



\noindent This program uses \glossref{destructuring assignment}\index{destructuring assignment}\index{assignment!destructuring}
to create two variables at once
by unpacking the elements of an array
(\figref{systems-programming-destructuring-assignment}).
It only works if the array contains the enough elements,
i.e.,
if both a source and destination are given on the command line;
we’ll add a check for that in the exercises.

\figpdf{systems-programming-destructuring-assignment}{./systems-programming/destructuring-assignment.pdf}{Assigning many values at once by destructuring.}{0.6}


A more serious problem is that
this program only works if the destination directory already exists:
\texttt{fs} and equivalent libraries in other languages usually won’t create directories for us automatically.
The need to do this comes up so often that there is a function called \texttt{ensureDir} to do it:


\begin{lstlisting}[frame=tblr]
import glob from 'glob'
import fs from 'fs-extra'
import path from 'path'

const [srcRoot, dstRoot] = process.argv.slice(2)

glob(`${srcRoot}/**/*.*`, { ignore: '*.bck' }, (err, files) => {
  if (err) {
    console.log(err)
  } else {
    for (const srcName of files) {
      const dstName = srcName.replace(srcRoot, dstRoot)
      const dstDir = path.dirname(dstName)
      fs.ensureDir(dstDir, (err) => {
        if (err) {
          console.error(err)
        }
      })
    }
  }
})
\end{lstlisting}



Notice that we import from \texttt{fs-extra} instead of \texttt{fs};
the \hreffoot{\texttt{fs-extra}}{https://www.npmjs.com/package/fs-extra} module provides some useful utilities on top of \texttt{fs}.
We also use \hreffoot{\texttt{path}}{https://nodejs.org/api/path.html} to manipulate pathnames
rather than concatenating or interpolating strings
because there are a lot of tricky \glossref{edge cases} in pathnames
that the authors of that module have figured out for us.

\begin{callout}


\subsubsection*{Using distinct names}


We are now calling our command-line arguments \texttt{srcRoot} and \texttt{dstRoot}
rather than \texttt{srcDir} and \texttt{dstDir}.
We originally used \texttt{dstDir} as both
the name of the top-level destination directory (from the command line)
and the name of the particular output directory to create.
This was legal,
since every function creates
a new \glossref{scope}\index{scope!of variable definitions}\index{variable definition!scope},
but hard for people to understand.

\end{callout}


Our file copying program currently creates empty destination directories
but doesn’t actually copy any files.
Let’s use \texttt{fs.copy} to do that:


\begin{lstlisting}[frame=tblr]
import glob from 'glob'
import fs from 'fs-extra'
import path from 'path'

const [srcRoot, dstRoot] = process.argv.slice(2)

glob(`${srcRoot}/**/*.*`, { ignore: '*.bck' }, (err, files) => {
  if (err) {
    console.log(err)
  } else {
    for (const srcName of files) {
      const dstName = srcName.replace(srcRoot, dstRoot)
      const dstDir = path.dirname(dstName)
      fs.ensureDir(dstDir, (err) => {
        if (err) {
          console.error(err)
        } else {
          fs.copy(srcName, dstName, (err) => {
            if (err) {
              console.error(err)
            }
          })
        }
      })
    }
  }
})
\end{lstlisting}



The program now has three levels of callback
(\figref{systems-programming-triple-callback}):

\begin{enumerate}

\item 

When \texttt{glob} has data, do things and then call \texttt{ensureDir}.



\item 

When \texttt{ensureDir} completes, copy a file.



\item 

When \texttt{copy} finishes, check the error status.



\end{enumerate}

\figpdf{systems-programming-triple-callback}{./systems-programming/triple-callback.pdf}{Three levels of callback in the running example.}{0.6}


Our program looks like it should work,
but if we try to copy everything in the directory containing these lessons
we get an error message:


\begin{lstlisting}[frame=shadowbox]
rm -rf /tmp/out
mkdir /tmp/out
node copy-file-unfiltered.js ../node_modules /tmp/out 2>&1 | head -n 6
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[Error: ENOENT: no such file or directory, chmod \
'/tmp/out/@nodelib/fs.stat/README.md'] {
  errno: -2,
  code: 'ENOENT',
  syscall: 'chmod',
  path: '/tmp/out/@nodelib/fs.stat/README.md'
}
\end{lstlisting}



The problem is that \texttt{node\_modules/fs.stat} and \texttt{node\_modules/fs.walk} match our globbing expression,
but are directories rather than files.
To prevent our program from trying to use \texttt{fs.copy} on directories,
we must use \texttt{fs.stat} to get the properties of the things \texttt{glob} gives us
and then check if they are files.
The name “stat” is short for “status”,
and since the status of something in the filesystem can be very complex,
\texttt{fs.stat}\index{fs.stat} returns \hreffoot{an object with methods that can answer common questions}{https://nodejs.org/api/fs.html\#fs\_class\_fs\_stats}.


Here’s the final version of our file copying program:


\begin{lstlisting}[frame=tblr]
import glob from 'glob'
import fs from 'fs-extra'
import path from 'path'

const [srcRoot, dstRoot] = process.argv.slice(2)

glob(`${srcRoot}/**/*.*`, { ignore: '*.bck' }, (err, files) => {
  if (err) {
    console.log(err)
  } else {
    for (const srcName of files) {
      fs.stat(srcName, (err, stats) => {
        if (err) {
          console.error(err)
        } else if (stats.isFile()) {
          const dstName = srcName.replace(srcRoot, dstRoot)
          const dstDir = path.dirname(dstName)
          fs.ensureDir(dstDir, (err) => {
            if (err) {
              console.error(err)
            } else {
              fs.copy(srcName, dstName, (err) => {
                if (err) {
                  console.error(err)
                }
              })
            }
          })
        }
      })
    }
  }
})
\end{lstlisting}



\noindent It works,
but four levels of asynchronous callbacks is hard for humans to understand.
\chapref{async-programming} will introduce a pair of tools
that make code like this easier to read.

\section{Exercises}\label{systems-programming-exercises}

\subsection*{Where is Node?}


Write a program called \texttt{wherenode.js} that prints the full path to the version of Node it is run with.

\subsection*{Tracing callbacks}


In what order does the program below print messages?


\begin{lstlisting}[frame=tblr]
const red = () => {
  console.log('RED')
}

const green = (func) => {
  console.log('GREEN')
  func()
}

const blue = (left, right) => {
  console.log('BLUE')
  left(right)
}

blue(green, red)
\end{lstlisting}


\subsection*{Tracing anonymous callbacks}


In what order does the program below print messages?


\begin{lstlisting}[frame=tblr]
const blue = (left, right) => {
  console.log('BLUE')
  left(right)
}

blue(
  (callback) => {
    console.log('GREEN')
    callback()
  },
  () => console.log('RED')
)
\end{lstlisting}


\subsection*{Checking arguments}


Modify the file copying program to check that it has been given the right number of command-line arguments
and to print a sensible error message (including a usage statement) if it hasn’t.

\subsection*{Glob patterns}


What filenames does each of the following glob patterns match?

\begin{itemize}

\item \texttt{results-[0123456789].csv}

\item \texttt{results.(tsv|csv)}

\item \texttt{results.dat?}

\item \texttt{./results.data}

\end{itemize}

\subsection*{Filtering arrays}


Fill in the blank in the code below so that the output matches the one shown.
Note: you can compare strings in JavaScript using \texttt{<}, \texttt{>=}, and other operators,
so that (for example) \texttt{person.personal > {\textquotesingle}P{\textquotesingle}} is \texttt{true}
if someone’s personal name starts with a letter that comes after ‘P’ in the alphabet.


\begin{lstlisting}[frame=tblr]
const people = [
  { personal: 'Jean', family: 'Jennings' },
  { personal: 'Marlyn', family: 'Wescoff' },
  { personal: 'Ruth', family: 'Lichterman' },
  { personal: 'Betty', family: 'Snyder' },
  { personal: 'Frances', family: 'Bilas' },
  { personal: 'Kay', family: 'McNulty' }
]

const result = people.filter(____ => ____)

console.log(result)
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  { personal: 'Jean', family: 'Jennings' },
  { personal: 'Ruth', family: 'Lichterman' },
  { personal: 'Frances', family: 'Bilas' }
]
\end{lstlisting}


\subsection*{String interpolation}


Fill in the code below so that it prints the message shown.


\begin{lstlisting}[frame=tblr]
const people = [
  { personal: 'Christine', family: 'Darden' },
  { personal: 'Mary', family: 'Jackson' },
  { personal: 'Katherine', family: 'Johnson' },
  { personal: 'Dorothy', family: 'Vaughan' }
]

for (const person of people) {
  console.log(`$____, $____`)
}
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
Darden, Christine
Jackson, Mary
Johnson, Katherine
Vaughan, Dorothy
\end{lstlisting}


\subsection*{Destructuring assignment}


What is assigned to each named variable in each statement below?

\begin{enumerate}

\item \texttt{const first = [10, 20, 30]}

\item \texttt{const [first, second] = [10, 20, 30]}

\item \texttt{const [first, second, third] = [10, 20, 30]}

\item \texttt{const [first, second, third, fourth] = [10, 20, 30]}

\item \texttt{const \{left, right\} = \{left: 10, right: 30\}}

\item \texttt{const \{left, middle, right\} = \{left: 10, middle: 20, right: 30\}}

\end{enumerate}

\subsection*{Counting lines}


Write a program called \texttt{lc} that counts and reports the number of lines in one or more files and the total number of lines,
so that \texttt{lc a.txt b.txt} displays something like:

\begin{lstlisting}[frame=tblr]
a.txt 475
b.txt 31
total 506
\end{lstlisting}

\subsection*{Renaming files}


Write a program called \texttt{rename} that takes three or more command-line arguments:

\begin{enumerate}

\item A \glossref{filename extension} to match.

\item An extension to replace it with.

\item The names of one or more existing files.

\end{enumerate}


When it runs,
\texttt{rename} renames any files with the first extension to create files with the second extension,
but will \emph{not} overwrite an existing file.
For example,
suppose a directory contains \texttt{a.txt}, \texttt{b.txt}, and \texttt{b.bck}.
The command:

\begin{lstlisting}[frame=tblr]
rename .txt .bck a.txt b.txt
\end{lstlisting}


\noindent will rename \texttt{a.txt} to \texttt{a.bck},
but will \emph{not} rename \texttt{b.txt} because \texttt{b.bck} already exists.

\chapter{Asynchronous Programming}\label{async-programming}


\noindent 
  Terms defined: \glossref{call stack}, \glossref{character encoding}, \glossref{class}, \glossref{constructor}, \glossref{event loop}, \glossref{exception}, \glossref{fluent interface}, \glossref{method}, \glossref{method chaining}, \glossref{non-blocking execution}, \glossref{promise}, \glossref{promisification}, \glossref{protocol}, \glossref{UTF-8}



Callbacks work,
but they are hard to read and debug,
which means they only “work” in a limited sense.
JavaScript’s developers added \glossref{promises}\index{promise!as alternative to callback} to the language in 2015
to make callbacks easier to write and understand,
and more recently they added the keywords \texttt{async} and \texttt{await} as well
to make asynchronous programming easier still.
To show how these work,
we will create a \glossref{class} of our own called \texttt{Pledge}
that provides the same core features as promises.
Our explanation was inspired by \hreffoot{Trey Huffine’s}{https://medium.com/@treyhuffine}\index{Huffine, Trey} \hreffoot{tutorial}{https://levelup.gitconnected.com/understand-javascript-promises-by-building-a-promise-from-scratch-84c0fd855720},
and we encourage you to read that as well.

\section{How can we manage asynchronous execution?}\label{async-programming-manage}


JavaScript is built around an \glossref{event loop}\index{event loop}\index{execution!event loop}.
Every task is represented by an entry in a queue;
the event loop repeatedly takes a task from the front of the queue,
runs it,
and adds any new tasks that it creates to the back of the queue to run later.
Only one task runs at a time;
each has its own \glossref{call stack},
but objects can be shared between tasks
(\figref{async-programming-event-loop}).

\figpdf{async-programming-event-loop}{./async-programming/event-loop.pdf}{Using an event loop to manage concurrent tasks.}{0.6}


Most tasks execute all the code available in the order it is written.
For example,
this one-line program uses \texttt{Array.forEach}\index{Array.forEach}
to print each element of an array in turn:


\begin{lstlisting}[frame=tblr]
[1000, 1500, 500].forEach(t => console.log(t))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
1000
1500
500
\end{lstlisting}



However,
a handful of special built-in functions make \hreffoot{Node}{https://nodejs.org/en/} switch tasks
or add new tasks to the run queue.
For example,
\texttt{setTimeout}\index{setTimeout} tells Node to run a callback function
after a certain number of milliseconds have passed.
Its first argument is a callback function that takes no arguments,
and its second is the delay.
When \texttt{setTimeout} is called,
Node sets the callback aside for the requested length of time,
then adds it to the run queue.
(This means the task runs \emph{at least} the specified number of milliseconds later).

\begin{callout}


\subsubsection*{Why zero arguments?}


\texttt{setTimeout}‘s requirement that callback functions take no arguments
is another example of a \glossref{protocol}\index{protocol!API as}\index{API!as protocol}.
One way to think about it is that protocols allow old code to use new code:
whoever wrote \texttt{setTimeout} couldn’t know what specific tasks we want to delay,
so they specified a way to wrap up any task at all.

\end{callout}


As the listing below shows,
the original task can generate many new tasks before it completes,
and those tasks can run in a different order than the order in which they were created
(\figref{async-programming-set-timeout}).


\begin{lstlisting}[frame=tblr]
[1000, 1500, 500].forEach(t => {
  console.log(`about to setTimeout for ${t}`)
  setTimeout(() => console.log(`inside timer handler for ${t}`), t)
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
about to setTimeout for 1000
about to setTimeout for 1500
about to setTimeout for 500
inside timer handler for 500
inside timer handler for 1000
inside timer handler for 1500
\end{lstlisting}


\figpdf{async-programming-set-timeout}{./async-programming/set-timeout.pdf}{Using \texttt{setTimeout} to delay operations.}{0.6}


If we give \texttt{setTimeout} a delay of zero milliseconds,
the new task can be run right away,
but any other tasks that are waiting have a chance to run as well:


\begin{lstlisting}[frame=tblr]
[1000, 1500, 500].forEach(t => {
  console.log(`about to setTimeout for ${t}`)
  setTimeout(() => console.log(`inside timer handler for ${t}`), 0)
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
about to setTimeout for 1000
about to setTimeout for 1500
about to setTimeout for 500
inside timer handler for 1000
inside timer handler for 1500
inside timer handler for 500
\end{lstlisting}



\noindent We can use this trick to build a generic
\glossref{non-blocking function}\index{execution!non-blocking}\index{non-blocking execution}
that takes a callback defining a task
and switches tasks if any others are available:


\begin{lstlisting}[frame=tblr]
const nonBlocking = (callback) => {
  setTimeout(callback, 0)
}

[1000, 1500, 500].forEach(t => {
  console.log(`about to do nonBlocking for ${t}`)
  nonBlocking(() => console.log(`inside timer handler for ${t}`))
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
about to do nonBlocking for 1000
about to do nonBlocking for 1500
about to do nonBlocking for 500
inside timer handler for 1000
inside timer handler for 1500
inside timer handler for 500
\end{lstlisting}



Node’s built-in function \texttt{setImmediate}\index{setImmediate}
does exactly what our \texttt{nonBlocking} function does:
Node also has \texttt{process.nextTick},
which doesn’t do quite the same thing—we’ll explore the differences in the exercises.


\begin{lstlisting}[frame=tblr]
[1000, 1500, 500].forEach(t => {
  console.log(`about to do setImmediate for ${t}`)
  setImmediate(() => console.log(`inside immediate handler for ${t}`))
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
about to do setImmediate for 1000
about to do setImmediate for 1500
about to do setImmediate for 500
inside immediate handler for 1000
inside immediate handler for 1500
inside immediate handler for 500
\end{lstlisting}


\section{How do promises work?}\label{async-programming-promises}


Before we start building our own promises\index{promise!behavior},
let’s look at how we want them to work:


\begin{lstlisting}[frame=tblr]
import Pledge from './pledge.js'

new Pledge((resolve, reject) => {
  console.log('top of a single then clause')
  setTimeout(() => {
    console.log('about to call resolve callback')
    resolve('this is the result')
  }, 0)
}).then((value) => {
  console.log(`in 'then' with "${value}"`)
  return 'first then value'
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
top of a single then clause
about to call resolve callback
in 'then' with "this is the result"
\end{lstlisting}



This short program creates a new \texttt{Pledge}
with a callback that takes two other callbacks as arguments:
\texttt{resolve}\index{promise!resolve}\index{resolve promise} (which will run when everything worked)
and \texttt{reject}\index{promise!reject}\index{reject promise} (which will run when something went wrong).
The top-level callback does the first part of what we want to do,
i.e.,
whatever we want to run before we expect a delay;
for demonstration purposes, we will use \texttt{setTimeout} with zero delay to switch tasks.
Once this task resumes,
we call the \texttt{resolve} callback to trigger whatever is supposed to happen after the delay.


Now look at the line with \texttt{then}.
This is a \glossref{method} of the \texttt{Pledge} object we just created,
and its job is to do whatever we want to do after the delay.
The argument to \texttt{then} is yet another callback function;
it will get the value passed to \texttt{resolve},
which is how the first part of the action communicates with the second
(\figref{async-programming-resolve}).

\figpdf{async-programming-resolve}{./async-programming/resolve.pdf}{Order of operations when a promise resolves.}{0.6}


In order to make this work,
\texttt{Pledge}‘s \glossref{constructor} must take a single function called \texttt{action}.
This function must take take two callbacks as arguments:
what to do if the action completes successfully
and what to do if it doesn’t (i.e., how to handle errors).
\texttt{Pledge} will provide these callbacks to the action at the right times.


\texttt{Pledge} also needs two methods:
\texttt{then}\index{promise!then} to enable more actions
and \texttt{catch}\index{promise!catch} to handle errors.
To simplify things just a little bit,
we will allow users to \glossref{chain}\index{method chaining} as many \texttt{then}s as they want,
but only allow one \texttt{catch}.

\section{How can we chain operations together?}\label{async-programming-fluent}


A \glossref{fluent interface}\index{fluent interface}\index{programming style!fluent interface}
is a style of object-oriented programming
in which the methods of an object return \texttt{this}
so that method calls can be chained together.
For example,
if our class is:

\begin{lstlisting}[frame=tblr]
class Fluent {
  constructor () {...}

  first (top) {
    ...do something with top...
    return this
  }

  second (left, right) {
    ...do something with left and right...
  }
}
\end{lstlisting}


\noindent then we can write:

\begin{lstlisting}[frame=tblr]
  const f = new Fluent()
  f.first('hello').second('and', 'goodbye')
\end{lstlisting}


\noindent or even

\begin{lstlisting}[frame=tblr]
  (new Fluent()).first('hello').second('and', 'goodbye')
\end{lstlisting}


\texttt{Array}‘s fluent interface lets us write expressions like
\texttt{Array.filter(...).map(...).map(...)}
that are usually more readable than assigning intermediate results to temporary variables.


If the original action given to our \texttt{Pledge} completes successfully,
the \texttt{Pledge} gives us a value by calling the \texttt{resolve} callback.
We pass this value to the first \texttt{then},
pass the result of that \texttt{then} to the second one,
and so on.
If any of them fail and throw an \glossref{exception}\index{exception!in promise},
we pass that exception to the error handler.
Putting it all together,
the whole class looks like this:


\begin{lstlisting}[frame=tblr]
class Pledge {
  constructor (action) {
    this.actionCallbacks = []
    this.errorCallback = () => {}
    action(this.onResolve.bind(this), this.onReject.bind(this))
  }

  then (thenHandler) {
    this.actionCallbacks.push(thenHandler)
    return this
  }

  catch (errorHandler) {
    this.errorCallback = errorHandler
    return this
  }

  onResolve (value) {
    let storedValue = value
    try {
      this.actionCallbacks.forEach((action) => {
        storedValue = action(storedValue)
      })
    } catch (err) {
      this.actionCallbacks = []
      this.onReject(err)
    }
  }

  onReject (err) {
    this.errorCallback(err)
  }
}

export default Pledge
\end{lstlisting}


\begin{callout}


\subsubsection*{Binding \texttt{this}}


\texttt{Pledge}‘s constructor makes two calls to a special function called \texttt{bind}\index{bind method to object}.
When we create an object \texttt{obj} and call a method \texttt{meth},
JavaScript sets the special variable \texttt{this} to \texttt{obj} inside \texttt{meth}.
If we use a method as a callback,
though,
\texttt{this} isn’t automatically set to the correct object.
To convert the method to a plain old function with the right \texttt{this},
we have to use \texttt{bind}.
\hreffoot{The documentation}{https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global\_objects/Function/bind} has more details and examples.

\end{callout}


Let’s create a \texttt{Pledge} and return a value:


\begin{lstlisting}[frame=tblr]
import Pledge from './pledge.js'

new Pledge((resolve, reject) => {
  console.log('top of a single then clause')
}).then((value) => {
  console.log(`then with "${value}"`)
  return 'first then value'
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
top of a single then clause
\end{lstlisting}



\noindent Why didn’t this work?

\begin{enumerate}

\item 

We can’t use \texttt{return} with pledges
    because the call stack of the task that created the pledge is gone
    by the time the pledge executes.
    Instead, we must call \texttt{resolve} or \texttt{reject}.



\item 

We haven’t done anything that defers execution,
    i.e.,
    there is no call to \texttt{setTimeout}, \texttt{setImmediate},
    or anything else that would switch tasks.
    Our original motivating example got this right.



\end{enumerate}


This example shows how we can chain actions together:


\begin{lstlisting}[frame=tblr]
import Pledge from './pledge.js'

new Pledge((resolve, reject) => {
  console.log('top of action callback with double then and a catch')
  setTimeout(() => {
    console.log('about to call resolve callback')
    resolve('initial result')
    console.log('after resolve callback')
  }, 0)
  console.log('end of action callback')
}).then((value) => {
  console.log(`first then with "${value}"`)
  return 'first value'
}).then((value) => {
  console.log(`second then with "${value}"`)
  return 'second value'
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
top of action callback with double then and a catch
end of action callback
about to call resolve callback
first then with "initial result"
second then with "first value"
after resolve callback
\end{lstlisting}



\noindent Notice that inside each \texttt{then} we \emph{do} use \texttt{return}
because these clauses all run in a single task.
As we will see in the next section,
the full implementation of \texttt{Promise} allows us to run both normal code
and delayed tasks inside \texttt{then} handlers.


Finally,
in this example we explicitly signal a problem by calling \texttt{reject}
to make sure our error handling does what it’s supposed to:


\begin{lstlisting}[frame=tblr]
import Pledge from './pledge.js'

new Pledge((resolve, reject) => {
  console.log('top of action callback with deliberate error')
  setTimeout(() => {
    console.log('about to reject on purpose')
    reject('error on purpose')
  }, 0)
}).then((value) => {
  console.log(`should not be here with "${value}"`)
}).catch((err) => {
  console.log(`in error handler with "${err}"`)
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
top of action callback with deliberate error
about to reject on purpose
in error handler with "error on purpose"
\end{lstlisting}


\section{How are real promises different?}\label{async-programming-real}


Let’s rewrite our chained pledge with built-in promises:


\begin{lstlisting}[frame=tblr]
new Promise((resolve, reject) => {
  console.log('top of action callback with double then and a catch')
  setTimeout(() => {
    console.log('about to call resolve callback')
    resolve('initial result')
    console.log('after resolve callback')
  }, 0)
  console.log('end of action callback')
}).then((value) => {
  console.log(`first then with "${value}"`)
  return 'first value'
}).then((value) => {
  console.log(`second then with "${value}"`)
  return 'second value'
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
top of action callback with double then and a catch
end of action callback
about to call resolve callback
after resolve callback
first then with "initial result"
second then with "first value"
\end{lstlisting}



It looks almost the same,
but if we read the output carefully
we can see that the callbacks run \emph{after} the main program finishes.
This is a signal that Node is delaying the execution of the code in the \texttt{then} handler.


A very common pattern is to return another promise from inside \texttt{then}
so that the next \texttt{then} is called on the returned promise,
not on the original promise
(\figref{async-programming-chained}).
This is another way to implement a fluent interface:
if a method of one object returns a second object,
we can call a method of the second object immediately.


\begin{lstlisting}[frame=tblr]
const delay = (message) => {
  return new Promise((resolve, reject) => {
    console.log(`constructing promise: ${message}`)
    setTimeout(() => {
      resolve(`resolving: ${message}`)
    }, 1)
  })
}

console.log('before')
delay('outer delay')
  .then((value) => {
    console.log(`first then: ${value}`)
    return delay('inner delay')
  }).then((value) => {
    console.log(`second then: ${value}`)
  })
console.log('after')
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
before
constructing promise: outer delay
after
first then: resolving: outer delay
constructing promise: inner delay
second then: resolving: inner delay
\end{lstlisting}


\figpdf{async-programming-chained}{./async-programming/chained.pdf}{Chaining promises to make asynchronous operations depend on each other.}{0.6}


We therefore have three rules for chaining promises:

\begin{enumerate}

\item 

If our code can run synchronously, just put it in \texttt{then}.



\item 

If we want to use our own asynchronous function,
    it must create and return a promise.



\item 

Finally,
    if we want to use a library function that relies on callbacks,
    we have to convert it to use promises.
    Doing this is called \glossref{promisification}
    (because programmers will rarely pass up an opportunity to add a bit of jargon to the world),
    and most functions in Node have already been promisified.



\end{enumerate}

\section{How can we build tools with promises?}\label{async-programming-tools}


Promises may seem more complex than callbacks right now,
but that’s because we’re looking at how they work rather than at how to use them.
To explore the latter subject,
let’s use promises to build a program to count the number of lines in a set of files.
A few moments of search on \hreffoot{NPM}{https://www.npmjs.com/} turns up a promisified version of \texttt{fs-extra}
called \texttt{fs-extra-promise},
so we will rely on it for file operations.


Our first step is to count the lines in a single file:


\begin{lstlisting}[frame=tblr]
import fs from 'fs-extra-promise'

const filename = process.argv[2]

fs.readFileAsync(filename, { encoding: 'utf-8' })
  .then(data => {
    const length = data.split('\n').length - 1
    console.log(`${filename}: ${length}`)
  })
  .catch(err => {
    console.error(err.message)
  })
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node count-lines-single-file.js count-lines-single-file.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
count-lines-single-file.js: 12
\end{lstlisting}


\begin{callout}


\subsubsection*{Character encoding}


A \glossref{character encoding}\index{character encoding}
specifies how characters are stored as bytes.
The most widely used is \glossref{UTF-8}\index{UTF-8}\index{character encoding!UTF-8},
which stores characters common in Western European languages in a single byte
and uses multi-byte sequences for other symbols.
If we don’t specify a character encoding,
\texttt{fs.readFileAsync} gives us an array of bytes rather than a string of characters.
We can tell we’ve made this mistake when we try to call a method of \texttt{String}
and Node tells us we can’t.

\end{callout}


The next step is to count the lines in multiple files.
We can use \texttt{glob-promise} to delay handling the output of \texttt{glob},
but we need some way to create a separate task to count the lines in each file
and to wait until those line counts are available before exiting our program.


The tool we want is \texttt{Promise.all}\index{Promise.all},
which waits until all of the promises in an array have completed.
To make our program a little more readable,
we will put the creation of the promise for each file in a separate function:


\begin{lstlisting}[frame=tblr]
import glob from 'glob-promise'
import fs from 'fs-extra-promise'

const main = (srcDir) => {
  glob(`${srcDir}/**/*.*`)
    .then(files => Promise.all(files.map(f => lineCount(f))))
    .then(counts => counts.forEach(c => console.log(c)))
    .catch(err => console.log(err.message))
}

const lineCount = (filename) => {
  return new Promise((resolve, reject) => {
    fs.readFileAsync(filename, { encoding: 'utf-8' })
      .then(data => resolve(data.split('\n').length - 1))
      .catch(err => reject(err))
  })
}

const srcDir = process.argv[2]
main(srcDir)
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node count-lines-globbed-files.js .
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
10
1
12
4
1
...
3
2
5
2
14
\end{lstlisting}



However,
we want to display the names of the files whose lines we’re counting along with the counts.
To do this our \texttt{then} must return two values.
We could put them in an array,
but it’s better practice to construct a temporary object with named fields
(\figref{async-programming-temporary-named-fields}).
This approach allows us to add or rearrange fields without breaking code
and also serves as a bit of documentation.
With this change
our line-counting program becomes:


\begin{lstlisting}[frame=tblr]
import glob from 'glob-promise'
import fs from 'fs-extra-promise'

const main = (srcDir) => {
  glob(`${srcDir}/**/*.*`)
    .then(files => Promise.all(files.map(f => lineCount(f))))
    .then(counts => counts.forEach(
      c => console.log(`${c.lines}: ${c.name}`)))
    .catch(err => console.log(err.message))
}

const lineCount = (filename) => {
  return new Promise((resolve, reject) => {
    fs.readFileAsync(filename, { encoding: 'utf-8' })
      .then(data => resolve({
        name: filename,
        lines: data.split('\n').length - 1
      }))
      .catch(err => reject(err))
  })
}

const srcDir = process.argv[2]
main(srcDir)
\end{lstlisting}


\figpdf{async-programming-temporary-named-fields}{./async-programming/temporary-named-fields.pdf}{Creating temporary objects with named fields to carry values forward.}{0.6}


As in \chapref{systems-programming},
this works until we run into a directory whose name name matches \texttt{*.*},
which we do when counting the lines in the contents of \texttt{node\_modules}.
The solution once again is to use \texttt{stat} to check if something is a file or not
before trying to read it.
And since \texttt{stat} returns an object that doesn’t include the file’s name,
we create another temporary object to pass information down the chain of \texttt{then}s.


\begin{lstlisting}[frame=tblr]
import glob from 'glob-promise'
import fs from 'fs-extra-promise'

const main = (srcDir) => {
  glob(`${srcDir}/**/*.*`)
    .then(files => Promise.all(files.map(f => statPair(f))))
    .then(files => files.filter(pair => pair.stats.isFile()))
    .then(files => files.map(pair => pair.filename))
    .then(files => Promise.all(files.map(f => lineCount(f))))
    .then(counts => counts.forEach(
      c => console.log(`${c.lines}: ${c.name}`)))
    .catch(err => console.log(err.message))
}

const statPair = (filename) => {
  return new Promise((resolve, reject) => {
    fs.statAsync(filename)
      .then(stats => resolve({ filename, stats }))
      .catch(err => reject(err))
  })
}

const lineCount = (filename) => {
  return new Promise((resolve, reject) => {
    fs.readFileAsync(filename, { encoding: 'utf-8' })
      .then(data => resolve({
        name: filename,
        lines: data.split('\n').length - 1
      }))
      .catch(err => reject(err))
  })
}

const srcDir = process.argv[2]
main(srcDir)
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node count-lines-with-stat.js .
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
10: ./assign-immediately.js
1: ./assign-immediately.out
12: ./await-fs.js
4: ./await-fs.out
1: ./await-fs.sh
...
3: ./x-multiple-catch/example.js
2: ./x-multiple-catch/example.txt
5: ./x-trace-load.md
2: ./x-trace-load/config.yml
14: ./x-trace-load/example.js
\end{lstlisting}



\noindent This code is complex, but much simpler than it would be if we were using callbacks.

\begin{callout}


\subsubsection*{Lining things up}


This code uses the expression \texttt{\{filename, stats\}}
to create an object whose keys are \texttt{filename} and \texttt{stats},
and whose values are the values of the corresponding variables.
Doing this makes the code easier to read,
both because it’s shorter
but also because it signals that the value associated with the key \texttt{filename}
is exactly the value of the variable with the same name.

\end{callout}

\section{How can we make this more readable?}\label{async-programming-readable}


Promises eliminate the deep nesting associated with callbacks of callbacks,
but they are still hard to follow.
The latest versions of JavaScript provide two new keywords \texttt{async}\index{async keyword} and \texttt{await}\index{await keyword}
to flatten code further.
\texttt{async} means “this function implicitly returns a promise”,
while \texttt{await} means “wait for a promise to resolve”.
This short program uses both keywords to print the first ten characters of a file:


\begin{lstlisting}[frame=tblr]
import fs from 'fs-extra-promise'

const firstTenCharacters = async (filename) => {
  const text = await fs.readFileAsync(filename, 'utf-8')
  console.log(`inside, raw text is ${text.length} characters long`)
  return text.slice(0, 10)
}

console.log('about to call')
const result = firstTenCharacters(process.argv[2])
console.log(`function result has type ${result.constructor.name}`)
result.then(value => console.log(`outside, final result is "${value}"`))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
about to call
function result has type Promise
inside, raw text is 24 characters long
outside, final result is "Begin at t"
\end{lstlisting}


\begin{quotation}

\subsection*{Translating code}


When Node sees \texttt{await} and \texttt{async}
it silently converts\index{promise!automatic creation of} the code to use promises with \texttt{then}, \texttt{resolve}, and \texttt{reject};
we will see how this works in \chapref{code-generator}.
In order to provide a context for this transformation
we must put \texttt{await} inside a function that is declared to be \texttt{async}:
we can’t simply write \texttt{await fs.statAsync(...)} at the top level of our program
outside a function.
This requirement is occasionally annoying,
but since we should be putting our code in functions anyway
it’s hard to complain.

\end{quotation}


To see how much cleaner our code is with \texttt{await} and \texttt{async},
let’s rewrite our line counting program to use them.
First,
we modify the two helper functions to look like they’re waiting for results and returning them.
They actually wrap their results in promises and return those,
but Node now takes care of that for us:


\begin{lstlisting}[frame=tblr]
const statPair = async (filename) => {
  const stats = await fs.statAsync(filename)
  return { filename, stats }
}

const lineCount = async (filename) => {
  const data = await fs.readFileAsync(filename, 'utf-8')
  return {
    filename,
    lines: data.split('\n').length - 1
  }
}
\end{lstlisting}



Next,
we modify \texttt{main} to wait for things to complete.
We must still use \texttt{Promise.all} to handle the promises
that are counting lines for individual files,
but the result is less cluttered than our previous version.


\begin{lstlisting}[frame=tblr]
const main = async (srcDir) => {
  const files = await glob(`${srcDir}/**/*.*`)
  const pairs = await Promise.all(
    files.map(async filename => await statPair(filename))
  )
  const filtered = pairs
    .filter(pair => pair.stats.isFile())
    .map(pair => pair.filename)
  const counts = await Promise.all(
    filtered.map(async name => await lineCount(name))
  )
  counts.forEach(
    ({ filename, lines }) => console.log(`${lines}: ${filename}`)
  )
}

const srcDir = process.argv[2]
main(srcDir)
\end{lstlisting}


\section{How can we handle errors with asynchronous code?}\label{async-programming-errors}


We created several intermediate variables in the line-counting program to make the steps clearer.
Doing this also helps with error handling:
to see how,
we will build up an example in stages.


First,
if we return a promise that fails without using \texttt{await},
then our main function will finish running before the error occurs,
and our \texttt{try}/\texttt{catch} doesn’t help us
(\figref{async-programming-handling-errors}):


\begin{lstlisting}[frame=tblr]
async function returnImmediately () {
  try {
    return Promise.reject(new Error('deliberate'))
  } catch (err) {
    console.log('caught exception')
  }
}

returnImmediately()
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
/u/stjs/async-programming/return-immediately.js:3
\end{lstlisting}


\figpdf{async-programming-handling-errors}{./async-programming/handling-errors.pdf}{Wrong and right ways to handle errors in asynchronous code.}{0.6}


One solution to this problem is to be consistent and always return something.
Because the function is declared \texttt{async},
the \texttt{Error} in the code below is automatically wrapped in a promise
so we can use \texttt{.then} and \texttt{.catch} to handle it as before:


\begin{lstlisting}[frame=tblr]
async function returnImmediately () {
  try {
    return Promise.reject(new Error('deliberate'))
  } catch (err) {
    return new Error('caught exception')
  }
}

const result = returnImmediately()
result.catch(err => console.log(`caller caught ${err}`))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
caller caught Error: deliberate
\end{lstlisting}



If instead we \texttt{return await}\index{exception!with await},
the function waits until the promise runs before returning.
The promise is turned into an exception because it failed,
and since we’re inside the scope of our \texttt{try}/\texttt{catch} block,
everything works as we want:


\begin{lstlisting}[frame=tblr]
async function returnAwait () {
  try {
    return await Promise.reject(new Error('deliberate'))
  } catch (err) {
    console.log('caught exception')
  }
}

returnAwait()
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
caught exception
\end{lstlisting}



\noindent We prefer the second approach,
but whichever you choose,
please be consistent.

\section{Exercises}\label{async-programming-exercises}

\subsection*{Immediate versus next tick}


What is the difference between \texttt{setImmediate} and \texttt{process.nextTick}?
When would you use each one?

\subsection*{Tracing promise execution}

\begin{enumerate}

\item 

What does this code print and why?

\begin{lstlisting}[frame=tblr]
Promise.resolve('hello')
\end{lstlisting}



\item 

What does this code print and why?

\begin{lstlisting}[frame=tblr]
Promise.resolve('hello').then(result => console.log(result))
\end{lstlisting}



\item 

What does this code print and why?

\begin{lstlisting}[frame=tblr]
const p = new Promise((resolve, reject) => resolve('hello'))
  .then(result => console.log(result))
\end{lstlisting}



\end{enumerate}


Hint: try each snippet of code interactively in the Node interpreter and as a command-line script.

\subsection*{Multiple catches}


Suppose we create a promise that deliberately fails and then add two error handlers:


\begin{lstlisting}[frame=tblr]
const oops = new Promise((resolve, reject) => reject(new Error('failure')))
oops.catch(err => console.log(err.message))
oops.catch(err => console.log(err.message))
\end{lstlisting}



\noindent When the code is run it produces:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
failure
failure
\end{lstlisting}


\begin{enumerate}

\item Trace the order of operations: what is created and executed when?

\item What happens if we run these same lines interactively?
    Why do we see something different than what we see when we run this file from the command line?

\end{enumerate}

\subsection*{Then after catch}


Suppose we create a promise that deliberately fails
and attach both \texttt{then} and \texttt{catch} to it:


\begin{lstlisting}[frame=tblr]
new Promise((resolve, reject) => reject(new Error('failure')))
  .catch(err => console.log(err))
  .then(err => console.log(err))
\end{lstlisting}



\noindent When the code is run it produces:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
Error: failure
    at /u/stjs/promises/catch-then/example.js:1:41
    at new Promise (<anonymous>)
    at Object.<anonymous> (/u/stjs/promises/catch-then/example.js:1:1)
    at Module._compile (internal/modules/cjs/loader.js:1151:30)
    at Object.Module._extensions..js \
 (internal/modules/cjs/loader.js:1171:10)
    at Module.load (internal/modules/cjs/loader.js:1000:32)
    at Function.Module._load (internal/modules/cjs/loader.js:899:14)
    at Function.executeUserEntryPoint [as runMain] \
 (internal/modules/run_main.js:71:12)
    at internal/main/run_main_module.js:17:47
undefined
\end{lstlisting}


\begin{enumerate}

\item Trace the order of execution.

\item Why is \texttt{undefined} printed at the end?

\end{enumerate}

\subsection*{Head and tail}


The Unix \texttt{head} command shows the first few lines of one or more files,
while the \texttt{tail} command shows the last few.
Write programs \texttt{head.js} and \texttt{tail.js} that do the same things using promises and \texttt{async}/\texttt{await},
so that:

\begin{lstlisting}[frame=tblr]
node head.js 5 first.txt second.txt third.txt
\end{lstlisting}


\noindent prints the first five lines of each of the three files and:

\begin{lstlisting}[frame=tblr]
node tail.js 5 first.txt second.txt third.txt
\end{lstlisting}


\noindent prints the last five lines of each file.

\subsection*{Histogram of line counts}


Extend \texttt{count-lines-with-stat-async.js} to create a program \texttt{lh.js}
that prints two columns of output:
the number of lines in one or more files
and the number of files that are that long.
For example,
if we run:

\begin{lstlisting}[frame=tblr]
node lh.js promises/*.*
\end{lstlisting}


\noindent the output might be:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Length}} & \textbf{\underline{Number of Files}} \\
1 & 7 \\
3 & 3 \\
4 & 3 \\
6 & 7 \\
8 & 2 \\
12 & 2 \\
13 & 1 \\
15 & 1 \\
17 & 2 \\
20 & 1 \\
24 & 1 \\
35 & 2 \\
37 & 3 \\
38 & 1 \\
171 & 1 \\
\end{tabular}

\vspace{\baselineskip}

\subsection*{Select matching lines}


Using \texttt{async} and \texttt{await},
write a program called \texttt{match.js} that finds and prints lines containing a given string.
For example:

\begin{lstlisting}[frame=tblr]
node match.js Toronto first.txt second.txt third.txt
\end{lstlisting}


\noindent would print all of the lines from the three files that contain the word “Toronto”.

\subsection*{Find lines in all files}


Using \texttt{async} and \texttt{await},
write a program called \texttt{in-all.js} that finds and prints lines found in all of its input files.
For example:

\begin{lstlisting}[frame=tblr]
node in-all.js first.txt second.txt third.txt
\end{lstlisting}


\noindent will print those lines that occur in all three files.

\subsection*{Find differences between two files}


Using \texttt{async} and \texttt{await},
write a program called \texttt{file-diff.js}
that compares the lines in two files
and shows which ones are only in the first file,
which are only in the second,
and which are in both.
For example,
if \texttt{left.txt} contains:

\begin{lstlisting}[frame=tblr]
some
people
\end{lstlisting}


\noindent and \texttt{right.txt} contains:

\begin{lstlisting}[frame=tblr]
write
some
code
\end{lstlisting}


\noindent then:

\begin{lstlisting}[frame=tblr]
node file-diff.js left.txt right.txt
\end{lstlisting}


\noindent would print:

\begin{lstlisting}[frame=tblr]
2 code
1 people
* some
2 write
\end{lstlisting}


\noindent where \texttt{1}, \texttt{2}, and \texttt{*} show whether lines are in only the first or second file
or are in both.
Note that the order of the lines in the file doesn’t matter.


Hint: you may want to use the \texttt{Set} class to store lines.

\subsection*{Trace file loading}


Suppose we are loading a YAML configuration file
using the promisified version of the \texttt{fs} library.
In what order do the print statements in this test program appear and why?


\begin{lstlisting}[frame=tblr]
import fs from 'fs-extra-promise'
import yaml from 'js-yaml'

const test = async () => {
  const raw = await fs.readFileAsync('config.yml', 'utf-8')
  console.log('inside test, raw text', raw)
  const cooked = yaml.safeLoad(raw)
  console.log('inside test, cooked configuration', cooked)
  return cooked
}

const result = test()
console.log('outside test, result is', result.constructor.name)
result.then(something => console.log('outside test we have', something))
\end{lstlisting}


\subsection*{Any and all}

\begin{enumerate}

\item 

Add a method \texttt{Pledge.any} that takes an array of pledges
    and as soon as one of the pledges in the array resolves,
    returns a single promise that resolves with the value from that pledge.



\item 

Add another method \texttt{Pledge.all} that takes an array of pledges
    and returns a single promise that resolves to an array
    containing the final values of all of those pledges.



\end{enumerate}


\hreffoot{This article}{https://2ality.com/2019/08/promise-combinators.html} may be helpful.

\chapter{Unit Testing}\label{unit-test}


\noindent 
  Terms defined: \glossref{absolute error}, \glossref{actual result (of test)}, \glossref{assertion}, \glossref{caching}, \glossref{defensive programming}, \glossref{design pattern}, \glossref{dynamic loading}, \glossref{error (in a test)}, \glossref{exception handler}, \glossref{expected result (of test)}, \glossref{exploratory programming}, \glossref{fail (a test)}, \glossref{fixture}, \glossref{global variable}, \glossref{introspection}, \glossref{lifecycle}, \glossref{pass (a test)}, \glossref{relative error}, \glossref{side effect}, \glossref{Singleton pattern}, \glossref{test runner}, \glossref{test subject}, \glossref{throw (exception)}, \glossref{unit test}



We have written many small programs in the previous two chapters,
but haven’t really tested any of them.
That’s OK for \glossref{exploratory programming}\index{exploratory programming},
but if our software is going to be used instead of just read,
we should try to make sure it works.


A tool for writing and running \glossref{unit tests}\index{unit test!requirements for} is a good first step.
Such a tool should:

\begin{itemize}

\item find files containing tests;

\item find the tests in those files;

\item run the tests;

\item capture their results; and

\item report each test’s result and a summary of those results.

\end{itemize}


Our design is inspired by tools like \hreffoot{Mocha}{https://mochajs.org/}\index{Mocha} and \hreffoot{Jest}{https://jestjs.io/}\index{Jest},
which were in turn inspired by tools built for other languages
from the 1980s onward \cite{Meszaros2007,Tudose2020}.

\section{How should we structure unit testing?}\label{unit-test-structure}


As in other unit testing frameworks,
each test will be a function of zero arguments
so that the framework can run them all in the same way.
Each test will create a \glossref{fixture}\index{fixture (in unit test)}\index{unit test!fixture} to be tested
and use \glossref{assertions}\index{assertion!in unit test}
to compare the \glossref{actual result}\index{actual result (in unit test)}\index{unit test!actual result}
against the \glossref{expected result}\index{expected result (in unit test)}\index{unit test!expected result}.
The outcome can be exactly one of:

\begin{itemize}

\item 

\glossref{Pass}\index{pass (in unit test)}\index{unit test!pass}:
    the \glossref{test subject}\index{test subject (in unit test)}\index{unit test!test subject} works as expected.



\item 

\glossref{Fail}\index{fail (in unit test)}\index{unit test!fail}:
    something is wrong with the test subject.



\item 

\glossref{Error}\index{error (in unit test)}\index{unit test!error}:
    something wrong in the test itself,
    which means we don’t know whether the test subject is working properly or not.



\end{itemize}


To make this work,
we need some way to distinguish failing tests from broken ones.
Our solution relies on the fact that exceptions are objects
and that a program can use \glossref{introspection}\index{introspection!in unit testing}
to determine the class of an object.
If a test \glossref{throws an exception}\index{exception!throw} whose class is \texttt{assert.AssertionError},
then we will assume the exception came from
one of the assertions we put in the test as a check
(\figref{unit-test-mental-model}).
Any other kind of assertion indicates that the test itself contains an error.

\figpdf{unit-test-mental-model}{./unit-test/mental-model.pdf}{Running tests that can pass, fail, or contain errors.}{0.6}

\section{How can we separate registration, execution, and reporting?}\label{unit-test-design}


To start,
let’s use a handful of \glossref{global variables} to record tests and their results:


\begin{lstlisting}[frame=tblr]
// State of tests.
const HopeTests = []
let HopePass = 0
let HopeFail = 0
let HopeError = 0
\end{lstlisting}



We don’t run tests immediately
because we want to wrap each one in our own \glossref{exception handler}\index{exception!handler}.
Instead,
the function \texttt{hopeThat} saves a descriptive message and a callback function that implements a test
in the \texttt{HopeTest} array.


\begin{lstlisting}[frame=tblr]
// Record a single test for running later.
const hopeThat = (message, callback) => {
  HopeTests.push([message, callback])
}
\end{lstlisting}


\begin{callout}


\subsubsection*{Independence}


Because we’re appending tests to an array,
they will be run in the order in which they are registered,
but we shouldn’t rely on that.
Every unit test should work independently of every other
so that an error or failure in an early test
doesn’t affect the result of a later one.

\end{callout}


Finally,
the function \texttt{main} runs all registered tests:


\begin{lstlisting}[frame=tblr]
// Run all of the tests that have been asked for and report summary.
const main = () => {
  HopeTests.forEach(([message, test]) => {
    try {
      test()
      HopePass += 1
    } catch (e) {
      if (e instanceof assert.AssertionError) {
        HopeFail += 1
      } else {
        HopeError += 1
      }
    }
  })

  console.log(`pass ${HopePass}`)
  console.log(`fail ${HopeFail}`)
  console.log(`error ${HopeError}`)
}
\end{lstlisting}



\noindent If a test completes without an exception, it passes.
If any of the \texttt{assert} calls inside the test raises an \texttt{AssertionError},
the test fails,
and if it raises any other exception,
it’s an error.
After all tests are run,
\texttt{main} reports the number of results of each kind.


Let’s try it out:


\begin{lstlisting}[frame=tblr]
// Something to test (doesn't handle zero properly).
const sign = (value) => {
  if (value < 0) {
    return -1
  } else {
    return 1
  }
}

// These two should pass.
hopeThat('Sign of negative is -1', () => assert(sign(-3) === -1))
hopeThat('Sign of positive is 1', () => assert(sign(19) === 1))

// This one should fail.
hopeThat('Sign of zero is 0', () => assert(sign(0) === 0))

// This one is an error.
hopeThat('Sign misspelled is error', () => assert(sgn(1) === 1))

// Call the main driver.
main()
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
pass 2
fail 1
error 1
\end{lstlisting}



This simple “framework” does what it’s supposed to, but:

\begin{enumerate}

\item 

It doesn’t tell us which tests have passed or failed.



\item 

Those global variables should be consolidated somehow
    so that it’s clear they belong together.



\item 

It doesn’t discover tests on its own.



\item 

We don’t have a way to test things that are supposed to raise \texttt{AssertionError}.
    Putting assertions into code to check that it is behaving correctly
    is called \glossref{defensive programming};
    it’s a good practice,
    but we should make sure those assertions are failing when they’re supposed to,
    just as we should test our smoke detectors every once in a while.



\end{enumerate}

\section{How should we structure test registration?}\label{unit-test-registration}


The next version of our testing tool solves the first two problems in the original
by putting the testing machinery in a class.
It uses the \glossref{Singleton}\index{Singleton pattern}\index{design pattern!Singleton} \glossref{design pattern}
to ensure that only one object of that class is ever created \cite{Osmani2017}.
Singletons are a way to manage global variables that belong together
like the ones we’re using to record tests and their results.
As an extra benefit,
if we decide later that we need several copies of those variables,
we can just construct more instances of the class.


The file \texttt{hope.js} defines the class and exports one instance of it:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'
import caller from 'caller'

class Hope {
  constructor () {
    this.todo = []
    this.passes = []
    this.fails = []
    this.errors = []
  }

  test (comment, callback) {
    this.todo.push([`${caller()}::${comment}`, callback])
  }

  run () {
    this.todo.forEach(([comment, test]) => {
      try {
        test()
        this.passes.push(comment)
      } catch (e) {
        if (e instanceof assert.AssertionError) {
          this.fails.push(comment)
        } else {
          this.errors.push(comment)
        }
      }
    })
  }

}

export default new Hope()
\end{lstlisting}



This strategy relies on two things:

\begin{enumerate}

\item 

\hreffoot{Node}{https://nodejs.org/en/} executes the code in a JavaScript module as it loads it,
    which means that it runs \texttt{new Hope()} and exports the newly-created object.



\item 

Node \glossref{caches}\index{cache!modules}\index{require!caching modules} modules
    so that a given module is only loaded once
    no matter how many times it is imported.
    This ensures that \texttt{new Hope()} really is only called once.



\end{enumerate}


Once a program has imported \texttt{hope},
it can call \texttt{Hope.test} to record a test for later execution
and \texttt{Hope.run} to execute all of the tests registered up until that point
(\figref{unit-test-hope-structure}).

\figpdf{unit-test-hope-structure}{./unit-test/hope-structure.pdf}{Creating a singleton, recording tests, and running them.}{0.6}


\newpage


Finally,
our \texttt{Hope} class can report results as both a terse one-line summary and as a detailed listing.
It can also provide the titles and results of individual tests
so that if someone wants to format them in a different way (e.g., as HTML) they can do so:


\begin{lstlisting}[frame=tblr]
  terse () {
    return this.cases()
      .map(([title, results]) => `${title}: ${results.length}`)
      .join(' ')
  }

  verbose () {
    let report = ''
    let prefix = ''
    for (const [title, results] of this.cases()) {
      report += `${prefix}${title}:`
      prefix = '\n'
      for (const r of results) {
        report += `${prefix}  ${r}`
      }
    }
    return report
  }

  cases () {
    return [
      ['passes', this.passes],
      ['fails', this.fails],
      ['errors', this.errors]]
  }
\end{lstlisting}


\begin{callout}


\subsubsection*{Who’s calling?}


\texttt{Hope.test} uses the \hreffoot{\texttt{caller}}{https://www.npmjs.com/package/caller}\index{caller module} module
to get the name of the function that is registering a test.
Reporting the test’s name helps the user figure out where to start debugging;
getting it via introspection
rather than requiring the user to pass the function’s name as a string
reduces typing
and guarantees that what we report is accurate.
Programmers will often copy, paste, and modify tests;
sooner or later (probably sooner) they will forget to modify
the copy-and-pasted function name being passed into \texttt{Hope.test}
and will then lose time trying to figure out why \texttt{test\_this} is failing
when the failure is actually in \texttt{test\_that}.

\end{callout}

\section{How can we build a command-line interface for testing?}\label{unit-test-cli}


Most programmers don’t enjoy writing tests,
so if we want them to do it,
we have to make it as painless as possible.
A couple of \texttt{import} statements to get \texttt{assert} and \texttt{hope}
and then one function call per test
is about as simple as we can make the tests themselves:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'
import hope from './hope.js'

hope.test('Sum of 1 and 2', () => assert((1 + 2) === 3))
\end{lstlisting}



But that just defines the tests—how will we find them so that we can run them?
One option is to require people to \texttt{import} each of the files containing tests
into another file:

\begin{lstlisting}[frame=tblr]
// all-the-tests.js

import './test-add.js'
import './test-sub.js'
import './test-mul.js'
import './test-div.js'

Hope.run()
...
\end{lstlisting}


\noindent Here,
\texttt{all-the-tests.js} imports other files so that they will register tests
as a \glossref{side effect}\index{side effect!for module registration} via calls to \texttt{hope.test}
and then calls \texttt{Hope.run} to execute them.
It works,
but sooner or later (probably sooner) someone will forget to import one of the test files.


A better strategy is to load test files \glossref{dynamically}\index{dynamic loading}.
While \texttt{import} is usually written as a statement,
it can also be used as an \texttt{async} function
that takes a path as a parameter and loads the corresponding file.
The program \texttt{pray.js} (shown below) does this;
as before,
loading files executes the code they contain,
which registers tests as a side effect:


\begin{lstlisting}[frame=tblr]
import minimist from 'minimist'
import glob from 'glob'
import hope from './hope.js'

const main = async (args) => {
  const options = parse(args)
  if (options.filenames.length === 0) {
    options.filenames = glob.sync(`${options.root}/**/test-*.js`)
  }
  for (const f of options.filenames) {
    await import(f)
  }
  hope.run()
  const result = (options.output === 'terse')
    ? hope.terse()
    : hope.verbose()
  console.log(result)
}


main(process.argv.slice(2))
\end{lstlisting}



By default,
this program finds all files below the current working directory
whose names match the pattern \texttt{test-*.js}
and uses terse output.
Since we may want to look for files somewhere else,
or request verbose output,
the program needs to handle command-line arguments.


The \hreffoot{\texttt{minimist}}{https://www.npmjs.com/package/minimist} module does this
in a way that is consistent with Unix conventions.
Given command-line arguments \emph{after} the program’s name
(i.e., from \texttt{process.argv[2]} onward),
it looks for patterns like \texttt{-x something}
and creates an object with flags as keys and values associated with them.

\begin{callout}


\subsubsection*{Filenames in \texttt{minimist}}


If we use a command line like \texttt{pray.js -v something.js},
then \texttt{something.js} becomes the value of \texttt{-v}.
To indicate that we want \texttt{something.js} added to the list of trailing filenames
associated with the special key \texttt{\_} (a single underscore),
we have to write \texttt{pray.js -v -- something.js}.
The double dash is a common Unix convention for signalling the end of parameters.

\end{callout}


Our \glossref{test runner}\index{test runner}\index{unit test!test runner} is now complete,
so we can try it out with some files containing tests that pass, fail, and contain errors:


\begin{lstlisting}[frame=shadowbox]
node pray.js -v
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
passes:
  /u/stjs/unit-test/test-add.js::Sum of 1 and 2
  /u/stjs/unit-test/test-sub.js::Difference of 1 and 2
fails:
  /u/stjs/unit-test/test-div.js::Quotient of 1 and 0
  /u/stjs/unit-test/test-mul.js::Product of 1 and 2
errors:
  /u/stjs/unit-test/test-missing.js::Sum of x and 0
\end{lstlisting}


\begin{callout}


\subsubsection*{Infinity is allowed}


\texttt{test-div.js} contains the line:

\begin{lstlisting}[frame=tblr]
hope.test('Quotient of 1 and 0', () => assert((1 / 0) === 0))
\end{lstlisting}


This test counts as a failure rather than an error
because thinks the result of dividing by zero is the special value \texttt{Infinity}
rather than an arithmetic error.

\end{callout}


Loading modules dynamically so that they can register something for us to call later
is a common pattern in many programming languages.
Control flow goes back and forth between the framework and the module being loaded
as this happens
so we must specify the \glossref{lifecycle}\index{lifecycle!of unit test}\index{unit test!lifecycle} of the loaded modules quite carefully.
\figref{unit-test-lifecycle} illustrates what happens
when a pair of files \texttt{test-add.js} and \texttt{test-sub.js} are loaded by our framework:

\begin{enumerate}

\item \texttt{pray} loads \texttt{hope.js}.

\item Loading \texttt{hope.js} creates a single instance of the class \texttt{Hope}.

\item \texttt{pray} uses \texttt{glob} to find files with tests.

\item \texttt{pray} loads \texttt{test-add.js} using \texttt{import} as a function.

\item As \texttt{test-add.js} runs, it loads \texttt{hope.js}.
    Since \texttt{hope.js} is already loaded, this does not create a new instance of \texttt{Hope}.

\item \texttt{test-add.js} uses \texttt{hope.test} to register a test (which does not run yet).

\item \texttt{pray} then loads \texttt{test-sub.js}...

\item ...which loads \texttt{Hope}...

\item ...then registers a test.

\item \texttt{pray} can now ask the unique instance of \texttt{Hope} to run all of the tests,
     then get a report from the \texttt{Hope} singleton and display it.

\end{enumerate}

\figpdf{unit-test-lifecycle}{./unit-test/lifecycle.pdf}{Lifecycle of dynamically-discovered unit tests.}{0.6}

\section{Exercises}\label{unit-test-exercises}

\subsection*{Asynchronous globbing}


Modify \texttt{pray.js} to use the asynchronous version of \texttt{glob} rather than \texttt{glob.sync}.

\subsection*{Timing tests}


Install the \hreffoot{\texttt{microtime}}{https://www.npmjs.com/package/microtime} package and then modify the \texttt{dry-run.js} example
so that it records and reports the execution times for tests.

\subsection*{Approximately equal}

\begin{enumerate}

\item 

Write a function \texttt{assertApproxEqual} that does nothing if two values are within a certain tolerance of each other
    but throws an exception if they are not:

\begin{lstlisting}[frame=tblr]
// throws exception
assertApproxEqual(1.0, 2.0, 0.01, 'Values are too far apart')

// does not throw
assertApproxEqual(1.0, 2.0, 10.0, 'Large margin of error')
\end{lstlisting}



\item 

Modify the function so that a default tolerance is used if none is specified:

\begin{lstlisting}[frame=tblr]
// throws exception
assertApproxEqual(1.0, 2.0, 'Values are too far apart')

// does not throw
assertApproxEqual(1.0, 2.0, 'Large margin of error', 10.0)
\end{lstlisting}



\item 

Modify the function again so that it checks the \glossref{relative error}
    instead of the \glossref{absolute error}.
    (The relative error is the absolute value of the difference between the actual and expected value,
    divided by the absolute value.)



\end{enumerate}

\subsection*{Rectangle overlay}


A windowing application represents rectangles using objects with four values:
\texttt{x} and \texttt{y} are the coordinates of the lower-left corner,
while \texttt{w} and \texttt{h} are the width and height.
All values are non-negative:
the lower-left corner of the screen is at \texttt{(0, 0)}
and the screen’s size is \texttt{WIDTH}x\texttt{HEIGHT}.

\begin{enumerate}

\item 

Write tests to check that an object represents a valid rectangle.



\item 

The function \texttt{overlay(a, b)} takes two rectangles and returns either
    a new rectangle representing the region where they overlap or \texttt{null} if they do not overlap.
    Write tests to check that \texttt{overlay} is working correctly.



\item 

Do your tests assume that two rectangles that touch on an edge overlap or not?
    What about two rectangles that only touch at a single corner?



\end{enumerate}

\subsection*{Selecting tests}


Modify \texttt{pray.js} so that if the user provides \texttt{-s pattern} or \texttt{--select pattern}
then the program only runs tests in files that contain the string \texttt{pattern} in their name.

\subsection*{Tagging tests}


Modify \texttt{hope.js} so that users can optionally provide an array of strings to tag tests:

\begin{lstlisting}[frame=tblr]
hope.test('Difference of 1 and 2',
          () => assert((1 - 2) === -1),
          ['math', 'fast'])
\end{lstlisting}


Then modify \texttt{pray.js} so that if users specify either \texttt{-t tagName} or \texttt{--tag tagName}
only tests with that tag are run.

\subsection*{Mock objects}


A mock object is a simplified replacement for part of a program
whose behavior is easier to control and predict than the thing it is replacing.
For example,
we may want to test that our program does the right thing if an error occurs while reading a file.
To do this,
we write a function that wraps \texttt{fs.readFileSync}:

\begin{lstlisting}[frame=tblr]
const mockReadFileSync = (filename, encoding = 'utf-8') => {
  return fs.readFileSync(filename, encoding)
}
\end{lstlisting}


\noindent and then modify it so that it throws an exception under our control.
For example,
if we define \texttt{MOCK\_READ\_FILE\_CONTROL} like this:

\begin{lstlisting}[frame=tblr]
const MOCK_READ_FILE_CONTROL = [false, false, true, false, true]
\end{lstlisting}


\noindent then the third and fifth calls to \texttt{mockReadFileSync} throw an exception instead of reading data,
as do any calls after the fifth.
Write this function.

\subsection*{Setup and teardown}


Testing frameworks often allow programmers to specify a \texttt{setup} function
that is to be run before each test
and a corresponding \texttt{teardown} function
that is to be run after each test.
(\texttt{setup} usually re-creates complicated test fixtures,
while \texttt{teardown} functions are sometimes needed to clean up after tests,
e.g., to close database connections or delete temporary files.)


Modify the testing framework in this chapter so that
if a file of tests contains something like this:

\begin{lstlisting}[frame=tblr]
const createFixtures = () => {
  ...do something...
}

hope.setup(createFixtures)
\end{lstlisting}


\noindent then the function \texttt{createFixtures} will be called
exactly once before each test in that file.
Add a similar way to register a teardown function with \texttt{hope.teardown}.

\subsection*{Multiple tests}


Add a method \texttt{hope.multiTest} that allows users to specify
multiple test cases for a function at once.
For example, this:

\begin{lstlisting}[frame=tblr]
hope.multiTest('check all of these`, functionToTest, [
  [['arg1a', 'arg1b'], 'result1'],
  [['arg2a', 'arg2b'], 'result2'],
  [['arg3a', 'arg3b'], 'result3']
])
\end{lstlisting}


\noindent should be equivalent to this:

\begin{lstlisting}[frame=tblr]
hope.test('check all of these 0',
  () => assert(functionToTest('arg1a', 'arg1b') === 'result1')
)
hope.test('check all of these 1',
  () => assert(functionToTest('arg2a', 'arg2b') === 'result2')
)
hope.test('check all of these 2',
  () => assert(functionToTest('arg3a', 'arg3b') === 'result3')
)
\end{lstlisting}

\subsection*{Assertions for sets and maps}

\begin{enumerate}

\item 

Write functions \texttt{assertSetEqual} and \texttt{assertMapEqual}
    that check whether two instances of \texttt{Set} or two instances of \texttt{Map} are equal.



\item 

Write a function \texttt{assertArraySame}
    that checks whether two arrays have the same elements,
    even if those elements are in different orders.



\end{enumerate}

\subsection*{Testing promises}


Modify the unit testing framework to handle \texttt{async} functions,
so that:

\begin{lstlisting}[frame=tblr]
hope.test('delayed test', async () => {...})
\end{lstlisting}


\noindent does the right thing.
(Note that you can use \texttt{typeof} to determine whether the object given to \texttt{hope.test}
is a function or a promise.)

\chapter{File Backup}\label{file-backup}


\noindent 
  Terms defined: \glossref{Application Programming Interface}, \glossref{collision}, \glossref{comma-separated values}, \glossref{Coordinated Universal Time}, \glossref{cryptographic hash function}, \glossref{data migration}, \glossref{handler}, \glossref{hash code}, \glossref{hash function}, \glossref{JavaScript Object Notation}, \glossref{mock object}, \glossref{pipe}, \glossref{race condition}, \glossref{SHA-1 hash}, \glossref{stream}, \glossref{streaming API}, \glossref{Time of check/time of use}, \glossref{timestamp}, \glossref{version control system}



Now that we can test software, we have something worth saving.
A \glossref{version control system}\index{version control system}
like \hreffoot{Git}{https://git-scm.com/}\index{Git}\index{version control system!Git}
keeps track of changes to files
so that we can recover old versions if we want to.
Its core is a way to archive files that:

\begin{enumerate}

\item records which versions of which files existed at the same time
    (so that we can go back to a consistent previous state), and

\item stores any particular version of a file only once,
    so that we don’t waste disk space.

\end{enumerate}


In this chapter we will build a tool for doing both tasks.
It won’t do everything Git does:
in particular, it won’t let us create and merge branches.
If you would like to know how that works,
please see \hreffoot{Mary Rose Cook’s}{https://maryrosecook.com/}\index{Cook, Mary Rose} excellent \hreffoot{Gitlet}{http://gitlet.maryrosecook.com/} project.

\section{How can we uniquely identify files?}\label{file-backup-unique}


To avoid storing redundant copies of files,
we need a way to tell when two files contain the same data.
We can’t rely on names because files can be renamed or moved over time;
we could compare the files byte-by-byte,
but a quicker way is to use a \glossref{hash function}\index{hash function}
that turns arbitrary data into a fixed-length string of bits
(\figref{file-backup-hash-function}).

\figpdf{file-backup-hash-function}{./file-backup/hash-function.pdf}{How hash functions speed up lookup.}{0.6}


A hash function always produces the same \glossref{hash code}\index{hash code} for a given input.
A \glossref{cryptographic hash function}\index{cryptographic hash function}\index{hash function!cryptographic}
has two extra properties:

\begin{enumerate}

\item 

The output depends on the entire input:
    changing even a single byte results in a different hash code.



\item 

The outputs look like random numbers:
    they are unpredictable and evenly distributed
    (i.e., the odds of getting any specific hash code are the same).



\end{enumerate}


It’s easy to write a bad hash function,
but very hard to write one that qualifies as cryptographic.
We will therefore use a library to calculate 160-bit \glossref{SHA-1}\index{hash code!SHA-1}\index{SHA-1 hash code} hashes for our files.
These are not random enough to keep data secret from a patient, well-funded attacker,
but that’s not what we’re using them for:
we just want hashes that are random to make \glossref{collision}\index{hash function!collision}\index{collision (in hashing)} extremely unlikely.

\begin{callout}


\subsubsection*{The Birthday Problem}


The odds that two people share a birthday are 1/365 (ignoring February 29).
The odds that they \emph{don’t} are therefore 364/365.
When we add a third person,
the odds that they don’t share a birthday with either of the preceding two people are 363/365,
so the overall odds that nobody shares a birthday are (365/365)×(364/365)×(363/365).
If we keep calculating, there’s a 50\% chance of two people sharing a birthday in a group of just 23 people,
and a 99.9\% chance with 70 people.


We can use the same math to calculate how many files we need to hash before there’s a 50\% chance of a collision.
Instead of 365, we use $2^{160}$ (the number of values that are 160 bits long),
and after checking \hreffoot{Wikipedia}{https://en.wikipedia.org/wiki/Birthday\_problem\#A\_simple\_exponentiation}
and doing a few calculations with \hreffoot{Wolfram Alpha}{http://wolframalpha.com}\index{Wolfram Alpha},
we calculate that we would need to have approximately $10^{24}$ files
in order to have a 50\% chance of a collision.
We’re willing to take that risk.

\end{callout}


\hreffoot{Node’s}{https://nodejs.org/en/} \hreffoot{\texttt{crypto}}{https://nodejs.org/api/crypto.html} module provides tools to create a SHA-1 hash.
To use them,
we create an object that keeps track of the current state of the hashing calculations,
tell it how we want to encode (or represent) the hash value,
and then feed it some bytes.
When we are done,
we call its \texttt{.end} method
and then use its \texttt{.read} method to get the final result:


\begin{lstlisting}[frame=tblr]
import crypto from 'crypto'

// create a SHA1 hasher
const hash = crypto.createHash('sha1')

// encode as hex (rather than binary)
hash.setEncoding('hex')

// send it some text
const text = process.argv[2]
hash.write(text)

// signal end of text
hash.end()

// display the result
const sha1sum = hash.read()
console.log(`SHA1 of "${text}" is ${sha1sum}`)
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node hash-text.js something
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
SHA1 of "something" is 1af17e73721dbe0c40011b82ed4bb1a7dbe3ce29
\end{lstlisting}



Hashing a file instead of a fixed string is straightforward:
we just read the file’s contents and pass those characters to the hashing object:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'
import crypto from 'crypto'

const filename = process.argv[2]
const data = fs.readFileSync(filename, 'utf-8')

const hash = crypto.createHash('sha1').setEncoding('hex')
hash.write(data)
hash.end()
const sha1sum = hash.read()

console.log(`SHA1 of "${filename}" is ${sha1sum}`)
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node hash-file.js hash-file.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
SHA1 of "hash-file.js" is c54c8ee3e576770d29ae2d0d73568e5a5c49eac0
\end{lstlisting}



However,
it is more efficient to process the file as a \glossref{stream}:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'
import crypto from 'crypto'

const filename = process.argv[2]
const hash = crypto.createHash('sha1').setEncoding('hex')
fs.createReadStream(filename).pipe(hash)
hash.on('finish', () => {
  const final = hash.read()
  console.log('final', final)
})
console.log('program ends')
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node hash-stream.js hash-stream.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
program ends
final dc9e6c231e243860dace2dbf52845b121062b60e
\end{lstlisting}



\noindent This kind of interface is called
a \glossref{streaming}\index{streaming API}\index{execution!streaming} \glossref{API}
because it is designed to process a stream of data one chunk at a time
rather than requiring all of the data to be in memory at once.
Many applications use streams
so that programs don’t have to read entire (possibly large) files into memory.


To start,
this program asks the \texttt{fs} library to create a reading stream for a file
and to \glossref{pipe} the data from that stream to the hashing object
(\figref{file-backup-streaming}).
It then tells the hashing object what to do when there is no more data
by providing a \glossref{handler}\index{event handler!streaming API}\index{streaming API!event handler} for the “finish” event.
This is called asynchronously:
as the output shows,
the main program ends before the task handling the end of data is scheduled and run.
Most programs also provide a handler for “data” events to do something with each block of data as it comes in;
the \texttt{hash} object in our program does that for us.

\figpdf{file-backup-streaming}{./file-backup/streaming.pdf}{Processing files as streams of chunks.}{0.6}

\section{How can we back up files?}\label{file-backup-backup}


Many files only change occasionally after they’re created, or not at all.
It would be wasteful for a version control system to make copies
each time the user wanted to save a snapshot of a project,
so instead our tool will copy each unique file to something like \texttt{abcd1234.bck},
where \texttt{abcd1234} is a hash of the file’s contents.
It will then store a data structure that records the filenames and hash keys for each snapshot.
The hash keys tell it which unique files are part of the snapshot,
while the filenames tell us what each file’s contents were called when the snapshot was made
(since files can be moved or renamed).
To restore a particular snapshot,
all we have to do is copy the saved \texttt{.bck} files back to where they were
(\figref{file-backup-storage}).

\figpdf{file-backup-storage}{./file-backup/storage.pdf}{Organization of backup file storage.}{0.6}


We can build the tools we need to do this using promises (\chapref{async-programming}).
The main function creates a promise that uses the asynchronous version of \texttt{glob} to find files
and then:

\begin{enumerate}

\item 

checks that entries in the list are actually files;



\item 

reads each file into memory; and



\item 

calculates hashes for those files.



\end{enumerate}


\begin{lstlisting}[frame=tblr]
import fs from 'fs-extra-promise'
import glob from 'glob-promise'
import crypto from 'crypto'

const hashExisting = (rootDir) => {
  const pattern = `${rootDir}/**/*`
  return new Promise((resolve, reject) => {
    glob(pattern, {})
      .then(matches => Promise.all(
        matches.map(path => statPath(path))))
      .then(pairs => pairs.filter(
        ([path, stat]) => stat.isFile()))
      .then(pairs => Promise.all(
        pairs.map(([path, stat]) => readPath(path))))
      .then(pairs => Promise.all(
        pairs.map(([path, content]) => hashPath(path, content))))
      .then(pairs => resolve(pairs))
      .catch(err => reject(err))
  })
}
\end{lstlisting}



\noindent This function uses \texttt{Promise.all}
to wait for the operations on all of the files in the list to complete
before going on to the next step.
A different design would combine stat, read, and hash into a single step
so that each file would be handled independently
and use one \texttt{Promise.all} at the end to bring them all together.


The first two helper functions\index{helper function} that \texttt{hashExisting} relies on
wrap asynchronous operation in promises:


\begin{lstlisting}[frame=tblr]
const statPath = (path) => {
  return new Promise((resolve, reject) => {
    fs.statAsync(path)
      .then(stat => resolve([path, stat]))
      .catch(err => reject(err))
  })
}

const readPath = (path) => {
  return new Promise((resolve, reject) => {
    fs.readFileAsync(path, 'utf-8')
      .then(content => resolve([path, content]))
      .catch(err => reject(err))
  })
}
\end{lstlisting}



The final helper function calculates the hash synchronously,
but we can use \texttt{Promise.all} to wait on those operations finishing anyway:


\begin{lstlisting}[frame=tblr]
const hashPath = (path, content) => {
  const hasher = crypto.createHash('sha1').setEncoding('hex')
  hasher.write(content)
  hasher.end()
  return [path, hasher.read()]
}
\end{lstlisting}



Let’s try running it:


\begin{lstlisting}[frame=tblr]
import hashExisting from './hash-existing-promise.js'

const root = process.argv[2]
hashExisting(root).then(pairs => pairs.forEach(
  ([path, hash]) => console.log(path, hash)
))
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node run-hash-existing-promise.js . | fgrep -v test/ | fgrep -v '~'
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
./backup.js 11422489e11be3d8ff76278503457665f6152ebe
./check-existing-files.js 66b933cf9e792e9a9204171d04e0f8b530ec3f4f
./figures/hash-function.pdf 0eb82de379a95ee2be3f00b38c0102e2f2f8170e
./figures/hash-function.svg 563996575d581f2a08e3e954d7faba4d189d0773
./figures/mock-fs.pdf 0b3bba44e69122ee53bcc9d777c186c84b7c2ff2
...
./x-from-to.md f0f63b3576042dfc0050029ddfcccc3c42fe275d
./x-io-streams.md 1fb4d8b7785c5e7b2f1e29588e2ba28d101ced1a
./x-json-manifests.md 223e0e4167acc6d4d81b76ba1287b90234c95e22
./x-mock-hashes.md 580edfc0cb8eaca4f3700307002ae10ee97af8d2
./x-pre-commit.md b7d945af4554fc0f64b708fe735417bee8b33eef
\end{lstlisting}



The code we have written is clearer than it would be with callbacks
(try rewriting it if you don’t believe this)
but the layer of promises around everything still obscures its meaning.
The same operations are easier to read when written using \texttt{async} and \texttt{await}:


\begin{lstlisting}[frame=tblr]
const statPath = async (path) => {
  const stat = await fs.statAsync(path)
  return [path, stat]
}

const readPath = async (path) => {
  const content = await fs.readFileAsync(path, 'utf-8')
  return [path, content]
}

const hashPath = (path, content) => {
  const hasher = crypto.createHash('sha1').setEncoding('hex')
  hasher.write(content)
  hasher.end()
  return [path, hasher.read()]
}

const hashExisting = async (rootDir) => {
  const pattern = `${rootDir}/**/*`
  const options = {}
  const matches = await glob(pattern, options)
  const stats = await Promise.all(matches.map(path => statPath(path)))
  const files = stats.filter(([path, stat]) => stat.isFile())
  const contents = await Promise.all(
    files.map(([path, stat]) => readPath(path)))
  const hashes = contents.map(
    ([path, content]) => hashPath(path, content))
  return hashes
}
\end{lstlisting}



\noindent This version creates and resolves exactly the same promises as the previous one,
but those promises are created for us automatically by Node.
To check that it works,
let’s run it for the same input files:


\begin{lstlisting}[frame=tblr]
import hashExisting from './hash-existing-async.js'

const root = process.argv[2]
hashExisting(root).then(
  pairs => pairs.forEach(([path, hash]) => console.log(path, hash)))
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node run-hash-existing-async.js . | fgrep -v test/ | fgrep -v '~'
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
./backup.js 11422489e11be3d8ff76278503457665f6152ebe
./check-existing-files.js 66b933cf9e792e9a9204171d04e0f8b530ec3f4f
./figures/hash-function.pdf 0eb82de379a95ee2be3f00b38c0102e2f2f8170e
./figures/hash-function.svg 563996575d581f2a08e3e954d7faba4d189d0773
./figures/mock-fs.pdf 0b3bba44e69122ee53bcc9d777c186c84b7c2ff2
...
./x-from-to.md f0f63b3576042dfc0050029ddfcccc3c42fe275d
./x-io-streams.md 1fb4d8b7785c5e7b2f1e29588e2ba28d101ced1a
./x-json-manifests.md 223e0e4167acc6d4d81b76ba1287b90234c95e22
./x-mock-hashes.md 580edfc0cb8eaca4f3700307002ae10ee97af8d2
./x-pre-commit.md b7d945af4554fc0f64b708fe735417bee8b33eef
\end{lstlisting}


\section{How can we track which files have already been backed up?}\label{file-backup-track}


The second part of our backup tool keeps track of which files have and haven’t been backed up already.
It stores backups in a directory that contains backup files like \texttt{abcd1234.bck}
and files describing the contents of particular snapshots.
The latter are named \texttt{ssssssssss.csv},
where \texttt{ssssssssss} is the \glossref{UTC} \glossref{timestamp} of the backup’s creation
and the \texttt{.csv} extension indicates that the file is formatted as \glossref{comma-separated values}.
(We could store these files as \glossref{JSON}, but CSV is easier for people to read.)

\begin{callout}


\subsubsection*{Time of check/time of use}


Our naming convention for index files will fail if we try to create more than one backup per second.
This might seem very unlikely,
but many faults and security holes are the result of programmers assuming things weren’t going to happen.


We could try to avoid this problem by using a two-part naming scheme \texttt{ssssssss-a.csv},
\texttt{ssssssss-b.csv}, and so on,
but this leads to a \glossref{race condition}\index{race condition}
called \glossref{time of check/time of use}\index{race condition!time of check/time of use}\index{time of check/time of use}.
If two users run the backup tool at the same time,
they will both see that there isn’t a file (yet) with the current timestamp,
so they will both try to create the first one.

\end{callout}


\begin{lstlisting}[frame=tblr]
import glob from 'glob-promise'
import path from 'path'

const findNew = async (rootDir, pathHashPairs) => {
  const hashToPath = pathHashPairs.reduce((obj, [path, hash]) => {
    obj[hash] = path
    return obj
  }, {})

  const pattern = `${rootDir}/*.bck`
  const options = {}
  const existingFiles = await glob(pattern, options)

  existingFiles.forEach(filename => {
    const stripped = path.basename(filename).replace(/\.bck$/, '')
    delete hashToPath[stripped]
  })

  return hashToPath
}

export default findNew
\end{lstlisting}



To test our program,
let’s manually create testing directories with manufactured (shortened) hashes:


\begin{lstlisting}[frame=shadowbox]
tree test
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
test
├── bck-0-csv-0
├── bck-1-csv-1
│   ├── 0001.csv
│   └── abcd1234.bck
├── bck-4-csv-2
│   ├── 0001.csv
│   ├── 3028.csv
│   ├── 3456cdef.bck
│   ├── abcd1234.bck
│   └── bcde2345.bck
├── test-backup.js
├── test-find-mock.js
└── test-find.js

3 directories, 10 files
\end{lstlisting}



We use \hreffoot{Mocha}{https://mochajs.org/}\index{Mocha} to manage our tests.
Every test is an \texttt{async} function;
Mocha automatically waits for them all to complete before reporting results.
To run them,
we add the line:

\begin{lstlisting}[frame=tblr]
"test": "mocha */test/test-*.js"
\end{lstlisting}


\noindent in the \texttt{scripts} section of our project’s \texttt{package.json} file
so that when we run \texttt{npm run test},
Mocha looks for files in \texttt{test} sub-directories of the directories holding our lessons.


Here are our first few tests:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import findNew from '../check-existing-files.js'

describe('pre-existing hashes and actual filesystem', () => {
  it('finds no pre-existing files when none given or exist', async () => {
    const expected = {}
    const actual = await findNew('file-backup/test/bck-0-csv-0', [])
    assert.deepStrictEqual(expected, actual,
      'Expected no files')
  })

  it('finds some files when one is given and none exist', async () => {
    const check = [['somefile.txt', '9876fedc']]
    const expected = { '9876fedc': 'somefile.txt' }
    const actual = await findNew('file-backup/test/bck-0-csv-0', check)
    assert.deepStrictEqual(expected, actual,
      'Expected one file')
  })

  it('finds nothing needs backup when there is a match', async () => {
    const check = [['alpha.js', 'abcd1234']]
    const expected = {}
    const actual = await findNew('file-backup/test/bck-1-csv-1', check)
    assert.deepStrictEqual(expected, actual,
      'Expected no files')
  })

  it('finds something needs backup when there is a mismatch', async () => {
    const check = [['alpha.js', 'a1b2c3d4']]
    const expected = { a1b2c3d4: 'alpha.js' }
    const actual = await findNew('file-backup/test/bck-1-csv-1', check)
    assert.deepStrictEqual(expected, actual,
      'Expected one file')
  })

  it('finds mixed matches', async () => {
    const check = [
      ['matches.js', '3456cdef'],
      ['matches.txt', 'abcd1234'],
      ['mismatch.txt', '12345678']
    ]
    const expected = { 12345678: 'mismatch.txt' }
    const actual = await findNew('file-backup/test/bck-4-csv-2', check)
    assert.deepStrictEqual(expected, actual,
      'Expected one file')
  })
})
\end{lstlisting}



\noindent and here is Mocha’s report:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test
> mocha */test/test-*.js "-g" "pre-existing hashes"


  pre-existing hashes and actual filesystem
    ✓ finds no pre-existing files when none given or exist
    ✓ finds some files when one is given and none exist
    ✓ finds nothing needs backup when there is a match
    ✓ finds something needs backup when there is a mismatch
    ✓ finds mixed matches


  5 passing (16ms)
\end{lstlisting}


\section{How can we test code that modifies files?}\label{file-backup-test}


The final thing our tool needs to do
is copy the files that need copying and create a new index file.
The code itself will be relatively simple,
but testing will be complicated by the fact
that our tests will need to create directories and files before they run
and then delete them afterward
(so that they don’t contaminate subsequent tests).


A better approach is to use a \glossref{mock object}\index{mock object!for testing}\index{unit test!using mock object}
instead of the real filesystem.
A mock object has the same interface as the function, object, class, or library that it replaces,
but is designed to be used solely for testing.
Node’s \hreffoot{\texttt{mock-fs}}{https://www.npmjs.com/package/mock-fs} library provides the same functions as the \texttt{fs} library,
but stores everything in memory
(\figref{file-backup-mock-fs}).
This prevents our tests from accidentally disturbing the filesystem,
and also makes tests much faster
(since in-memory operations are thousands of times faster than operations that touch the disk).

\figpdf{file-backup-mock-fs}{./file-backup/mock-fs.pdf}{Using a mock filesystem to simplify testing.}{0.6}


We can create a mock filesystem by giving the library a JSON description of
the files and what they should contain:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'
import mock from 'mock-fs'

import findNew from '../check-existing-files.js'

describe('checks for pre-existing hashes using mock filesystem', () => {
  beforeEach(() => {
    mock({
      'bck-0-csv-0': {},
      'bck-1-csv-1': {
        '0001.csv': 'alpha.js,abcd1234',
        'abcd1234.bck': 'alpha.js content'
      },
      'bck-4-csv-2': {
        '0001.csv': ['alpha.js,abcd1234',
          'beta.txt,bcde2345'].join('\n'),
        '3024.csv': ['alpha.js,abcd1234',
          'gamma.png,3456cdef',
          'subdir/renamed.txt,bcde2345'].join('\n'),
        '3456cdef.bck': 'gamma.png content',
        'abcd1234.bck': 'alpha content',
        'bcde2345.bck': 'beta.txt became subdir/renamed.txt'
      }
    })
  })

  afterEach(() => {
    mock.restore()
  })

})
\end{lstlisting}



\noindent Mocha\index{Mocha!beforeEach} automatically calls \texttt{beforeEach} before running each tests,
and \texttt{afterEach}\index{Mocha!afterEach} after each tests completes
(which is yet another protocol\index{protocol!for unit testing}).
All of the tests stay exactly the same,
and since \texttt{mock-fs} replaces the functions in the standard \texttt{fs} library with its own,
nothing in our application needs to change either.


We are finally ready to write the program that actually backs up files:


\begin{lstlisting}[frame=tblr]
import fs from 'fs-extra-promise'

import hashExisting from './hash-existing-async.js'
import findNew from './check-existing-files.js'

const backup = async (src, dst, timestamp = null) => {
  if (timestamp === null) {
    timestamp = Math.round((new Date()).getTime() / 1000)
  }
  timestamp = String(timestamp).padStart(10, '0')

  const existing = await hashExisting(src)
  const needToCopy = await findNew(dst, existing)
  await copyFiles(dst, needToCopy)
  await saveManifest(dst, timestamp, existing)
}

const copyFiles = async (dst, needToCopy) => {
  const promises = Object.keys(needToCopy).map(hash => {
    const srcPath = needToCopy[hash]
    const dstPath = `${dst}/${hash}.bck`
    fs.copyFileAsync(srcPath, dstPath)
  })
  return Promise.all(promises)
}

const saveManifest = async (dst, timestamp, pathHash) => {
  pathHash = pathHash.sort()
  const content = pathHash.map(
    ([path, hash]) => `${path},${hash}`).join('\n')
  const manifest = `${dst}/${timestamp}.csv`
  fs.writeFileAsync(manifest, content, 'utf-8')
}

export default backup
\end{lstlisting}



The tests for this are more complicated than tests we have written previously
because we want to check with actual file hashes.
Let’s set up some fixtures to run tests on:


\begin{lstlisting}[frame=tblr]
import backup from '../backup.js'

const hashString = (data) => {
  const hasher = crypto.createHash('sha1').setEncoding('hex')
  hasher.write(data)
  hasher.end()
  return hasher.read()
}

const Contents = {
  aaa: 'AAA',
  bbb: 'BBB',
  ccc: 'CCC'
}

const Hashes = Object.keys(Contents).reduce((obj, key) => {
  obj[key] = hashString(Contents[key])
  return obj
}, {})

const Fixture = {
  source: {
    'alpha.txt': Contents.aaa,
    'beta.txt': Contents.bbb,
    gamma: {
      'delta.txt': Contents.ccc
    }
  },
  backup: {}
}

const InitialBackups = Object.keys(Hashes).reduce((set, filename) => {
  set.add(`backup/${Hashes[filename]}.bck`)
  return set
}, new Set())
\end{lstlisting}



\noindent and then run some tests:


\begin{lstlisting}[frame=tblr]
describe('check entire backup process', () => {
  beforeEach(() => {
    mock(Fixture)
  })

  afterEach(() => {
    mock.restore()
  })

  it('creates an initial CSV manifest', async () => {
    await backup('source', 'backup', 0)

    assert.strictEqual((await glob('backup/*')).length, 4,
      'Expected 4 files')

    const actualBackups = new Set(await glob('backup/*.bck'))
    assert.deepStrictEqual(actualBackups, InitialBackups,
      'Expected 3 backup files')

    const actualManifests = await glob('backup/*.csv')
    assert.deepStrictEqual(actualManifests, ['backup/0000000000.csv'],
      'Expected one manifest')
  })

  it('does not duplicate files unnecessarily', async () => {
    await backup('source', 'backup', 0)
    assert.strictEqual((await glob('backup/*')).length, 4,
      'Expected 4 files after first backup')

    await backup('source', 'backup', 1)
    assert.strictEqual((await glob('backup/*')).length, 5,
      'Expected 5 files after second backup')
    const actualBackups = new Set(await glob('backup/*.bck'))
    assert.deepStrictEqual(actualBackups, InitialBackups,
      'Expected 3 backup files after second backup')

    const actualManifests = (await glob('backup/*.csv')).sort()
    assert.deepStrictEqual(actualManifests,
      ['backup/0000000000.csv', 'backup/0000000001.csv'],
      'Expected two manifests')
  })

  it('adds a file as needed', async () => {
    await backup('source', 'backup', 0)
    assert.strictEqual((await glob('backup/*')).length, 4,
      'Expected 4 files after first backup')

    await fs.writeFileAsync('source/newfile.txt', 'NNN')
    const hashOfNewFile = hashString('NNN')

    await backup('source', 'backup', 1)
    assert.strictEqual((await glob('backup/*')).length, 6,
      'Expected 6 files after second backup')
    const expected = new Set(InitialBackups)
      .add(`backup/${hashOfNewFile}.bck`)
    const actualBackups = new Set(await glob('backup/*.bck'))
    assert.deepStrictEqual(actualBackups, expected,
      'Expected 4 backup files after second backup')

    const actualManifests = (await glob('backup/*.csv')).sort()
    assert.deepStrictEqual(actualManifests,
      ['backup/0000000000.csv', 'backup/0000000001.csv'],
      'Expected two manifests')
  })
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test
> mocha */test/test-*.js "-g" "check entire backup process"



  check entire backup process
    ✓ creates an initial CSV manifest
    ✓ does not duplicate files unnecessarily
    ✓ adds a file as needed


  3 passing (18ms)
\end{lstlisting}


\begin{callout}


\subsubsection*{Design for test}


One of the best ways—maybe \emph{the} best way—to evaluate software design
is by thinking about testability\index{testability!as design criterion}\index{software design!testability} \cite{Feathers2004}.
We were able to use a mock filesystem instead of a real one
because the filesystem has a well-defined API
that is provided to us in a single library,
so replacing it is a matter of changing one thing in one place.
If you have to change several parts of your code in order to test it,
the code is telling you to consolidate those parts into one component.

\end{callout}

\section{Exercises}\label{file-backup-exercises}

\subsection*{Odds of collision}


If hashes were only 2 bits long,
then the chances of collision with each successive file
assuming no previous collision are:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Number of Files}} & \textbf{\underline{Odds of Collision}} \\
1 & 0\% \\
2 & 25\% \\
3 & 50\% \\
4 & 75\% \\
5 & 100\% \\
\end{tabular}

\vspace{\baselineskip}


A colleague of yours says this means that if we hash four files,
there’s only a 75\% chance of any collision occurring.
What are the actual odds?

\subsection*{Streaming I/O}


Write a small program using \texttt{fs.createReadStream} and \texttt{fs.createWriteStream}
that copies a file piece-by-piece
instead of reading it into memory and then writing it out again.

\subsection*{Sequencing backups}


Modify the backup program so that manifests are numbered sequentially
as \texttt{00000001.csv}, \texttt{00000002.csv}, and so on
rather than being timestamped.
Why doesn’t this solve the time of check/time of use race condition mentioned earlier?

\subsection*{JSON manifests}

\begin{enumerate}

\item 

Modify \texttt{backup.js} so that it can save JSON manifests as well as CSV manifests
    based on a command-line flag.



\item 

Write another program called \texttt{migrate.js} that converts a set of manifests
    from CSV to JSON.
    (The program’s name comes from the term \glossref{data migration}.)



\item 

Modify \texttt{backup.js} programs so that each manifest stores the user name of the person who created it
    along with file hashes,
    and then modify \texttt{migrate.js} to transform old files into the new format.



\end{enumerate}

\subsection*{Mock hashes}

\begin{enumerate}

\item 

Modify the file backup program so that it uses a function called \texttt{ourHash} to hash files.



\item 

Create a replacement that returns some predictable value, such as the first few characters of the data.



\item 

Rewrite the tests to use this function.



\end{enumerate}


\noindent How did you modify the main program so that the tests could control which hashing function is used?

\subsection*{Comparing manifests}


Write a program \texttt{compare-manifests.js} that reads two manifest files and reports:

\begin{itemize}

\item 

Which files have the same names but different hashes
    (i.e., their contents have changed).



\item 

Which files have the same hashes but different names
    (i.e., they have been renamed).



\item 

Which files are in the first hash but neither their names nor their hashes are in the second
    (i.e., they have been deleted).



\item 

Which files are in the second hash but neither their names nor their hashes are in the first
    (i.e., they have been added).



\end{itemize}

\subsection*{From one state to another}

\begin{enumerate}

\item 

Write a program called \texttt{from-to.js} that takes the name of a directory
    and the name of a manifest file
    as its command-line arguments,
    then adds, removes, and/or renames files in the directory
    to restore the state described in the manifest.
    The program should only perform file operations when it needs to,
    e.g.,
    it should not delete a file and re-add it if the contents have not changed.



\item 

Write some tests for \texttt{from-to.js} using Mocha and \texttt{mock-fs}.



\end{enumerate}

\subsection*{File history}

\begin{enumerate}

\item 

Write a program called \texttt{file-history.js}
    that takes the name of a file as a command-line argument
    and displays the history of that file
    by tracing it back in time through the available manifests.



\item 

Write tests for your program using Mocha and \texttt{mock-fs}.



\end{enumerate}

\subsection*{Pre-commit hooks}


Modify \texttt{backup.js} to load and run a function called \texttt{preCommit} from a file called \texttt{pre-commit.js}
stored in the root directory of the files being backed up.
If \texttt{preCommit} returns \texttt{true}, the backup proceeds;
if it returns \texttt{false} or throws an exception,
no backup is created.

\chapter{Data Tables}\label{data-table}


\noindent 
  Terms defined: \glossref{character encoding}, \glossref{column-major storage}, \glossref{data frame}, \glossref{fixed-width (of strings)}, \glossref{garbage collection}, \glossref{heterogeneous}, \glossref{homogeneous}, \glossref{immutable}, \glossref{index (in a database)}, \glossref{JavaScript Object Notation}, \glossref{join}, \glossref{pad (a string)}, \glossref{row-major storage}, \glossref{sparse matrix}, \glossref{SQL}, \glossref{tagged data}, \glossref{test harness}



\chapref{systems-programming} said that
operations in memory are thousands of times faster than operations that touch the filesystem,
but what about different in-memory operations—how do they compare with each other?
Putting it another way,
how can we tell which of several designs is going to be the most efficient?


The best answer is to conduct some experiments\index{experiments}.
To see how to do this,
we will take a look at several ways to implement data tables
with one or more named columns and zero or more rows.
Each row has one value for each column,
and all the values in a column have the same type
(\figref{data-table-conceptual}).
Data tables appear over and over again in programming,
from spreadsheets and databases
to the \glossref{data frames}\index{data frame} in
R’s\index{R} \hreffoot{tidyverse}{https://www.tidyverse.org/}\index{tidyverse} packages,
\hreffoot{Python’s}{https://www.python.org/}\index{Python} \hreffoot{Pandas}{https://pandas.pydata.org/}\index{Pandas} library,
or the \hreffoot{DataForge}{http://www.data-forge-js.com/}\index{DataForge} library for JavaScript \cite{Davis2018}.

\figpdf{data-table-conceptual}{./data-table/conceptual.pdf}{The structure of a data table.}{0.6}


The key operations on data tables are those provided by \glossref{SQL}\index{SQL}:
filter, select, summarize, and join.
These can be implemented in about 500 lines of code,
but their performance depends on how the data table is stored.

\section{How can we implement data tables?}\label{data-table-implement}


One way to store a table is \glossref{row-major}\index{row-major storage order}\index{storage order!row-major} order,
in which the values in each row are stored together in memory.
This is sometimes also called \glossref{heterogeneous}\index{heterogeneous storage}\index{storage!heterogeneous} storage
because each “unit” of storage can contain values of different types.
We can implement this design in JavaScript using an array of objects,
each of which has the same keys
(\figref{data-table-storage-order}).


Another option is \glossref{column-major}\index{column-major storage order}\index{storage order!column-major}
or \glossref{homogeneous}\index{homogeneous storage}\index{storage!homogeneous} order,
in which all the values in a column are stored together.
In JavaScript,
this could be implemented using an object
whose members are all arrays of the same length.

\figpdf{data-table-storage-order}{./data-table/storage-order.pdf}{Row-major storage vs. column-major storage for data tables.}{0.6}


To find out which is better
we will construct one of each,
try some operations,
record their execution times and memory use,
and then compare them.
Crucially,
the answer will depend on both the implementations themselves
and on what mix of operations we measure.
For example,
if one strategy works better for filter and another for select,
the ratio of filters to selects may determine which is “best”.

\begin{callout}


\subsubsection*{Immutability}


All of our implementations will treat each data table as \glossref{immutable}\index{immutable data}:
once we have created it,
we will not modify its contents.
This doesn’t actually have much impact on performance
an makes the programming easier and safer,
since shared data structures are a rich source of bugs.

\end{callout}


For our first experiment,
let’s build a row-major table with some number of columns.
To keep it simple,
we will use the row indexes to fill the table.


\begin{lstlisting}[frame=tblr]
export const buildRows = (nRows, labels) => {
  const result = []
  for (let iR = 0; iR < nRows; iR += 1) {
    const row = {}
    labels.forEach(label => {
      row[label] = iR
    })
    result.push(row)
  }
  return result
}
\end{lstlisting}



Next,
we write \texttt{filter} and \texttt{select} for tables laid out this way.
We need to provide a callback function to \texttt{filter} to determine which rows to keep
like the callback for \texttt{Array.filter};
for selecting columns,
we provide a list of the keys that identify the columns we want to keep.
We expect filtering to be relatively fast,
since it is recycling\index{recycling data} rows,
while selecting should be relatively slow because we have to construct a new set of arrays
(\figref{data-table-row-ops}).


\begin{lstlisting}[frame=tblr]
const rowFilter = (table, func) => {
  return table.filter(row => func(row))
}

const rowSelect = (table, toKeep) => {
  return table.map(row => {
    const newRow = {}
    toKeep.forEach(label => {
      newRow[label] = row[label]
    })
    return newRow
  })
}
\end{lstlisting}


\figpdf{data-table-row-ops}{./data-table/row-ops.pdf}{Operations on row-major data tables.}{0.6}


Now let’s do the same for column-major storage.
Building the object that holds the columns is straightforward:


\begin{lstlisting}[frame=tblr]
export const buildCols = (nRows, labels) => {
  const result = {}
  labels.forEach(label => {
    result[label] = []
    for (let iR = 0; iR < nRows; iR += 1) {
      result[label].push(iR)
    }
  })
  return result
}
\end{lstlisting}



Filtering is more complex because the values in each row are scattered across several arrays,
but selecting is just a matter of recycling the arrays we want in the new table.
We expect selecting to be relatively fast,
since only the references to the columns need to be copied,
but filtering will be relatively slow since we are constructing multiple new arrays
(\figref{data-table-col-ops}).


\begin{lstlisting}[frame=tblr]
const colFilter = (table, func) => {
  const result = {}
  const labels = Object.keys(result)
  labels.forEach(label => {
    result[label] = []
  })
  for (let iR = 0; iR < table.label_1.length; iR += 1) {
    if (func(table, iR)) {
      labels.forEach(label => {
        result[label].push(table[label][iR])
      })
    }
  }
  return result
}

const colSelect = (table, toKeep) => {
  const result = {}
  toKeep.forEach(label => {
    result[label] = table[label]
  })
  return result
}
\end{lstlisting}


\figpdf{data-table-col-ops}{./data-table/col-ops.pdf}{Operations on column-major data tables.}{0.6}

\begin{callout}


\subsubsection*{Not quite polymorphic}


Our tests would be simpler to write if the two versions of \texttt{filter} and \texttt{select}
took exactly the same parameters,
but the row-testing functions for \texttt{filter} are different
because of the differences in the ways the tables are stored.
We could force them to be the same by (for example)
packing the values for each row in the column-major implementation
into a temporary object
and passing that to the same filtering function we used for the row-major implementation,
but that extra work would bias the performance comparison in row-major’s favor.

\end{callout}

\section{How can we test the performance of our implementations?}\label{data-table-profile}


Now that we have our tables and operations,
we can build a \glossref{test harness}\index{test harness}\index{experiments!test harness} to run those operations
on data tables of varying sizes.
We arbitrarily decide to keep half of the columns and one-third of the rows;
these ratios will affect our decision about which is better,
so if we were doing this for a real application we would test what happens
as these fractions vary.
And as we said earlier,
the relative performance will also depend on the how many filters we do for each select;
our balance should be based on data from whatever application we intend to support.


Our performance measurement program looks like this:


\begin{lstlisting}[frame=tblr]
const RANGE = 3

const main = () => {
  const nRows = parseInt(process.argv[2])
  const nCols = parseInt(process.argv[3])
  const filterPerSelect = parseFloat(process.argv[4])

  const labels = [...Array(nCols).keys()].map(i => `label_${i + 1}`)
  const someLabels = labels.slice(0, Math.floor(labels.length / 2))
  assert(someLabels.length > 0,
    'Must have some labels for select (array too short)')

  const [rowTable, rowSize, rowHeap] = memory(buildRows, nRows, labels)
  const [colTable, colSize, colHeap] = memory(buildCols, nRows, labels)

  const rowFilterTime =
    time(rowFilter, rowTable,
      row => ((row.label_1 % RANGE) === 0))
  const rowSelectTime =
    time(rowSelect, rowTable, someLabels)
  const colFilterTime =
    time(colFilter, colTable,
      (table, iR) => ((table.label_1[iR] % RANGE) === 0))
  const colSelectTime =
    time(colSelect, colTable, someLabels)

  const ratio = calculateRatio(filterPerSelect,
    rowFilterTime, rowSelectTime,
    colFilterTime, colSelectTime)

  const result = {
    nRows,
    nCols,
    filterPerSelect,
    rowSize,
    rowHeap,
    colSize,
    colHeap,
    rowFilterTime,
    rowSelectTime,
    colFilterTime,
    colSelectTime,
    ratio
  }
  console.log(yaml.safeDump(result))
}
\end{lstlisting}



The functions that actually do the measurements
use the \hreffoot{\texttt{microtime}}{https://www.npmjs.com/package/microtime} library to get microsecond level timing
because JavaScript’s \texttt{Date} only gives us millisecond-level resolution.
We use \hreffoot{\texttt{object-sizeof}}{https://www.npmjs.com/package/object-sizeof} to estimate how much memory our structures require;
we also call \texttt{process.memoryUsage()} and look at the \texttt{heapUsed} value
to see how much memory \hreffoot{Node}{https://nodejs.org/en/} is using while the program runs,
but that may be affected by \glossref{garbage collection}
and a host of other factors outside our control.


\begin{lstlisting}[frame=tblr]
const memory = (func, ...params) => {
  const before = process.memoryUsage()
  const result = func(...params)
  const after = process.memoryUsage()
  const heap = after.heapUsed - before.heapUsed
  const size = sizeof(result)
  return [result, size, heap]
}

const time = (func, ...params) => {
  const before = microtime.now()
  func(...params)
  const after = microtime.now()
  return after - before
}

const calculateRatio = (f2S, rFilterT, rSelectT, cFilterT, cSelectT) => {
  return ((f2S * rFilterT) + rSelectT) / ((f2S * cFilterT) + cSelectT)
}
\end{lstlisting}



Let’s run our program for a table with 100 rows and 3 columns and a 3:1 ratio of filter to select:


\begin{lstlisting}[frame=shadowbox]
node table-performance.js 100 3 3
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
nRows: 100
nCols: 3
filterPerSelect: 3
rowSize: 6600
rowHeap: 26512
colSize: 2442
colHeap: 8536
rowFilterTime: 75
rowSelectTime: 111
colFilterTime: 137
colSelectTime: 48
ratio: 0.7320261437908496
\end{lstlisting}



\noindent What if we increase the table size to 10,000 rows by 30 columns with the same 3:1 filter/select ratio?


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
nRows: 10000
nCols: 30
filterPerSelect: 3
rowSize: 7020000
rowHeap: 18392064
colSize: 2400462
colHeap: -3473800
rowFilterTime: 2929
rowSelectTime: 15863
colFilterTime: 4529
colSelectTime: 104
ratio: 1.8004528522386969
\end{lstlisting}



\noindent And if we keep the table size the same but use a 10:1 filter/select ratio?


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
nRows: 10000
nCols: 30
filterPerSelect: 10
rowSize: 7020000
rowHeap: 18287160
colSize: 2400462
colHeap: -3645056
rowFilterTime: 2376
rowSelectTime: 15566
colFilterTime: 4380
colSelectTime: 90
ratio: 0.8960127591706539
\end{lstlisting}


\begin{table}
\begin{tabular}{llll}
\textbf{\underline{value}} & \textbf{\underline{100-03-03}} & \textbf{\underline{10000-30-03}} & \textbf{\underline{10000-30-10}} \\
nRows & 100 & 10000 & 10000 \\
nCols & 3 & 30 & 30 \\
filterPerSelect & 3 & 3 & 10 \\
rowFilterTime & 75 & 2929 & 2376 \\
rowSelectTime & 111 & 15863 & 15566 \\
colFilterTime & 137 & 4529 & 4380 \\
colSelectTime & 48 & 104 & 90 \\
\end{tabular}
\caption{Relative performance of operations on row-major and column-major data tables.}
\label{data-table-performance}
\end{table}



The results in \tblref{data-table-performance} show that column-major storage is better.
It uses less memory (presumably because column labels aren’t duplicated once per row)
and the time required to construct new objects when doing select with row-major storage
outweighs cost of appending to arrays when doing filter with column-major storage.
Unfortunately,
the code for column-major storage is a little more complicated to write,
which is a cost that doesn’t show up in experiments.

\section{What is the most efficient way to save a table?}\label{data-table-save}


Data is valuable,
so we are going to store data tables in files of some kind.
If one storage scheme is much more efficient than another and we are reading or writing frequently,
that could change our mind about which implementation to pick.


Two simple text-based schemes are row-oriented and column-oriented \glossref{JSON}—basically,
just printing the data structures we have.
Let’s run the 10,000×30 test:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
nRows: 10000
nCols: 30
rowStringTime: 57342
rowStringSize: 9393402
colStringTime: 13267
colStringSize: 2934164
\end{lstlisting}



The time needed for the row-major version is almost ten times greater than
that needed for the column-major version;
we assume that the redundant printing of the labels is mostly to blame,
just as redundant storage of the labels was to blame for row-major’s greater memory requirements.


If that diagnosis is correct,
then a packed version of row-major storage ought to be faster.
We save the column headers once,
then copy the data values into an array of arrays and save that:


\begin{lstlisting}[frame=tblr]
const asPackedJson = (table) => {
  const temp = {}
  temp.keys = Object.keys(table[0])
  temp.values = table.map(row => temp.keys.map(k => row[k]))
  return JSON.stringify(temp)
}
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
nRows: 10000
nCols: 30
packedRowStringTime: 29659
packedRowStringSize: 2974084
\end{lstlisting}



These results show that changing layout for storage
is faster than turning the data structure we have into a string.
Again,
we assume this is because copying data takes less time than turning labels into strings over and over,
but column-major storage is still the best approach.

\section{Does binary storage improve performance?}\label{data-table-binary}


Let’s try one more strategy for storing our tables.
JavaScript stores values in \glossref{tagged}\index{tagged data structure} data structures:
some bits define the value’s type
while other bits store the value itself in a type-dependent way
(\figref{data-table-object-storage}).

\figpdf{data-table-object-storage}{./data-table/object-storage.pdf}{How JavaScript uses tagged data structures to store objects.}{0.6}


We can save space by keeping track of the types ourselves
and just storing the bits that represent the values.
JavaScript has an \texttt{ArrayBuffer}\index{ArrayBuffer} class for exactly this purpose.
It stores any value we want as a set of bits;
we then access those bits through a view that presents the data as a particular type,
such as Boolean (one byte per value) or number (64 bits per number).
As \figref{data-table-packed-storage} shows,
we can mix different types of data in a single \texttt{ArrayBuffer},
but it’s up to us to keep track of which bytes belong to which values.

\figpdf{data-table-packed-storage}{./data-table/packed-storage.pdf}{Storing object values as bits with lookup information.}{0.6}


To store a column-major table we will fill an \texttt{ArrayBuffer} with:

\begin{enumerate}

\item 

Two integers that hold the table’s size (number of rows and number of columns).



\item 

A string with the column labels joined by newline characters.
    (We use newlines as a separator because we assume column labels can’t contain them.)



\item 

The numbers themselves.



\end{enumerate}


\begin{lstlisting}[frame=tblr]
const asBinary = (table) => {
  const labels = Object.keys(table)

  const nCols = labels.length
  const nRows = table[labels[0]].length
  const dimensions = new Uint32Array([nCols, nRows])

  const allLabels = labels.join('\n')
  const encoder = new TextEncoder()
  const encodedLabels = encoder.encode(allLabels)

  const dataSize = sizeof(0) * nCols * nRows
  const totalSize =
    dimensions.byteLength + encodedLabels.byteLength + dataSize

  const buffer = new ArrayBuffer(totalSize)
  const result = new Uint8Array(buffer)
  result.set(dimensions, 0)
  result.set(encodedLabels, dimensions.byteLength)

  let current = dimensions.byteLength + encodedLabels.byteLength
  labels.forEach(label => {
    const temp = new Float64Array(table[label])
    result.set(temp, current)
    current += temp.byteLength
  })

  return result
}
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
nRows: 10000
nCols: 30
packedColBinaryTime: 6074
packedColBinarySize: 2400268
\end{lstlisting}



Packing the data table saves time
because copying bits is faster than turning numbers into characters,
but it doesn’t save as much space as expected.
The reason is that double-precision numbers are 8 bytes long,
but because we have chosen simple integer values for our tests,
they can be represented by just 5 characters (which is 10 bytes).
If we had “real” numbers the storage benefit would probably be more pronounced;
once again,
the result of our experiment depends on the test cases we choose.

\begin{callout}


\subsubsection*{Engineering}


If science is the use of the experimental method to investigate the world,
engineering is the use of the experimental method
to investigate and improve the things that people build.
Good software designers collect and analyze data all the time
to find out whether one website design works better than another \cite{Kohavi2020}
or to improve the performance of CPUs \cite{Patterson2017};
a few simple experiments like these can sometimes save weeks or months of effort.

\end{callout}

\section{Exercises}\label{data-table-exercises}

\subsection*{Varying filter behavior}


How does our decision about which storage format is better change
if we keep 1\% of rows when filtering instead of one third?
What if we keep 90\% of rows?

\subsection*{Filtering by strings}


Modify the comparison of filter and select to work with tables
that contain columns of strings instead of columns of numbers
and see how that changes performance.
For testing,
create random 4-letter strings using the characters A-Z
and then filter by:

\begin{itemize}

\item an exact match,

\item strings starting with a specific character, and

\item strings that contain a specific character.

\end{itemize}

\subsection*{Join performance}


A join combines data from two tables based on matching keys.
For example,
if the two tables are:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Key}} & \textbf{\underline{Left}} \\
A & a1 \\
B & b1 \\
C & c1 \\
\end{tabular}

\vspace{\baselineskip}


\noindent and:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Key}} & \textbf{\underline{Right}} \\
A & a2 \\
A & a3 \\
B & b2 \\
\end{tabular}

\vspace{\baselineskip}


\noindent then the join is:


\vspace{\baselineskip}
\begin{tabular}{lll}
\textbf{\underline{Key}} & \textbf{\underline{Left}} & \textbf{\underline{Right}} \\
A & a1 & a2 \\
A & a1 & a3 \\
B & b1 & b2 \\
\end{tabular}

\vspace{\baselineskip}


\noindent Write a test to compare the performance of row-wise vs. column-wise storage
when joining two tables based on matching numeric keys.
Does the answer depend on the fraction of keys that match?

\subsection*{Join optimization}


The simplest way to \glossref{join} two tables is
to look for matching keys using a double loop.
An alternative is to build an \glossref{index} for each table
and then use it to construct matches.
For example, suppose the tables are:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Key}} & \textbf{\underline{Left}} \\
A & a1 \\
B & b1 \\
C & c1 \\
\end{tabular}

\vspace{\baselineskip}


\noindent and:


\vspace{\baselineskip}
\begin{tabular}{ll}
\textbf{\underline{Key}} & \textbf{\underline{Right}} \\
A & a2 \\
A & a3 \\
B & b2 \\
\end{tabular}

\vspace{\baselineskip}


\noindent The first step is to create a \texttt{Map} showing where each key is found in the first table:

\begin{lstlisting}[frame=tblr]
{A: [0], B: [1], C: [2]}
\end{lstlisting}


\noindent The second step is to create a similar \texttt{Map} for the second table:

\begin{lstlisting}[frame=tblr]
{A: [0, 1], B: [2]}
\end{lstlisting}


\noindent We can then loop over the keys in one of the maps,
look up values in the second map,
and construct all of the matches.


Write a function that joins two tables this way.
Is it faster or slower than using a double loop?
How does the answer depend on the number of keys and the fraction that match?

\subsection*{Flipping storage}


Our tests showed that storing row-oriented tables as JSON
is much slower than storing column-oriented tables.
Write a test to determine whether converting a row-oriented table to a column-oriented table
and then saving the latter
is faster than saving the row-oriented table directly.

\subsection*{Sparse storage}


A \glossref{sparse matrix} is one in which most of the values are zero.
Instead of storing them all,
a program can use a map to store non-zero values
and a lookup function to return zero for anything that isn’t stored explicitly:

\begin{lstlisting}[frame=tblr]
def spareMatrixGet(matrix, row, col) => {
  return matrix.contains(row, col)
    ? matrix.get(row, col)
    : 0
}
\end{lstlisting}


The same technique can be used if most of the entries in a data table are missing.
Write a function that creates a sparse table in which a random 5\% of the values are non-zero
and the other 95\% are zero,
then compare the memory requirements and performance of filter and select for this implementation
versus those of row-wise and column-wise storage.

\subsection*{Loading time}


Modify the programs in this section to measure the time required to convert a data table from JSON or binary form
back to a data structure.

\subsection*{Saving fixed-width strings}


To improve performance,
databases often store \glossref{fixed-width} strings,
i.e.,
they limit the length of the strings in a column to some fixed size
and \glossref{pad} strings that are shorter than that.

\begin{enumerate}

\item 

Write a function that takes an array of strings and an integer with
    and creates an \texttt{ArrayBuffer} containing the strings padded to that width.
    The function should throw an exception if any of the strings
    are longer than the specified width.



\item 

Write another function that takes an \texttt{ArrayBuffer} as input
    and returns an array of strings.
    This function should remove the padding
    so that strings shorter than the fixed width are restored to their original form.



\end{enumerate}

\subsection*{Saving variable-width strings}


\glossref{Fixed-width} storage is inefficient for large blocks of text
such as contracts, novels, and resumés,
since padding every document to the length of the longest will probably waste a lot of space.
An alternative way to store these in binary is to save each entry as a (length, text) pair.

\begin{enumerate}

\item 

Write a function that takes a list of strings as input
    and returns an \texttt{ArrayBuffer} containing (length, text) pairs.



\item 

Write another function that takes such an \texttt{ArrayBuffer}
    and returns an array containing the original text.



\item 

Write tests with Mocha to confirm that your functions work correctly.



\end{enumerate}

\subsection*{ASCII storage}


The original ASCII standard specified
a 7-bit \glossref{character encoding} for letters commonly used in English,
and many data files still only use characters whose numeric codes are in the range 0–127.

\begin{enumerate}

\item 

Write a function that takes an array of single-letter strings
    and returns an \texttt{ArrayBuffer} that stores them using one byte per character
    if all of the characters will fit into 7 bits,
    and multiple bytes per character if any of the characters require more than 7 bits.



\item 

Write another function that takes an \texttt{ArrayBuffer} generated by the first function
    and re-creates the array of characters.
    The function must \emph{only} take the \texttt{ArrayBuffer} as an argument,
    so the first element of the \texttt{ArrayBuffer} should indicate
    how to interpret the rest of its contents.



\item 

Write tests with Mocha to check that your functions work correctly.



\end{enumerate}

\chapter{Pattern Matching}\label{pattern-matching}


\noindent 
  Terms defined: \glossref{base class}, \glossref{Chain of Responsibility pattern}, \glossref{child (in a tree)}, \glossref{coupling}, \glossref{depth-first}, \glossref{derived class}, \glossref{Document Object Model}, \glossref{eager matching}, \glossref{greedy algorithm}, \glossref{lazy matching}, \glossref{node}, \glossref{Open-Closed Principle}, \glossref{polymorphism}, \glossref{query selector}, \glossref{regular expression}, \glossref{scope creep}, \glossref{test-driven development}



We have been globbing to match filenames against patterns since \chapref{systems-programming}.
This lesson will explore how that works
by building a simple version of the \glossref{regular expressions}\index{regular expression}
used to match text in everything from editor and shell commands to web scrapers.
Our approach is inspired by \hreffoot{Brian Kernighan’s}{https://en.wikipedia.org/wiki/Brian\_Kernighan}\index{Kernighan, Brian} entry
in \cite{Oram2007}.


Regular expressions have inspired pattern matching for many other kinds of data,
such as \glossref{query selectors}\index{query selector (for HTML)} for HTML.
They are easier to understand and implement than patterns for matching text,
so we will start by looking at them.

\section{How can we match query selectors?}\label{pattern-matching-selectors}


Programs stores HTML pages in memory using a \glossref{document object model}\index{DOM}\index{Document Object Model} or DOM.
Each element in the page,
such as a heading and or paragraph,
is a \glossref{nodes};
the \glossref{children} of a node are the elements it contains
(\figref{pattern-matching-dom-tree}).

\figpdf{pattern-matching-dom-tree}{./pattern-matching/dom-tree.pdf}{Representing an HTML document as a tree.}{0.6}


The first step is to define the patterns we want to support
(\tblref{pattern-matching-supported}).
According to this grammar,
\texttt{blockquote\#important p.highlight} is a highlighted paragraph inside the blockquote whose ID is \texttt{"important"}.
To find elements in a page that match it,
our \texttt{select} function breaks the query into pieces
and uses \texttt{firstMatch} to search recursively down the document tree
until all the selectors in the query string have matched or no matches have been found
(\figref{pattern-matching-query-selectors}).

\begin{table}
\begin{tabular}{ll}
\textbf{\underline{Meaning}} & \textbf{\underline{Selector}} \\
Element with tag \texttt{"elt"} & \texttt{elt} \\
Element with \texttt{class="cls"} & \texttt{.cls} \\
Element with \texttt{id="ident"} & \texttt{\#ident} \\
\texttt{child} element inside a \texttt{parent} element & \texttt{parent child} \\
\end{tabular}
\caption{Supported patterns.}
\label{pattern-matching-supported}
\end{table}


\figpdf{pattern-matching-query-selectors}{./pattern-matching/query-selectors.pdf}{Matching a simple set of query selectors.}{0.6}


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

const select = (root, selector) => {
  const selectors = selector.split(' ').filter(s => s.length > 0)
  return firstMatch(root, selectors)
}

const firstMatch = (node, selectors) => {
  assert(selectors.length > 0,
    'Require selector(s)')

  // Not a tag.
  if (node.type !== 'tag') {
    return null
  }

  // This node matches.
  if (matchHere(node, selectors[0])) {
    // This is the last selector, so matching worked.
    if (selectors.length === 1) {
      return node
    }

    // Try to match remaining selectors.
    return firstChildMatch(node, selectors.slice(1))
  }

  // This node doesn't match, so try further down.
  return firstChildMatch(node, selectors)
}


export default select
\end{lstlisting}



The \texttt{firstMatch} function handles three cases:

\begin{enumerate}

\item 

This node isn’t an element, i.e., it is plain text or a comment.
    This can’t match a selector, and these nodes don’t have children,
    so the function returns \texttt{null} to indicate that matching has failed.



\item 

This node matches the current selector.
    If there aren’t any selectors left then the whole pattern must have matched,
    so the function returns this node as the match.
    If there \emph{are} more selectors,
    we try to match those that remain against this node’s children
    and return whatever result that produces.



\item 

This node \emph{doesn’t} match the current selector,
    so we search the children one by one to see if there is a match further down.



\end{enumerate}


This algorithm is called \glossref{depth-first search}\index{depth-first search}\index{search!depth-first}:
it explores one possible match to the end before considering any others.
\texttt{firstMatch} relies on a helper function called \texttt{firstChildMatch},
which finds the first child of a node to match a set of selectors:


\begin{lstlisting}[frame=tblr]
const firstChildMatch = (node, selectors) => {
  assert(node.type === 'tag',
    `Should only try to match first child of tags, not ${node.type}`)

  // First working match.
  for (const child of node.children) {
    const match = firstMatch(child, selectors)
    if (match) {
      return match
    }
  }

  // Nothing worked.
  return null
}
\end{lstlisting}



\noindent and on the function \texttt{matchHere} which compares a node against a selector:


\begin{lstlisting}[frame=tblr]
const matchHere = (node, selector) => {
  let name = null
  let id = null
  let cls = null
  if (selector.includes('#')) {
    [name, id] = selector.split('#')
  } else if (selector.includes('.')) {
    [name, cls] = selector.split('.')
  } else {
    name = selector
  }
  return (node.name === name) &&
    ((id === null) || (node.attribs.id === id)) &&
    ((cls === null) || (node.attribs.class === cls))
}
\end{lstlisting}



This version of \texttt{matchHere} is simple but inefficient,
since it breaks the selector into parts each time it is called
rather than doing that once and re-using the results.
We will build a more efficient version in the exercises,
but let’s try out the one we have.
Our test cases are all in one piece of HTML:


\begin{lstlisting}[frame=tblr]
const HTML = `<main>
  <p>text of first p</p>
  <p id="id-01">text of p#id-01</p>
  <p id="id-02">text of p#id-02</p>
  <p class="class-03">text of p.class-03</p>
  <div>
    <p>text of div / p</p>
    <p id="id-04">text of div / p#id-04</p>
    <p class="class-05">text of div / p.class-05</p>
    <p class="class-06">should not be found</p>
  </div>
  <div id="id-07">
    <p>text of div#id-07 / p</p>
    <p class="class-06">text of div#id-07 / p.class-06</p>
  </div>
</main>`
\end{lstlisting}



The program contains a table of queries and the expected matches.
The function \texttt{main} loops over it to report whether each test passes or fails:


\begin{lstlisting}[frame=tblr]
const main = () => {
  const doc = htmlparser2.parseDOM(HTML)[0]
  const tests = [
    ['p', 'text of first p'],
    ['p#id-01', 'text of p#id-01'],
    ['p#id-02', 'text of p#id-02'],
    ['p.class-03', 'text of p.class-03'],
    ['div p', 'text of div / p'],
    ['div p#id-04', 'text of div / p#id-04'],
    ['div p.class-05', 'text of div / p.class-05'],
    ['div#id-07 p', 'text of div#id-07 / p'],
    ['div#id-07 p.class-06', 'text of div#id-07 / p.class-06']
  ]
  tests.forEach(([selector, expected]) => {
    const node = select(doc, selector)
    const actual = getText(node)
    const result = (actual === expected) ? 'pass' : 'fail'
    console.log(`"${selector}": ${result}`)
  })
}

main()
\end{lstlisting}



\noindent \texttt{main} uses a helper function called \texttt{getText} to extract text from a node
or return an error message if something has gone wrong:


\begin{lstlisting}[frame=tblr]
const getText = (node) => {
  if (!node) {
    return 'MISSING NODE'
  }
  if (!('children' in node)) {
    return 'MISSING CHILDREN'
  }
  if (node.children.length !== 1) {
    return 'WRONG NUMBER OF CHILDREN'
  }
  if (node.children[0].type !== 'text') {
    return 'NOT TEXT'
  }
  return node.children[0].data
}
\end{lstlisting}



When we run our program it produces this result:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
"p": pass
"p#id-01": pass
"p#id-02": pass
"p.class-03": pass
"div p": pass
"div p#id-04": pass
"div p.class-05": pass
"div#id-07 p": pass
"div#id-07 p.class-06": pass
\end{lstlisting}



We will rewrite these tests using \hreffoot{Mocha}{https://mochajs.org/}\index{Mocha} in the exercises.

\begin{callout}


\subsubsection*{Test then build}


We actually wrote our test cases \emph{before} implementing the code to match query selectors
in order to give ourselves a goal to work toward.
Doing this is called \glossref{test-driven development}\index{test-driven development}\index{TDD}, or TDD;
while research doesn’t support the claim that
it makes programmers more productive \cite{Fucci2016,Fucci2017},
we find it helps prevent \glossref{scope creep}\index{scope creep!when writing lessons} when writing lessons.

\end{callout}

\section{How can we implement a simple regular expression matcher?}\label{pattern-matching-re}


Matching regular expressions against text relies on the same recursive strategy
as matching query selectors against nodes in an HTML page.
If the first element of the pattern matches where we are,
we see if the rest of the pattern matches what’s left;
otherwise,
we see if the the pattern will match further along.
Our matcher will initially handle just the five cases shown in
\tblref{pattern-matching-cases}.
These cases are a small subset of what JavaScript provides,
but as Kernighan\index{Kernighan, Brian} wrote,
“This is quite a useful class;
in my own experience of using regular expressions on a day-to-day basis,
it easily accounts for 95 percent of all instances.”

\begin{table}[h]
\begin{tabular}{ll}
\textbf{\underline{Meaning}} & \textbf{\underline{Character}} \\
Any literal character \emph{c} & \emph{c} \\
Any single character & . \\
Beginning of input & {\textasciicircum} \\
End of input & \$ \\
Zero or more of the previous character & * \\
\end{tabular}
\caption{Pattern matching cases.}
\label{pattern-matching-cases}
\end{table}



The top-level function that users call
handles the special case of \texttt{{\textasciicircum}} at the start of a pattern
matching the start of the target string being searched.
It then tries the pattern against each successive substring of the target string
until it finds a match or runs out of characters:


\begin{lstlisting}[frame=tblr]
const match = (pattern, text) => {
  // '^' at start of pattern matches start of text.
  if (pattern[0] === '^') {
    return matchHere(pattern, 1, text, 0)
  }

  // Try all possible starting points for pattern.
  let iText = 0
  do {
    if (matchHere(pattern, 0, text, iText)) {
      return true
    }
    iText += 1
  } while (iText < text.length)

  // Nothing worked.
  return false
}
\end{lstlisting}



\texttt{matchHere} does the matching and recursing:


\begin{lstlisting}[frame=tblr]
const matchHere = (pattern, iPattern, text, iText) => {
  // There is no more pattern to match.
  if (iPattern === pattern.length) {
    return true
  }

  // '$' at end of pattern matches end of text.
  if ((iPattern === (pattern.length - 1)) &&
      (pattern[iPattern] === '$') &&
      (iText === text.length)) {
    return true
  }

  // '*' following current character means match many.
  if (((pattern.length - iPattern) > 1) &&
      (pattern[iPattern + 1] === '*')) {
    while ((iText < text.length) && (text[iText] === pattern[iPattern])) {
      iText += 1
    }
    return matchHere(pattern, iPattern + 2, text, iText)
  }

  // Match a single character.
  if ((pattern[iPattern] === '.') ||
      (pattern[iPattern] === text[iText])) {
    return matchHere(pattern, iPattern + 1, text, iText + 1)
  }

  // Nothing worked.
  return false
}
\end{lstlisting}



Once again,
we use a table of test cases and expected results to test it:


\begin{lstlisting}[frame=tblr]
const main = () => {
  const tests = [
    ['a', 'a', true],
    ['b', 'a', false],
    ['a', 'ab', true],
    ['b', 'ab', true],
    ['ab', 'ba', false],
    ['^a', 'ab', true],
    ['^b', 'ab', false],
    ['a$', 'ab', false],
    ['a$', 'ba', true],
    ['a*', '', true],
    ['a*', 'baac', true],
    ['ab*c', 'ac', true],
    ['ab*c', 'abc', true],
    ['ab*c', 'abbbc', true],
    ['ab*c', 'abxc', false]
  ]
  tests.forEach(([regexp, text, expected]) => {
    const actual = match(regexp, text)
    const result = (actual === expected) ? 'pass' : 'fail'
    console.log(`"${regexp}" X "${text}": ${result}`)
  })
}

main()
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
"a" X "a": pass
"b" X "a": pass
"a" X "ab": pass
"b" X "ab": pass
"ab" X "ba": pass
"^a" X "ab": pass
"^b" X "ab": pass
"a$" X "ab": pass
"a$" X "ba": pass
"a*" X "": pass
"a*" X "baac": pass
"ab*c" X "ac": pass
"ab*c" X "abc": pass
"ab*c" X "abbbc": pass
"ab*c" X "abxc": pass
\end{lstlisting}



This program seems to work,
but it actually contains an error that we will correct in the exercises.
(Think about what happens if we match the pattern \texttt{/a*ab/} against the string \texttt{{\textquotesingle}aab{\textquotesingle}}.)
Our design is also hard to extend:
handling parentheses in patterns like \texttt{/a(bc)*d/} will require major changes.
We need to explore a different approach.

\section{How can we implement an extensible matcher?}\label{pattern-matching-extensible}


Instead of packing all of our code into one long function,
we can implement each kind of match as separate function.
Doing this makes it much easier to add more matchers:
we just define a function that we can mix in with calls to the ones we already have.


Rather than having these functions do the matching immediately,
though,
we will have each one return an object that knows how to match itself against some text.
Doing this allows us to build a complex match once and re-use it many times.
This is a common pattern in text processing:
we may want to apply a regular expression to each line in a large set of files,
so recycling the matchers will make our programs more efficient.


Each matching object has a method that takes the target string and the index to start matching at as inputs.
Its output is the index to continue matching at
or \texttt{undefined} indicating that matching failed.
We can then combine these objects to match complex patterns
(\figref{pattern-matching-regex-objects}).

\figpdf{pattern-matching-regex-objects}{./pattern-matching/regex-objects.pdf}{Using nested objects to match regular expressions.}{0.6}


The first step in implementing this is to write test cases,
which forces us to define the syntax we are going to support:


\begin{lstlisting}[frame=tblr]
import Alt from './regex-alt.js'
import Any from './regex-any.js'
import End from './regex-end.js'
import Lit from './regex-lit.js'
import Seq from './regex-seq.js'
import Start from './regex-start.js'

const main = () => {
  const tests = [
    ['a', 'a', true, Lit('a')],
    ['b', 'a', false, Lit('b')],
    ['a', 'ab', true, Lit('a')],
    ['b', 'ab', true, Lit('b')],
    ['ab', 'ab', true, Seq(Lit('a'), Lit('b'))],
    ['ba', 'ab', false, Seq(Lit('b'), Lit('a'))],
    ['ab', 'ba', false, Lit('ab')],
    ['^a', 'ab', true, Seq(Start(), Lit('a'))],
    ['^b', 'ab', false, Seq(Start(), Lit('b'))],
    ['a$', 'ab', false, Seq(Lit('a'), End())],
    ['a$', 'ba', true, Seq(Lit('a'), End())],
    ['a*', '', true, Any('a')],
    ['a*', 'baac', true, Any('a')],
    ['ab*c', 'ac', true, Seq(Lit('a'), Any('b'), Lit('c'))],
    ['ab*c', 'abc', true, Seq(Lit('a'), Any('b'), Lit('c'))],
    ['ab*c', 'abbbc', true, Seq(Lit('a'), Any('b'), Lit('c'))],
    ['ab*c', 'abxc', false, Seq(Lit('a'), Any('b'), Lit('c'))],
    ['ab|cd', 'xaby', true, Alt(Lit('ab'), Lit('cd'))],
    ['ab|cd', 'acdc', true, Alt(Lit('ab'), Lit('cd'))],
    ['a(b|c)d', 'xabdy', true,
      Seq(Lit('a'), Alt(Lit('b'), Lit('c')), Lit('d'))],
    ['a(b|c)d', 'xabady', false,
      Seq(Lit('a'), Alt(Lit('b'), Lit('c')), Lit('d'))]
  ]
  tests.forEach(([pattern, text, expected, matcher]) => {
    const actual = matcher.match(text)
    const result = (actual === expected) ? 'pass' : 'fail'
    console.log(`"${pattern}" X "${text}": ${result}`)
  })
}

main()
\end{lstlisting}



Next,
we define a \glossref{base class} that all matchers will inherit from.
This class contains the \texttt{match} method that users will call
so that we can start matching right away
no matter what kind of matcher we have at the top level of our pattern.


\begin{lstlisting}[frame=tblr]
class RegexBase {
  match (text) {
    for (let i = 0; i < text.length; i += 1) {
      if (this._match(text, i)) {
        return true
      }
    }
    return false
  }

  _match (text, start) {
    throw new Error('derived classes must override "_match"')
  }
}

export default RegexBase
\end{lstlisting}



\noindent The base class also defines a \texttt{\_match} method (with a leading underscore)
that other classes will fill in with actual matching code.
The base implementation of this method throws an exception
so that if we forget to provide \texttt{\_match} in a \glossref{derived class}
our code will fail with a meaningful reminder.

\begin{callout}


\subsubsection*{One interface to call them all}


Our design makes use of \glossref{polymorphism}\index{polymorphism (in software design)}\index{software design!polymorphism},
which literally means “having multiple forms”.
If a set of objects all have methods that can be called the same way,
then those objects can be used interchangeably;
putting it another way,
a program can use them without knowing exactly what they are.
Polymorphism reduces the \glossref{coupling}\index{coupling}\index{software design!coupling} between different parts of our program,
which in turn makes it easier for those programs to evolve.

\end{callout}


We can now define empty versions of each matching class that all say “no match here”
like this one for literal characters:


\begin{lstlisting}[frame=tblr]
import RegexBase from './regex-base.js'

class RegexLit extends RegexBase {
  constructor (chars) {
    super()
    this.chars = chars
  }

  _match (text, start) {
    return undefined // FIXME
  }
}

export default (chars) => new RegexLit(chars)
\end{lstlisting}



\noindent Our tests now run, but most of them fail:
“most” because we expect some tests not to match,
so the test runner reports \texttt{true}.


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
"a" X "a": fail
"b" X "a": pass
"a" X "ab": fail
"b" X "ab": fail
"ab" X "ab": fail
"ba" X "ab": pass
"ab" X "ba": pass
"^a" X "ab": fail
"^b" X "ab": pass
"a$" X "ab": pass
"a$" X "ba": fail
"a*" X "": fail
"a*" X "baac": fail
"ab*c" X "ac": fail
"ab*c" X "abc": fail
"ab*c" X "abbbc": fail
"ab*c" X "abxc": pass
"ab|cd" X "xaby": fail
"ab|cd" X "acdc": fail
"a(b|c)d" X "xabdy": fail
"a(b|c)d" X "xabady": pass
\end{lstlisting}



\noindent This output tells us how much work we have left to do:
when all of these tests pass,
we’re finished.


Let’s implement a literal character string matcher first:


\begin{lstlisting}[frame=tblr]
import RegexBase from './regex-base.js'

class RegexLit extends RegexBase {
  constructor (chars) {
    super()
    this.chars = chars
  }

  _match (text, start) {
    const nextIndex = start + this.chars.length
    if (nextIndex > text.length) {
      return undefined
    }
    if (text.slice(start, nextIndex) !== this.chars) {
      return undefined
    }
    return nextIndex
  }
}

export default (chars) => new RegexLit(chars)
\end{lstlisting}



Some tests now pass, others still fail as expected:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
"a" X "a": pass
"b" X "a": pass
"a" X "ab": pass
"b" X "ab": pass
"ab" X "ab": fail
"ba" X "ab": pass
"ab" X "ba": pass
"^a" X "ab": fail
"^b" X "ab": pass
"a$" X "ab": pass
"a$" X "ba": fail
"a*" X "": fail
"a*" X "baac": fail
"ab*c" X "ac": fail
"ab*c" X "abc": fail
"ab*c" X "abbbc": fail
"ab*c" X "abxc": pass
"ab|cd" X "xaby": fail
"ab|cd" X "acdc": fail
"a(b|c)d" X "xabdy": fail
"a(b|c)d" X "xabady": pass
\end{lstlisting}



We will tackle \texttt{RegexSeq} next so that we can combine other matchers.
This is why we have tests for \texttt{Seq(Lit({\textquotesingle}a{\textquotesingle}), Lit({\textquotesingle}b{\textquotesingle}))} and \texttt{Lit({\textquotesingle}ab{\textquotesingle})}:
all children have to match in order without gaps.


But wait:
suppose we have the pattern \texttt{/a*ab/}.
This ought to match the text \texttt{"ab"}, but will it?
The \texttt{/*/} is \glossref{greedy}\index{greedy algorithm}\index{algorithm!greedy}: it matches as much as it can
(which is also called \glossref{eager matching}\index{eager matching}\index{matching!eager}).
As a result,
\texttt{/a*/} will match the leading \texttt{"a"}, leaving nothing for the literal \texttt{/a/} to match
(\figref{pattern-matching-greedy-failure}).
Our current implementation doesn’t give us a way to try other possible matches when this happens.

\figpdf{pattern-matching-greedy-failure}{./pattern-matching/greedy-failure.pdf}{Why overly greedy matching doesn’t work.}{0.6}


Let’s re-think our design
and have each matcher take its own arguments and a \texttt{rest} parameter containing the rest of the matchers
(\figref{pattern-matching-rest}).
(We will provide a default of \texttt{null} in the creation function
so we don’t have to type \texttt{null} over and over again.)
Each matcher will try each of its possibilities and then see if the rest will also match.

\figpdf{pattern-matching-rest}{./pattern-matching/rest.pdf}{Using \texttt{rest} to match the remainder of a pattern.}{0.6}


This design means we can get rid of \texttt{RegexSeq},
but it does make our tests a little harder to read:


\begin{lstlisting}[frame=tblr]
import Alt from './regex-alt.js'
import Any from './regex-any.js'
import End from './regex-end.js'
import Lit from './regex-lit.js'
import Start from './regex-start.js'

const main = () => {
  const tests = [
    ['a', 'a', true, Lit('a')],
    ['b', 'a', false, Lit('b')],
    ['a', 'ab', true, Lit('a')],
    ['b', 'ab', true, Lit('b')],
    ['ab', 'ab', true, Lit('a', Lit('b'))],
    ['ba', 'ab', false, Lit('b', Lit('a'))],
    ['ab', 'ba', false, Lit('ab')],
    ['^a', 'ab', true, Start(Lit('a'))],
    ['^b', 'ab', false, Start(Lit('b'))],
    ['a$', 'ab', false, Lit('a', End())],
    ['a$', 'ba', true, Lit('a', End())],
    ['a*', '', true, Any(Lit('a'))],
    ['a*', 'baac', true, Any(Lit('a'))],
    ['ab*c', 'ac', true, Lit('a', Any(Lit('b'), Lit('c')))],
    ['ab*c', 'abc', true, Lit('a', Any(Lit('b'), Lit('c')))],
    ['ab*c', 'abbbc', true, Lit('a', Any(Lit('b'), Lit('c')))],
    ['ab*c', 'abxc', false, Lit('a', Any(Lit('b'), Lit('c')))],
    ['ab|cd', 'xaby', true, Alt(Lit('ab'), Lit('cd'))],
    ['ab|cd', 'acdc', true, Alt(Lit('ab'), Lit('cd'))],
    ['a(b|c)d', 'xabdy', true, Lit('a', Alt(Lit('b'), Lit('c'), Lit('d')))],
    ['a(b|c)d', 'xabady', false, Lit('a', Alt(Lit('b'), Lit('c'), Lit('d')))]
  ]
  tests.forEach(([pattern, text, expected, matcher]) => {
    const actual = matcher.match(text)
    const result = (actual === expected) ? 'pass' : 'fail'
    console.log(`"${pattern}" X "${text}": ${result}`)
  })
}

main()
\end{lstlisting}



Here’s how this works for matching a literal expression:


\begin{lstlisting}[frame=tblr]
import RegexBase from './regex-base.js'

class RegexLit extends RegexBase {
  constructor (chars, rest) {
    super(rest)
    this.chars = chars
  }

  _match (text, start) {
    const nextIndex = start + this.chars.length
    if (nextIndex > text.length) {
      return undefined
    }
    if (text.slice(start, nextIndex) !== this.chars) {
      return undefined
    }
    if (this.rest === null) {
      return nextIndex
    }
    return this.rest._match(text, nextIndex)
  }
}

export default (chars, rest = null) => new RegexLit(chars, rest)
\end{lstlisting}



\noindent The \texttt{\_match} method checks whether all of the pattern matches the target text starting at the current location.
If so, it checks whether the rest of the overall pattern matches what’s left.
Matching the start \texttt{/{\textasciicircum}/} and end \texttt{/\$/} anchors is just as straightforward:


\begin{lstlisting}[frame=tblr]
import RegexBase from './regex-base.js'

class RegexStart extends RegexBase {
  _match (text, start) {
    if (start !== 0) {
      return undefined
    }
    if (this.rest === null) {
      return 0
    }
    return this.rest._match(text, start)
  }
}

export default (rest = null) => new RegexStart(rest)
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
import RegexBase from './regex-base.js'

class RegexEnd extends RegexBase {
  _match (text, start) {
    if (start !== text.length) {
      return undefined
    }
    if (this.rest === null) {
      return text.length
    }
    return this.rest._match(text, start)
  }
}

export default (rest = null) => new RegexEnd(rest)
\end{lstlisting}



Matching either/or is done by trying the first pattern and the rest,
and if that fails,
trying the second pattern and the rest:


\begin{lstlisting}[frame=tblr]
import RegexBase from './regex-base.js'

class RegexAlt extends RegexBase {
  constructor (left, right, rest) {
    super(rest)
    this.left = left
    this.right = right
  }

  _match (text, start) {
    for (const pat of [this.left, this.right]) {
      const afterPat = pat._match(text, start)
      if (afterPat !== undefined) {
        if (this.rest === null) {
          return afterPat
        }
        const afterRest = this.rest._match(text, afterPat)
        if (afterRest !== undefined) {
          return afterRest
        }
      }
    }
    return undefined
  }
}

const create = (left, right, rest = null) => {
  return new RegexAlt(left, right, rest)
}

export default create
\end{lstlisting}



To match a repetition,
we figure out the maximum number of matches that might be left,
then count down until something succeeds.
(We start with the maximum because matching is supposed to be greedy.)
Each non-empty repetition matches at least one character,
so the number of remaining characters is the maximum number of matches worth trying.


\begin{lstlisting}[frame=tblr]
import RegexBase from './regex-base.js'

class RegexAny extends RegexBase {
  constructor (child, rest) {
    super(rest)
    this.child = child
  }

  _match (text, start) {
    const maxPossible = text.length - start
    for (let num = maxPossible; num >= 0; num -= 1) {
      const afterMany = this._matchMany(text, start, num)
      if (afterMany !== undefined) {
        return afterMany
      }
    }
    return undefined
  }

  _matchMany (text, start, num) {
    for (let i = 0; i < num; i += 1) {
      start = this.child._match(text, start)
      if (start === undefined) {
        return undefined
      }
    }
    if (this.rest !== null) {
      return this.rest._match(text, start)
    }
    return start
  }
}

const create = (child, rest = null) => {
  return new RegexAny(child, rest)
}

export default create
\end{lstlisting}



With these classes in place,
our tests all pass:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
"a" X "a": pass
"b" X "a": pass
"a" X "ab": pass
"b" X "ab": pass
"ab" X "ab": pass
"ba" X "ab": pass
"ab" X "ba": pass
"^a" X "ab": pass
"^b" X "ab": pass
"a$" X "ab": pass
"a$" X "ba": pass
"a*" X "": pass
"a*" X "baac": pass
"ab*c" X "ac": pass
"ab*c" X "abc": pass
"ab*c" X "abbbc": pass
"ab*c" X "abxc": pass
"ab|cd" X "xaby": pass
"ab|cd" X "acdc": pass
"a(b|c)d" X "xabdy": pass
"a(b|c)d" X "xabady": pass
\end{lstlisting}



The most important thing about this design is how extensible it is:
if we want to add other kinds of matching,
all we have to do is add more classes.
That extensibility comes from the lack of centralized decision-making,
which in turn comes from our use of polymorphism
and the \glossref{Chain of Responsibility}\index{Chain of Responsibility pattern}\index{design pattern!Chain of Responsibility} design pattern.
Each component does its part and asks something else to handle the remaining work;
so long as each component takes the same inputs,
we can put them together however we want.

\begin{callout}


\subsubsection*{The Open-Closed Principle}


The \glossref{Open-Closed Principle}\index{Open-Closed Principle}\index{software design!Open-Closed Principle} states that
software should be open for extension but closed for modification,
i.e., that it should be possible to extend functionality
without having to rewrite existing code.
As we said in \chapref{async-programming},
this allows old code to use new code,
but only if our design permits the kinds of extensions people are going to want to make.
Since we can’t anticipate everything,
it is normal to have to revise a design the first two or three times we try to extend it.
Looking at it another way,
the things we build learn how to do their jobs better
as we use them and improve them \cite{Brand1995}.

\end{callout}

\section{Exercises}\label{pattern-matching-exercises}

\subsection*{Split once}


Modify the query selector code so that selectors like \texttt{div\#id} and \texttt{div.class} are only split into pieces once
rather than being re-split each time \texttt{matchHere} is called.

\subsection*{Find and fix the error}


The first regular expression matcher contains an error:
the pattern \texttt{{\textquotesingle}a*ab{\textquotesingle}} should match the string \texttt{{\textquotesingle}aab{\textquotesingle}} but doesn’t.
Figure out why it fails and fix it.

\subsection*{Unit tests}


Rewrite the tests for selectors and regular expressions to use Mocha.

\subsection*{Find all with query selectors}


Modify the query selector so that it returns \emph{all} matches, not just the first one.

\subsection*{Select based on attributes}


Modify the query selector to handle \texttt{[attribute="value"]} selectors,
so that (for example) \texttt{div[align=center]} returns all \texttt{div} elements
whose \texttt{align} attribute has the value \texttt{"center"}.

\subsection*{Child selectors}


The expression \texttt{parent > child} selects all nodes of type \texttt{child}
that are immediate children of nodes of type \texttt{parent}—for example,
\texttt{div > p} selects all paragraphs that are immediate children of \texttt{div} elements.
Modify \texttt{simple-selectors.js} to handle this kind of matching.

\subsection*{Find all with regular expressions}


Modify the regular expression matcher to return \emph{all} matches rather than just the first one.

\subsection*{Find one or more with regular expressions}


Extend the regular expression matcher to support \texttt{+}, meaning “one or more”.

\subsection*{Match sets of characters}


Add a new regular expression matching class that matches any character from a set,
so that \texttt{Charset({\textquotesingle}aeiou{\textquotesingle})} matches any lower-case vowel.

\subsection*{Make repetition more efficient}


Rewrite \texttt{RegexAny} so that it does not repeatedly re-match text.

\subsection*{Lazy matching}


The regular expressions we have seen so far are \glossref{eager}:
they match as much as they can, as early as they can.
An alternative is \glossref{lazy matching},
in which expressions match as little as they need to.
For example,
given the string \texttt{"ab"},
an eager match with the expression \texttt{/ab*/} will match both letters
(because \texttt{/b*/} matches a ‘b’ if one is available)
but a lazy match will only match the first letter
(because \texttt{/b*/} can match no letters at all).
Implement lazy matching for the \texttt{*} operator.

\subsection*{Optional matching}


The \texttt{?} operator means “optional”,
so that \texttt{/a?/} matches either zero or one occurrences of the letter ‘a’.
Implement this operator.

\chapter{Parsing Expressions}\label{regex-parser}


\noindent 
  Terms defined: \glossref{finite state machine}, \glossref{literal}, \glossref{parser}, \glossref{precedence}, \glossref{token}, \glossref{Turing Machine}, \glossref{well formed}, \glossref{YAML}



In \chapref{pattern-matching} we created regular expressions by constructing objects.
It takes a lot less typing to write them as strings as we did for HTML selectors,
but if we’re going to do that we need something to convert those strings to the required objects.
In other words, we need to write a \glossref{parser}\index{parser}.

\begin{table}[h]
\begin{tabular}{ll}
\textbf{\underline{Meaning}} & \textbf{\underline{Character}} \\
Any literal character \emph{c} & \emph{c} \\
Beginning of input & {\textasciicircum} \\
End of input & \$ \\
Zero or more of the previous thing & * \\
Either/or & | \\
Grouping & (...) \\
\end{tabular}
\caption{Regular expression grammar.}
\label{regex-parser-grammar-codes}
\end{table}



\tblref{regex-parser-grammar-codes} shows the grammar we will handle.
When we are done
we should be able to parse \texttt{/{\textasciicircum}(a|b|\$)*z\$/} as
“start of text”,
“any number of ‘a’, ‘b’, or ‘\$’“,
“a single ‘z’,
and “end of text”.
(We write regular expressions inside slashes to distinguish them from strings.)
To keep things simple,
we will create a tree of objects (\figref{regex-parser-expression-tree})
rather than instances of the regular expression classes from \chapref{pattern-matching};
the exercises will tackle the latter.

\figpdf{regex-parser-expression-tree}{./regex-parser/expression-tree.pdf}{Representing the result of parsing a regular expression as an tree.}{0.6}

\begin{callout}


\subsubsection*{Please don’t write parsers}


Languages that are comfortable for people to read are usually difficult for computers to understand
and vice versa,
so we need parsers to translate human-friendly notation into computer-friendly representations.
However,
the world doesn’t need more file formats\index{parser!reasons not to write};
if you need a configuration file or lookup table,
please use CSV, JSON, \glossref{YAML},
or something else that already has an acronym
rather than inventing a format of your own.

\end{callout}

\section{How can we break text into tokens?}\label{regex-parser-tokenize}


A \glossref{token}\index{token (in parsing)} is an atom of text,
such as the digits making up a number or the letters making up a variable name.
In our grammar the tokens are the special characters \texttt{*}, \texttt{|}, \texttt{(}, \texttt{)}, \texttt{{\textasciicircum}}, and \texttt{\$},
plus any sequence of one or more other characters (which count as one multi-letter token).
This classification guides the design of our parser:

\begin{enumerate}

\item 

If a character is special, create a token for it.



\item 

If it is a \glossref{literal}\index{literal (in parsing)} then
    combine it with the current literal if there is one
    or start a new literal.



\item 

Since \texttt{{\textasciicircum}} and \texttt{\$} are either special or regular depending on position,
    we must treat them as separate tokens or as part of a literal
    based on where they appear.



\end{enumerate}


We can translate these rules almost directly into code
to create a list of objects whose keys are \texttt{kind} and \texttt{loc} (short for location),
with the extra key \texttt{value} for literal values:


\begin{lstlisting}[frame=tblr]
const SIMPLE = {
  '*': 'Any',
  '|': 'Alt',
  '(': 'GroupStart',
  ')': 'GroupEnd'
}

const tokenize = (text) => {
  const result = []
  for (let i = 0; i < text.length; i += 1) {
    const c = text[i]
    if (c in SIMPLE) {
      result.push({ kind: SIMPLE[c], loc: i })
    } else if (c === '^') {
      if (i === 0) {
        result.push({ kind: 'Start', loc: i })
      } else {
        combineOrPush(result, c, i)
      }
    } else if (c === '$') {
      if (i === (text.length - 1)) {
        result.push({ kind: 'End', loc: i })
      } else {
        combineOrPush(result, c, i)
      }
    } else {
      combineOrPush(result, c, i)
    }
  }

  return result
}


export default tokenize
\end{lstlisting}



The helper function \texttt{combineOrPush} does exactly what its name says.
If the thing most recently added to the list of tokens isn’t a literal,
the new character becomes a new token;
otherwise,
we append the new character to the literal we’re building:


\begin{lstlisting}[frame=tblr]
const combineOrPush = (soFar, character, location) => {
  const topIndex = soFar.length - 1
  if ((soFar.length === 0) || (soFar[topIndex].token !== 'Lit')) {
    soFar.push({ kind: 'Lit', value: character, loc: location })
  } else {
    soFar[topIndex].value += character
  }
}
\end{lstlisting}



We can try this out with a three-line test program:


\begin{lstlisting}[frame=tblr]
import tokenize from './tokenizer-collapse.js'

const test = '^a^b*'
const result = tokenize(test)
console.log(JSON.stringify(result, null, 2))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  {
    "kind": "Start",
    "loc": 0
  },
  {
    "kind": "Lit",
    "value": "a",
    "loc": 1
  },
  {
    "kind": "Lit",
    "value": "^",
    "loc": 2
  },
  {
    "kind": "Lit",
    "value": "b",
    "loc": 3
  },
  {
    "kind": "Any",
    "loc": 4
  }
]
\end{lstlisting}



This simple tokenizer is readable, efficient, and wrong.
The problem is that the expression \texttt{/ab*/} means “a single \texttt{a} followed by zero or more \texttt{b}”.
If we combine the \texttt{a} and \texttt{b} as we read them,
though,
we wind up with “zero or more repetitions of \texttt{ab}”.
(Don’t feel bad if you didn’t spot this:
we didn’t notice the problem until we were implementing the next step.)


The solution is to treat each regular character as its own literal in this stage
and then combine things later.
Doing this lets us get rid of the nested \texttt{if} for handling \texttt{{\textasciicircum}} and \texttt{\$} as well:


\begin{lstlisting}[frame=tblr]
const SIMPLE = {
  '*': 'Any',
  '|': 'Alt',
  '(': 'GroupStart',
  ')': 'GroupEnd'
}

const tokenize = (text) => {
  const result = []
  for (let i = 0; i < text.length; i += 1) {
    const c = text[i]
    if (c in SIMPLE) {
      result.push({ kind: SIMPLE[c], loc: i })
    } else if ((c === '^') && (i === 0)) {
      result.push({ kind: 'Start', loc: i })
    } else if ((c === '$') && (i === (text.length - 1))) {
      result.push({ kind: 'End', loc: i })
    } else {
      result.push({ kind: 'Lit', loc: i, value: c })
    }
  }

  return result
}

export default tokenize
\end{lstlisting}



Software isn’t done until it’s tested,
so let’s build some \hreffoot{Mocha}{https://mochajs.org/}\index{Mocha} tests for our tokenizer.
The listing below shows a few of these
along with the output for the full set:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import tokenize from '../tokenizer.js'

describe('tokenizes correctly', async () => {
  it('tokenizes a single character', () => {
    assert.deepStrictEqual(tokenize('a'), [
      { kind: 'Lit', value: 'a', loc: 0 }
    ])
  })

  it('tokenizes a sequence of characters', () => {
    assert.deepStrictEqual(tokenize('ab'), [
      { kind: 'Lit', value: 'a', loc: 0 },
      { kind: 'Lit', value: 'b', loc: 1 }
    ])
  })

  it('tokenizes start anchor alone', () => {
    assert.deepStrictEqual(tokenize('^'), [
      { kind: 'Start', loc: 0 }
    ])
  })

  it('tokenizes start anchor followed by characters', () => {
    assert.deepStrictEqual(tokenize('^a'), [
      { kind: 'Start', loc: 0 },
      { kind: 'Lit', value: 'a', loc: 1 }
    ])
  })

  it('tokenizes a complex expression', () => {
    assert.deepStrictEqual(tokenize('^a*(bcd|e^)*f$gh$'), [
      { kind: 'Start', loc: 0 },
      { kind: 'Lit', loc: 1, value: 'a' },
      { kind: 'Any', loc: 2 },
      { kind: 'GroupStart', loc: 3 },
      { kind: 'Lit', loc: 4, value: 'b' },
      { kind: 'Lit', loc: 5, value: 'c' },
      { kind: 'Lit', loc: 6, value: 'd' },
      { kind: 'Alt', loc: 7 },
      { kind: 'Lit', loc: 8, value: 'e' },
      { kind: 'Lit', loc: 9, value: '^' },
      { kind: 'GroupEnd', loc: 10 },
      { kind: 'Any', loc: 11 },
      { kind: 'Lit', loc: 12, value: 'f' },
      { kind: 'Lit', loc: 13, value: '$' },
      { kind: 'Lit', loc: 14, value: 'g' },
      { kind: 'Lit', loc: 15, value: 'h' },
      { kind: 'End', loc: 16 }
    ])
  })
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test /u/stjs
> mocha */test/test-*.js "-g" "tokenizes correctly"



  tokenizes correctly
    ✓ tokenizes a single character
    ✓ tokenizes a sequence of characters
    ✓ tokenizes start anchor alone
    ✓ tokenizes start anchor followed by characters
    ✓ tokenizes circumflex not at start
    ✓ tokenizes start anchor alone
    ✓ tokenizes end anchor preceded by characters
    ✓ tokenizes dollar sign not at end
    ✓ tokenizes repetition alone
    ✓ tokenizes repetition in string
    ✓ tokenizes repetition at end of string
    ✓ tokenizes alternation alone
    ✓ tokenizes alternation in string
    ✓ tokenizes alternation at start of string
    ✓ tokenizes the start of a group alone
    ✓ tokenizes the start of a group in a string
    ✓ tokenizes the end of a group alone
    ✓ tokenizes the end of a group at the end of a string
    ✓ tokenizes a complex expression


  19 passing (12ms)
\end{lstlisting}


\section{How can we turn a list of tokens into a tree?}\label{regex-parser-tree}


We now have a list of tokens,
but we need a tree that captures the nesting introduced by parentheses
and the way that \texttt{*} applies to whatever comes before it.
Let’s trace a few cases in order to see how to build this tree:

\begin{enumerate}

\item 

If the regular expression is \texttt{/a/}, we create a \texttt{Lit} token for the letter \texttt{a}
    (where “create” means “append to the output list”).



\item 

What if the regular expression is \texttt{/a*/}?
    We first create a \texttt{Lit} token for the \texttt{a} and append it to the output list.
    When we see the \texttt{*},
    we take that \texttt{Lit} token off the tail of the output list
    and replace it with an \texttt{Any} token that has the \texttt{Lit} token as its child.



\item 

Our next thought experiment is \texttt{/(ab)/}.
    We don’t know how long the group is going to be when we see the \texttt{(},
    so we put the parenthesis onto the output as a marker.
    We then add the \texttt{Lit} tokens for the \texttt{a} and \texttt{b}
    until we see the \texttt{)},
    at which point we pull tokens off the end of the output list
    until we get back to the \texttt{(} marker.
    When we find it,
    we put everything we have temporarily collected into a \texttt{Group} token and append it to the output list.
    This algorithm automatically handles \texttt{/(a*)/} and \texttt{/(a(b*)c)/}.



\item 

What about \texttt{/a|b/}?
    We append a \texttt{Lit} token for \texttt{a}, get the \texttt{|} and—and we’re stuck,
    because we don’t yet have the next token we need to finish building the \texttt{Alt}.



\end{enumerate}


\noindent One way to solve this problem is to check if the thing on the top of the stack is waiting to combine
each time we append a new token.
However,
this doesn’t handle \texttt{/a|b*/} properly.
The pattern is supposed to mean “one \texttt{a} or any number of \texttt{b}”,
but the check-and-combine strategy\index{parser!check-and-combine} will turn it into the equivalent of \texttt{/(a|b)*/}.


A better (i.e., correct) solution is
to leave some partially-completed tokens in the output and compress\index{parser!post-hoc compression strategy} them later
(\figref{regex-parser-mechanics}).
If our input is the pattern \texttt{/a|b/}, we can:

\begin{enumerate}

\item 

Append a \texttt{Lit} token for \texttt{a}.



\item 

When we see \texttt{|},
    make that \texttt{Lit} token the left child of the \texttt{Alt}
    and append that without filling in the right child.



\item 

Append the \texttt{Lit} token for \texttt{b}.



\item 

After all tokens have been handled,
    look for partially-completed \texttt{Alt} tokens and make whatever comes after them their right child.



\end{enumerate}


\noindent Again, this automatically handles patterns like \texttt{/(ab)|c*|(de)/}.

\figpdf{regex-parser-mechanics}{./regex-parser/mechanics.pdf}{Mechanics of combining tokens while parsing regular expressions.}{0.6}


It’s time to turn these ideas into code.
The main structure of our parser is:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import tokenize from './tokenizer.js'

const parse = (text) => {
  const result = []
  const allTokens = tokenize(text)
  for (let i = 0; i < allTokens.length; i += 1) {
    const token = allTokens[i]
    const last = i === allTokens.length - 1
    handle(result, token, last)
  }
  return compress(result)
}


export default parse
\end{lstlisting}



We handle tokens case-by-case
(with a few assertions to check that patterns are \glossref{well formed}):


\begin{lstlisting}[frame=tblr]
const handle = (result, token, last) => {
  if (token.kind === 'Lit') {
    result.push(token)
  } else if (token.kind === 'Start') {
    assert(result.length === 0,
      'Should not have start token after other tokens')
    result.push(token)
  } else if (token.kind === 'End') {
    assert(last,
      'Should not have end token before other tokens')
    result.push(token)
  } else if (token.kind === 'GroupStart') {
    result.push(token)
  } else if (token.kind === 'GroupEnd') {
    result.push(groupEnd(result, token))
  } else if (token.kind === 'Any') {
    assert(result.length > 0,
      `No operand for '*' (location ${token.loc})`)
    token.child = result.pop()
    result.push(token)
  } else if (token.kind === 'Alt') {
    assert(result.length > 0,
      `No operand for '*' (location ${token.loc})`)
    token.left = result.pop()
    token.right = null
    result.push(token)
  } else {
    assert(false, 'UNIMPLEMENTED')
  }
}
\end{lstlisting}



When we find the \texttt{)} that marks the end of a group,
we take items from the end of the output list
until we find the matching start
and use them to create a group:


\begin{lstlisting}[frame=tblr]
const groupEnd = (result, token) => {
  const group = {
    kind: 'Group',
    loc: null,
    end: token.loc,
    children: []
  }
  while (true) {
    assert(result.length > 0,
           `Unmatched end parenthesis (location ${token.loc})`)
    const child = result.pop()
    if (child.kind === 'GroupStart') {
      group.loc = child.loc
      break
    }
    group.children.unshift(child)
  }
  return group
}
\end{lstlisting}



Finally,
when we have finished with the input,
we go through the output list one last time to fill in the right side of \texttt{Alt}s:


\begin{lstlisting}[frame=tblr]
const compress = (raw) => {
  const cooked = []
  while (raw.length > 0) {
    const token = raw.pop()
    if (token.kind === 'Alt') {
      assert(cooked.length > 0,
             `No right operand for alt (location ${token.loc})`)
      token.right = cooked.shift()
    }
    cooked.unshift(token)
  }
  return cooked
}
\end{lstlisting}



Once again,
it’s not done until we’ve tested it:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import parse from '../parser.js'

describe('parses correctly', async () => {
  it('parses the empty string', () => {
    assert.deepStrictEqual(parse(''), [])
  })

  it('parses a single literal', () => {
    assert.deepStrictEqual(parse('a'), [
      { kind: 'Lit', loc: 0, value: 'a' }
    ])
  })

  it('parses multiple literals', () => {
    assert.deepStrictEqual(parse('ab'), [
      { kind: 'Lit', loc: 0, value: 'a' },
      { kind: 'Lit', loc: 1, value: 'b' }
    ])
  })


  it('parses alt of groups', () => {
    assert.deepStrictEqual(parse('a|(bc)'), [
      {
        kind: 'Alt',
        loc: 1,
        left: { kind: 'Lit', loc: 0, value: 'a' },
        right: {
          kind: 'Group',
          loc: 2,
          end: 5,
          children: [
            { kind: 'Lit', loc: 3, value: 'b' },
            { kind: 'Lit', loc: 4, value: 'c' }
          ]
        }
      }
    ])
  })
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test /u/stjs
> mocha */test/test-*.js "-g" "parses correctly"



  parses correctly
    ✓ parses the empty string
    ✓ parses a single literal
    ✓ parses multiple literals
    ✓ parses start anchors
    ✓ handles circumflex not at start
    ✓ parses end anchors
    ✓ parses circumflex not at start
    ✓ parses empty groups
    ✓ parses groups containing characters
    ✓ parses two groups containing characters
    ✓ parses any
    ✓ parses any of group
    ✓ parses alt
    ✓ parses alt of any
    ✓ parses alt of groups


  15 passing (11ms)
\end{lstlisting}



While our final parser is less than 90 lines of code,
it is doing a lot of complex things.
Compared to parsers for things like JSON and YAML,
though,
it is still very simple.
If we have more operators with different \glossref{precedences}\index{operator precedence!implementing}
we should switch to the \hreffoot{shunting-yard algorithm}{https://en.wikipedia.org/wiki/Shunting-yard\_algorithm}\index{shunting-yard algorithm}\index{parser!shunting-yard algorithm},
and if we need to handle a language like JavaScript we should explore tools like \hreffoot{ANTLR}{https://www.antlr.org/}\index{ANTLR},
which can generate a parser automatically given a description of the language to be parsed.
As we said at the start,
though,
if our design requires us to write a parser we should try to come up with a better design.
CSV, JSON, YAML, and other formats \hreffoot{have their quirks}{https://third-bit.com/2015/06/11/why-we-cant-have-nice-things/},
but at least they’re broken the same way everywhere.

\begin{callout}


\subsubsection*{The limits of computing}


One of the most important theoretical results in computer science is that
every formal language corresponds to a type of abstract machine and vice versa,
and that some languages (or machines) are more or less powerful than others.
For example,
every regular expression corresponds to a \glossref{finite state machine}\index{finite state machine!correspondence with regular expressions} (FSM)
like the one in \figref{regex-parser-finite-state-machine}.
As powerful as FSMs are,
they cannot match things like nested parentheses or HTML tags,
and \hreffoot{attempting to do so is a sin}{https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454\#1732454}\index{sin!using regular expressions to parse HTML}.
If you add a stack to the system you can process a much richer set of languages,
and if you add two stacks you have something equivalent to a \glossref{Turing Machine}\index{Turing Machine}
that can do any conceivable computation.
\cite{Conery2021} presents this idea and others for self-taught developers.

\end{callout}

\figpdf{regex-parser-finite-state-machine}{./regex-parser/finite-state-machine.pdf}{A finite state machine equivalent to a regular expression.}{0.6}

\section{Exercises}\label{regex-parser-exercises}

\subsection*{Create objects}


Modify the parser to return instances of classes derived from \texttt{RegexBase}.

\subsection*{Escape characters}


Modify the parser to handle escape characters,
so that (for example) \texttt{{\textbackslash}*} is interpreted as “a literal asterisk”
and \texttt{{\textbackslash}{\textbackslash}} is interpreted as “a literal backslash”.

\subsection*{Lazy matching}


Modify the parser so that \texttt{*?} is interpreted as a single token
meaning “lazy match zero or more”.

\subsection*{Character sets}


Modify the parser so that expressions like \texttt{[xyz]} are interpreted to mean
“match any one of the characters x, y, or z”.

\subsection*{Back reference}


Modify the tokenizer so that it recognizes \texttt{{\textbackslash}1}, \texttt{{\textbackslash}2}, and so on to mean “back reference”.
The number may contain any number of digits.

\subsection*{Named groups}

\begin{enumerate}

\item 

Modify the tokenizer to recognize named groups.
    For example, the named group \texttt{/(?<triple>aaa)/}
    would create a named group called \texttt{triple} that matches exactly three consecutive occurrences of ‘a’.



\item 

Write Mocha tests for your modified tokenizer.
    Does it handle nested named groups?



\end{enumerate}

\subsection*{Object streams}


Write a parser that turns files of key-value pairs separated by blank lines into objects.
For example, if the input is:

\begin{lstlisting}[frame=tblr]
left: "left value"
first: 1

middle: "middle value"
second: 2

right: "right value"
third: 3
\end{lstlisting}


\noindent then the output will be:

\begin{lstlisting}[frame=tblr]
[
  {left: "left value", first: 1},
  {middle: "middle value", second: 2},
  {right: "right value", third: 3}
]
\end{lstlisting}


Keys are always upper- and lower-case characters;
values may be strings in double quotes or unquoted numbers.

\subsection*{Tokenize HTML}

\begin{enumerate}

\item 

Write a tokenizer for a subset of HTML that consists of:

\begin{itemize}

\item Opening tags without attributes, such as \texttt{<div>} and \texttt{<p>}

\item Closing tags, such as \texttt{</p>} and \texttt{</div>}

\item Plain text between tags that does \emph{not} contain ‘<’ or ‘>’ characters

\end{itemize}



\item 

Modify the tokenizer to handle \texttt{key="value"} attributes in opening tags.



\item 

Write Mocha tests for your tokenizer.



\end{enumerate}

\subsection*{The Shunting-Yard Algorithm}

\begin{enumerate}

\item 

Use the \hreffoot{shunting-yard algorithm}{https://en.wikipedia.org/wiki/Shunting-yard\_algorithm}
    to implement a tokenizer for a simple subset of arithmetic that includes:

\begin{itemize}

\item single-letter variable names

\item single-digit numbers

\item the \texttt{+}, \texttt{*}, and \texttt{{\textasciicircum}} operators, where \texttt{+} has the lowest precedence and \texttt{{\textasciicircum}} has the highest

\end{itemize}



\item 

Write Mocha tests for your tokenizer.



\end{enumerate}

\subsection*{Handling errors}

\begin{enumerate}

\item 

What does the regular expression tokenizer do
    with expressions that contain unmatched opening parentheses like \texttt{/a(b/}?
    What about expressions that contain unmatched closing parentheses like \texttt{/ab)/}?



\item 

Modify it so it produces a more useful error message.



\end{enumerate}

\chapter{Page Templates}\label{page-templates}


\noindent 
  Terms defined: \glossref{bare object}, \glossref{dynamic scoping}, \glossref{environment}, \glossref{lexical scoping}, \glossref{stack frame}, \glossref{static site generator}, \glossref{Visitor pattern}



Every program needs documentation in order to be usable,
and the best place to put that documentation is on the web.
Writing and updating pages by hand is time-consuming and error-prone,
particularly when many parts are the same,
so most documentation sites use some kind of
\glossref{static site generator}\index{static site generator}
to create web pages from templates.


At the heart of every static site generator is a page templating system.
Thousands of these have been written in the last thirty years
in every popular programming language
(and one language, \hreffoot{PHP}{https://www.php.net/}\index{PHP}, was created for this purpose).
Most of these systems use one of three designs
(\figref{page-templates-options}):

\begin{enumerate}

\item 

Mix commands in a language such as JavaScript with the HTML or Markdown
    using some kind of marker to indicate which parts are commands
    and which parts are to be taken as-is.
    This approach is taken by \hreffoot{EJS}{https://ejs.co/}\index{EJS},
    which we used to write these lessons.



\item 

Create a mini-language with its own commands like \hreffoot{Jekyll}{https://jekyllrb.com/}\index{Jekyll}
    (which is used by \hreffoot{GitHub Pages}{https://pages.github.com/}\index{GitHub Pages}).
    Mini-languages are appealing because they are smaller and safer than general-purpose languages,
    but experience shows that they eventually grow
    most of the features of a general-purpose language.
    Again, some kind of marker must be used to show
    which parts of the page are code and which are ordinary text.



\item 

Put directives in specially-named attributes in the HTML.
    This approach has been the least popular,
    but since pages are valid HTML,
    it eliminates the need for a special parser.



\end{enumerate}

\figpdf{page-templates-options}{./page-templates/options.pdf}{Three different ways to implement page templating.}{0.6}


\noindent In this chapter we will build a simple page templating system using the third strategy.
We will process each page independently by parsing the HTML
and walking the DOM\index{DOM} to find nodes with special attributes.
Our program will execute the instructions in those nodes
to do the equivalent of loops and if/else statements;
other nodes will be copied as-is to create text.

\section{What will our system look like?}\label{page-templates-syntax}


Let’s start by deciding what “done” looks like.
Suppose we want to turn an array of strings into an HTML list.
Our page will look like this:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body>
    <p>Expect three items</p>
    <ul z-loop="item:names">
      <li><span z-var="item"/></li>
    </ul>
  </body>
</html>
\end{lstlisting}



\noindent The attribute \texttt{z-loop} tells the tool to repeat the contents of that node;
the loop variable and the collection being looped over are separated by a colon.
The attribute \texttt{z-var} tells the tool to fill in the node with the value of the variable.


When our tool processes this page,
the output will be standard HTML without any traces of how it was created:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body style="font-size: 200%; margin-left: 0.5em">
    <p>Expect three items</p>
    <ul>
      <li><span>Johnson</span></li>

      <li><span>Vaughan</span></li>

      <li><span>Jackson</span></li>
    </ul>
  </body>
</html>
\end{lstlisting}


\begin{callout}


\subsubsection*{Human-readable vs. machine-readable}


The introduction said that mini-languages for page templating
quickly start to accumulate extra features.
We have already started down that road
by putting the loop variable and loop target in a single attribute
and splitting that attribute to get them out.
Doing this makes loops easy for people to type,
but hides important information from standard HTML processing tools.
They can’t know that this particular attribute of these particular elements
contains multiple values
or that those values should be extracted by splitting a string on a colon.
We could instead require people to use two attributes, as in:

\begin{lstlisting}[frame=tblr]
<ul z-loop="names" z-loop-var="item">
\end{lstlisting}


\noindent but we have decided to err on the side of minimal typing.
And note that strictly speaking,
we should call our attributes \texttt{data-something} instead of \texttt{z-something}
to conform with \hreffoot{the HTML5 specification}{https://developer.mozilla.org/en-US/docs/Learn/HTML/Howto/Use\_data\_attributes}\index{HTML5 specification},
but by the time we’re finished processing our templates,
there shouldn’t be any \texttt{z-*} attributes left to confuse a browser.

\end{callout}


The next step is to define the API for filling in templates.
Our tool needs the template itself,
somewhere to write its output,
and some variables to use in the expansion.
These variables might come from a configuration file,
from a YAML header in the file itself,
or from some mix of the two;
for the moment,
we will just pass them into the expansion function as an object:


\begin{lstlisting}[frame=tblr]
const variables = {
  names: ['Johnson', 'Vaughan', 'Jackson']
}
const dom = readHtml('template.html')
const expander = new Expander(dom, variables)
expander.walk()
console.log(expander.result)
\end{lstlisting}


\section{How can we keep track of values?}\label{page-templates-values}


Speaking of variables,
we need a way to keep track of their current values;
we say “current” because the value of a loop variable changes each time we go around the loop.
We also need to maintain multiple sets of variables
so that variables used inside a loop
don’t conflict with ones used outside it.
(We don’t actually “need” to do this—we could just have one global set of variables—but
experience teaches us that if all our variables are global,
all of our programs will be buggy.)


The standard way to manage variables is to create a stack of lookup tables.
Each \glossref{stack frame}\index{stack frame} is an object with names and values;
when we need to find a variable,
we look through the stack frames in order to find the uppermost definition of that variable..

\begin{callout}


\subsubsection*{Scoping rules}


Searching the stack frame\index{call stack!stack frame}\index{stack frame}-by-frame
while the program is running
is called is \glossref{dynamic scoping}\index{dynamic scoping}\index{scoping!dynamic},
since we find variables while the program is running.
In contrast,
most programming languages used \glossref{lexical scoping}\index{lexical scoping}\index{scoping!lexical},
which figures out what a variable name refers to based on the structure of the program text.

\end{callout}


The values in a running program are sometimes called
an \glossref{environment}\index{environment (to store variables)}\index{call stack!environment},
so we have named our stack-handling class \texttt{Env}.
Its methods let us push and pop new stack frames
and find a variable given its name;
if the variable can’t be found,
\texttt{Env.find} returns \texttt{undefined} instead of throwing an exception
(\figref{page-templates-stack}).


\begin{lstlisting}[frame=tblr]
class Env {
  constructor (initial) {
    this.stack = []
    this.push(Object.assign({}, initial))
  }

  push (frame) {
    this.stack.push(frame)
  }

  pop () {
    this.stack.pop()
  }

  find (name) {
    for (let i = this.stack.length - 1; i >= 0; i--) {
      if (name in this.stack[i]) {
        return this.stack[i][name]
      }
    }
    return undefined
  }

  toString () {
    return JSON.stringify(this.stack)
  }
}

export default Env
\end{lstlisting}


\figpdf{page-templates-stack}{./page-templates/stack.pdf}{Using a stack to manage variables.}{0.6}

\section{How do we handle nodes?}\label{page-templates-nodes}


HTML pages have a nested structure,
so we will process them using
the \glossref{Visitor}\index{Visitor pattern}\index{design pattern!Visitor} design pattern.
\texttt{Visitor}‘s constructor takes the root node of the DOM tree as an argument and saves it.
When we call \texttt{Visitor.walk} without a value,
it starts recursing from that saved root;
if \texttt{.walk} is given a value (as it is during recursive calls),
it uses that instead.


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

class Visitor {
  constructor (root) {
    this.root = root
  }

  walk (node = null) {
    if (node === null) {
      node = this.root
    }
    if (this.open(node)) {
      node.children.forEach(child => {
        this.walk(child)
      })
    }
    this.close(node)
  }

  open (node) {
    assert(false,
      'Must implemented "open"')
  }

  close (node) {
    assert(false,
      'Must implemented "close"')
  }
}

export default Visitor
\end{lstlisting}



\noindent \texttt{Visitor} defines two methods called \texttt{open} and \texttt{close} that are called
when we first arrive at a node and when we are finished with it
(\figref{page-templates-visitor}).
The default implementations of these methods throw exceptions
to remind the creators of derived classes to implement their own versions.

\figpdf{page-templates-visitor}{./page-templates/visitor.pdf}{Using the Visitor pattern to evaluate a page template.}{0.6}


The \texttt{Expander} class is specialization of \texttt{Visitor}
that uses an \texttt{Env} to keep track of variables.
It imports a handler
for each type of special node we support—we will write those in a moment—and
uses them to process each type of node:

\begin{enumerate}

\item 

If the node is plain text, copy it to the output.



\item 

If there is a handler for the node,
    call the handler’s \texttt{open} or \texttt{close} method.



\item 

Otherwise, open or close a regular tag.



\end{enumerate}


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import Visitor from './visitor.js'
import Env from './env.js'

import z_if from './z-if.js'
import z_loop from './z-loop.js'
import z_num from './z-num.js'
import z_var from './z-var.js'

const HANDLERS = {
  'z-if': z_if,
  'z-loop': z_loop,
  'z-num': z_num,
  'z-var': z_var
}

class Expander extends Visitor {
  constructor (root, vars) {
    super(root)
    this.env = new Env(vars)
    this.handlers = HANDLERS
    this.result = []
  }

  open (node) {
    if (node.type === 'text') {
      this.output(node.data)
      return false
    } else if (this.hasHandler(node)) {
      return this.getHandler(node).open(this, node)
    } else {
      this.showTag(node, false)
      return true
    }
  }

  close (node) {
    if (node.type === 'text') {
      return
    }
    if (this.hasHandler(node)) {
      this.getHandler(node).close(this, node)
    } else {
      this.showTag(node, true)
    }
  }

}

export default Expander
\end{lstlisting}



Checking to see if there is a handler for a particular node
and getting that handler are straightforward—we just
look at the node’s attributes:


\begin{lstlisting}[frame=tblr]
  hasHandler (node) {
    for (const name in node.attribs) {
      if (name in this.handlers) {
        return true
      }
    }
    return false
  }

  getHandler (node) {
    const possible = Object.keys(node.attribs)
      .filter(name => name in this.handlers)
    assert(possible.length === 1,
      'Should be exactly one handler')
    return this.handlers[possible[0]]
  }
\end{lstlisting}



Finally, we need a few helper methods to show tags and generate output:


\begin{lstlisting}[frame=tblr]
  showTag (node, closing) {
    if (closing) {
      this.output(`</${node.name}>`)
      return
    }

    this.output(`<${node.name}`)
    if (node.name === 'body') {
      this.output(' style="font-size: 200%; margin-left: 0.5em"')
    }
    for (const name in node.attribs) {
      if (!name.startsWith('z-')) {
        this.output(` ${name}="${node.attribs[name]}"`)
      }
    }
    this.output('>')
  }

  output (text) {
    this.result.push((text === undefined) ? 'UNDEF' : text)
  }

  getResult () {
    return this.result.join('')
  }
\end{lstlisting}



\noindent Notice that this class adds strings to an array and joins them all right at the end
rather than concatenating strings repeatedly.
Doing this is more efficient and also helps with debugging,
since each string in the array corresponds to a single method call.

\section{How do we implement node handlers?}\label{page-templates-handlers}


At this point
we have built a lot of infrastructure but haven’t actually processed any special nodes.
To do that,
let’s write a handler that copies a constant number into the output:


\begin{lstlisting}[frame=tblr]
export default {
  open: (expander, node) => {
    expander.showTag(node, false)
    expander.output(node.attribs['z-num'])
  },

  close: (expander, node) => {
    expander.showTag(node, true)
  }
}
\end{lstlisting}



\noindent When we enter a node like \texttt{<span z-num="123"/>}
this handler asks the expander to show an opening tag
followed by the value of the \texttt{z-num} attribute.
When we exit the node,
the handler asks the expander to close the tag.
The handler doesn’t know whether things are printed immediately,
added to an output list,
or something else;
it just knows that whoever called it implements the low-level operations it needs.


Note that this expander is \emph{not} a class,
but instead an object with two functions stored under the keys \texttt{open} and \texttt{close}.
We could use a class for each handler
so that handlers can store any extra state they need,
but \glossref{bare objects}\index{bare object}\index{software design!bare object} are common and useful in JavaScript
(though we will see below that we \emph{should} have used classes).


So much for constants; what about variables?


\begin{lstlisting}[frame=tblr]
export default {
  open: (expander, node) => {
    expander.showTag(node, false)
    expander.output(expander.env.find(node.attribs['z-var']))
  },

  close: (expander, node) => {
    expander.showTag(node, true)
  }
}
\end{lstlisting}



\noindent This code is almost the same as the previous example.
The only difference is that instead of copying the attribute’s value
directly to the output,
we use it as a key to look up a value in the environment.


These two pairs of handlers look plausible, but do they work?
To find out,
we can build a program that loads variable definitions from a JSON file,
reads an HTML template,
and does the expansion:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'
import htmlparser2 from 'htmlparser2'

import Expander from './expander.js'

const main = () => {
  const vars = readJSON(process.argv[2])
  const doc = readHtml(process.argv[3])
  const expander = new Expander(doc, vars)
  expander.walk()
  console.log(expander.getResult())
}

const readJSON = (filename) => {
  const text = fs.readFileSync(filename, 'utf-8')
  return JSON.parse(text)
}

const readHtml = (filename) => {
  const text = fs.readFileSync(filename, 'utf-8')
  return htmlparser2.parseDOM(text)[0]
}

main()
\end{lstlisting}



We added new variables for our test cases one-by-one
as we were writing this chapter.
To avoid repeating text repeatedly,
we show the entire set once:


\begin{lstlisting}[frame=tblr]
{
  "firstVariable": "firstValue",
  "secondVariable": "secondValue",
  "variableName": "variableValue",
  "showThis": true,
  "doNotShowThis": false,
  "names": ["Johnson", "Vaughan", "Jackson"]
}
\end{lstlisting}



Our first test:
is static text copied over as-is (\figref{page-templates-output-static-text})?


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body>
    <h1>Static Text</h1>
    <p>This page has:</p>
    <ul>
      <li>static</li>
      <li>text</li>
    </ul>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node template.js vars.json input-static-text.html
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body style="font-size: 200%; margin-left: 0.5em">
    <h1>Static Text</h1>
    <p>This page has:</p>
    <ul>
      <li>static</li>
      <li>text</li>
    </ul>
  </body>
</html>
\end{lstlisting}


\figpdf{page-templates-output-static-text}{./page-templates/output-static-text.pdf}{Static text generated by page templates.}{0.6}


Good.
Now, does the expander handle constants (\figref{page-templates-output-single-constant})?


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body>
    <p><span z-num="123"/></p>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body style="font-size: 200%; margin-left: 0.5em">
    <p><span>123</span></p>
  </body>
</html>
\end{lstlisting}


\figpdf{page-templates-output-single-constant}{./page-templates/output-single-constant.pdf}{A single constant generated by page templates.}{0.6}


What about a single variable (\figref{page-templates-output-single-variable})?


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body>
    <p><span z-var="variableName"/></p>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body style="font-size: 200%; margin-left: 0.5em">
    <p><span>variableValue</span></p>
  </body>
</html>
\end{lstlisting}


\figpdf{page-templates-output-single-variable}{./page-templates/output-single-variable.pdf}{A single variable generated by page templates.}{0.6}


What about a page containing multiple variables?
There’s no reason it should fail if the single-variable case works,
but we should still check—again,
software isn’t done until it has been tested (\figref{page-templates-output-multiple-variables}).


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body>
    <p><span z-var="firstVariable" /></p>
    <p><span z-var="secondVariable" /></p>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body style="font-size: 200%; margin-left: 0.5em">
    <p><span>firstValue</span></p>
    <p><span>secondValue</span></p>
  </body>
</html>
\end{lstlisting}


\figpdf{page-templates-output-multiple-variables}{./page-templates/output-multiple-variables.pdf}{Multiple variables generated by page templates.}{0.6}

\section{How can we implement control flow?}\label{page-templates-flow}


Our tool supports two types of control flow:
conditional expressions and loops.
Since we don’t support Boolean expressions like \texttt{and} and \texttt{or},
implementing a conditional is as simple as looking up a variable
(which we know how to do)
and then expanding the node if the value is true:


\begin{lstlisting}[frame=tblr]
export default {
  open: (expander, node) => {
    const doRest = expander.env.find(node.attribs['z-if'])
    if (doRest) {
      expander.showTag(node, false)
    }
    return doRest
  },

  close: (expander, node) => {
    if (expander.env.find(node.attribs['z-if'])) {
      expander.showTag(node, true)
    }
  }
}
\end{lstlisting}



Let’s test it (\figref{page-templates-output-conditional}):


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body>
    <p z-if="showThis">This should be shown.</p>
    <p z-if="doNotShowThis">This should <em>not</em> be shown.</p>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body style="font-size: 200%; margin-left: 0.5em">
    <p>This should be shown.</p>

  </body>
</html>
\end{lstlisting}


\figpdf{page-templates-output-conditional}{./page-templates/output-conditional.pdf}{Conditional text generated by page templates.}{0.6}

\begin{quotation}

\subsection*{Spot the bug}


The \texttt{open} and \texttt{close} functions for \texttt{if} both check the value of the control variable.
If something inside the \texttt{if}‘s body changes that value
we could produce an opening tag without a matching closing tag.
We haven’t implemented assignment,
so there’s no way for that to happen now,
but it could be a headache if we add it later.

\end{quotation}


Finally we come to loops.
For these,
we need to get the array we’re looping over from the environment
and do something for each of its elements.
That “something” is:

\begin{enumerate}

\item 

Create a new stack frame holding the current value of the loop variable.



\item 

Expand all of the node’s children with that stack frame in place.



\item 

Pop the stack frame to get rid of the temporary variable.



\end{enumerate}


\begin{lstlisting}[frame=tblr]
export default {
  open: (expander, node) => {
    const [indexName, targetName] = node.attribs['z-loop'].split(':')
    delete node.attribs['z-loop']
    expander.showTag(node, false)
    const target = expander.env.find(targetName)
    for (const index of target) {
      expander.env.push({ [indexName]: index })
      node.children.forEach(child => expander.walk(child))
      expander.env.pop()
    }
    return false
  },

  close: (expander, node) => {
    expander.showTag(node, true)
  }
}
\end{lstlisting}



Once again,
it’s not done until we test it (\figref{page-templates-output-loop}):


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body>
    <p>Expect three items</p>
    <ul z-loop="item:names">
      <li><span z-var="item"/></li>
    </ul>
  </body>
</html>
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html>
  <body style="font-size: 200%; margin-left: 0.5em">
    <p>Expect three items</p>
    <ul>
      <li><span>Johnson</span></li>

      <li><span>Vaughan</span></li>

      <li><span>Jackson</span></li>
    </ul>
  </body>
</html>
\end{lstlisting}


\figpdf{page-templates-output-loop}{./page-templates/output-loop.pdf}{Repeated text generated with a loop by page templates.}{0.6}


Notice how we create the new stack frame using:

\begin{lstlisting}[frame=tblr]
{ [indexName]: index }
\end{lstlisting}


\noindent This is an ugly but useful trick.
We can’t write:

\begin{lstlisting}[frame=tblr]
{ indexName: index }
\end{lstlisting}


\noindent because that would create an object with the string \texttt{indexName} as a key,
rather than one with the value of the variable \texttt{indexName} as its key.
We can’t do this either:

\begin{lstlisting}[frame=tblr]
{ `${indexName}`: index }
\end{lstlisting}


\noindent though it seems like we should be able to.
Instead,
we create an array containing the string we want.
Since JavaScript automatically converts arrays to strings
by concatenating their elements when it needs to,
our expression is a quick way to get the same effect as:

\begin{lstlisting}[frame=tblr]
const temp = {}
temp[indexName] = index
expander.env.push(temp)
\end{lstlisting}


\noindent Those three lines \emph{are} much easier to understand, though,
so we should probably have been less clever.

\section{How did we know how to do all of this?}\label{page-templates-learning}


We have just implemented a simple programming language.
It can’t do arithmetic,
but if we wanted to add tags like:

\begin{lstlisting}[frame=tblr]
<span z-math="+"><span z-var="width"/><span z-num="1"//>
\end{lstlisting}


\noindent we could.
It’s unlikely anyone would use the result—typing all of that
is so much clumsier than typing \texttt{width+1} that people wouldn’t use it
unless they had no other choice—but the basic design is there.


We didn’t invent any of this from scratch,
any more than we invented the parsing algorithm of \chapref{regex-parser}.
Instead,
we did what you are doing now:
we read what other programmers had written
and tried to make sense of the key ideas.


The problem is that “making sense” depends on who we are.
When we use a low-level language,
we incur the cognitive load\index{cognitive load} of assembling micro-steps into something more meaningful.
When we use a high-level language,
on the other hand,
we incur a similar load translating functions of functions (of functions...)
(or meta-classes templated on object factories)
into actual operations on actual data.


More experienced programmers are more capable at both ends of the curve,
but that’s not the only thing that changes.
If a novice’s comprehension curve looks like the one on the left
of \figref{page-templates-comprehension},
then an expert’s looks like the one on the right.
Experts don’t just understand more at all levels of abstraction;
their \emph{preferred} level has also shifted
so that $\sqrt{x^2 + y^2}$
is actually more readable than the medieval expression
“the side of the square whose area is the sum of the areas of the two squares
whose sides are given by the first part and the second part”.

\figpdf{page-templates-comprehension}{./page-templates/comprehension.pdf}{Novice and expert comprehension curves.}{0.6}


One implication of this is that for any given task,
the software that is quickest for a novice to comprehend
will almost certainly be different from the software that
an expert can understand most quickly.
In an ideal world our tools would automatically re-represent programs at different levels,
so that with a click of a button we could view our code as either:

\begin{lstlisting}[frame=tblr]
const hosts = links.map(a => a.href.split(':')[1].split('/')[0]).unique()
\end{lstlisting}


or:

\begin{lstlisting}[frame=tblr]
hosts = []
for (each a in links) do
  temp <- attr(a, 'href').split(':')[1].split('/')[0]
  if (not (temp in hosts)) do
    hosts.append(temp)
  end
end
\end{lstlisting}


\noindent just as we could change the colors used for syntax highlighting
or the depth to which loop bodies are indented.
But today’s tools don’t do that,
and I suspect that any tool smart enough to translate between comprehension levels automatically
would also be smart enough to write the code without our help.

\section{Exercises}\label{page-templates-exercises}

\subsection*{Tracing execution}


Add a directive \texttt{<span z-trace="variable"/>}
that prints the current value of a variable using \texttt{console.error} for debugging.

\subsection*{Unit tests}


Write unit tests for template expansion using Mocha.

\subsection*{Trimming text}


Modify all of the directives to take an extra optional attribute \texttt{z-trim="true"}
If this attribute is set,
leading and trailing whitespace is trimmed from the directive’s expansion.

\subsection*{Literal text}


Add a directive \texttt{<div z-literal="true">...</div>} that copies the enclosed text as-is
without interpreting or expanding any contained directives.
(A directive like this would be needed when writing documentation for the template expander.)

\subsection*{Including other files}

\begin{enumerate}

\item 

Add a directive \texttt{<div z-include="filename.html"/>} that includes another file
    in the file being processed.



\item 

Should included files be processed and the result copied into the including file,
    or should the text be copied in and then processed?
    What difference does it make to the way variables are evaluated?



\end{enumerate}

\subsection*{HTML snippets}


Add a directive \texttt{<div z-snippet="variable">...</div>} that saves some text in a variable
so that it can be displayed later.
For example:

\begin{lstlisting}[frame=tblr]
<html>
  <body>
    <div z-snippet="prefix"><strong>Important:</strong></div>
    <p>Expect three items</p>
    <ul>
      <li z-loop="item:names">
        <span z-var="prefix"><span z-var="item"/>
      </li>
    </ul>
  </body>
</html>
\end{lstlisting}


\noindent would printed the word “Important:” in bold before each item in the list.

\subsection*{YAML headers}


Modify the template expander to handle variables defined in a YAML header in the page being processed.
For example, if the page is:

\begin{lstlisting}[frame=tblr]
---
name: "Dorothy Johnson Vaughan"
---
<html>
  <body>
    <p><span z-var="name"/></p>
  </body>
</html>
\end{lstlisting}


\noindent will create a paragraph containing the given name.

\subsection*{Expanding all files}


Write a program \texttt{expand-all.js} that takes two directory names as command-line arguments
and builds a website in the second directory by expanding all of the HTML files found in the first
or in sub-directories of the first.

\subsection*{Counting loops}


Add a directive \texttt{<div z-index="indexName" z-limit="limitName">...</div>}
that loops from zero to the value in the variable \texttt{limitName},
putting the current iteration index in \texttt{indexName}.

\subsection*{Auxiliary functions}

\begin{enumerate}

\item 

Modify \texttt{Expander} so that it takes an extra argument \texttt{auxiliaries}
    containing zero or more named functions:

\begin{lstlisting}[frame=tblr]
const expander = new Expander(root, vars, {
  max: Math.max,
  trim: (x) => x.trim()
})
\end{lstlisting}



\item 

Add a directive \texttt{<span z-call="functionName" z-args="var,var"/>}
    that looks up a function in \texttt{auxiliaries} and calls it
    with the given variables as arguments.



\end{enumerate}

\chapter{Build Manager}\label{build-manager}


\noindent 
  Terms defined: \glossref{automatic variable}, \glossref{build manager}, \glossref{build recipe}, \glossref{build rule}, \glossref{build target}, \glossref{compiled language}, \glossref{cycle (in a graph)}, \glossref{dependency}, \glossref{directed acyclic graph}, \glossref{driver}, \glossref{interpreted language}, \glossref{link (a program)}, \glossref{pattern rule}, \glossref{runnable documentation}, \glossref{stale (in build)}, \glossref{Template Method pattern}, \glossref{topological order}



Suppose we are using a page templating system to create a website (\chapref{page-templates}).
If we a change a single page our tool should translate it,
but shouldn’t waste time translating others.
If we change a template,
on the other hand,
the tool should realize that every page in the site is potentially affected
and automatically re-translate all of them.


Choosing what actions to take based on how files depend on one another is a common pattern.
For example,
programs in \glossref{compiled languages}\index{compiled language}\index{language!compiled}
like C\index{C} and Java\index{Java}
have to be translated into lower-level forms before they can run.
In fact,
there are usually two stages to the translation:
compiling each source file into some intermediate form,
and then \glossref{linking}\index{linking (compiled language)}\index{compiled language!linking} the compiled modules
to each other and to libraries
to create a runnable program
(\figref{build-manager-compiling}).
If a source file hasn’t changed,
there’s no need to recompile it before linking.

\figpdf{build-manager-compiling}{./build-manager/compiling.pdf}{Compiling source files and linking the resulting modules.}{0.6}


A \glossref{build manager}\index{build manager} takes a description of what depends on what,
figures out which files are out of date,
determines an order in which to rebuild things,
and then executes any necessary steps.
Originally created to manage compilation,
they are also useful for programs written in \glossref{interpreted languages}\index{language!interpreted}\index{interpreted language}
like JavaScript
when we want to bundle multiple modules into a single loadable file (\chapref{module-bundler})
or re-create documentation from source code (\chapref{doc-generator}).
In this chapter we will create a simple build manager
based on \hreffoot{Make}{https://www.gnu.org/software/make/}\index{Make}, \hreffoot{Bajel}{https://www.npmjs.com/package/bajel}\index{Bajel}, \hreffoot{Jake}{https://jakejs.com/}\index{Jake},
and other systems discussed in \cite{Smith2011}.

\section{What’s in a build manager?}\label{build-manager-contents}


The input to a build manager is a set of rules,
each of which has:

\begin{itemize}

\item 

a \glossref{target}\index{build target}\index{target!build}, which is the file to be updated;



\item 

some \glossref{dependencies}\index{dependency (in build)}\index{build!dependency}, which are the things that file depends on;
    and



\item 

a \glossref{recipe}\index{recipe (in build)}\index{build!recipe} that specifies how to update the target
    if it is out of date compared to its dependencies.



\end{itemize}


\noindent The target of one rule can be a dependency of another rule,
so the relationships between the files form a \glossref{directed acyclic graph}\index{directed acyclic graph (DAG)}\index{DAG} or DAG
(\figref{build-manager-dependencies}).
The graph is directed because “A depends on B” is a one-way relationship;
it cannot contain cycles (or loops) because
if something depends on itself we can never finish updating it.
We say that a target is \glossref{stale}\index{stale (in build)}\index{build!stale} if it is older than any of its dependencies.
When this happens,
we use the recipes to bring it up to date.

\figpdf{build-manager-dependencies}{./build-manager/dependencies.pdf}{How a build manager finds and respects dependencies.}{0.6}


Our build manager must:

\begin{enumerate}

\item 

Read a file containing rules.



\item 

Construct the dependency graph.



\item 

Figure out which targets are stale.



\item 

Build those targets,
    making sure to build things \emph{before} anything that depends on them is built.



\end{enumerate}

\begin{callout}


\subsubsection*{Topological order}


A \glossref{topological ordering}\index{topological order} of a graph
arranges the nodes so that every node comes after everything it depends on.
For example,
if A depends on both B and C,
then (B, C, A) and (C, B, A) are both valid topological orders of the graph.

\end{callout}

\section{Where should we start?}\label{build-manager-start}


We will store our rules in YAML files like this:


\begin{lstlisting}[frame=tblr]
- target: A
  depends:
  - B
  - C
  recipes:
  - "update A from B and C"
- target: B
  depends:
  - C
  recipes:
  - "update B from C"
- target: C
  depends: []
  recipes: []
\end{lstlisting}



\noindent We could equally well have used JSON,
but it wouldn’t have made sense to use CSV:
rules have a nested structure,
and CSV doesn’t represent nesting particularly gracefully.


We are going to create our build manager in stages,
so we start by writing a simple \glossref{driver}\index{software design!driver} that loads a JavaScript source file,
creates an object of whatever class that file exports,
and runs the \texttt{.build} method of that object with the rest of the command-line parameters:


\begin{lstlisting}[frame=tblr]
const main = async () => {
  const BuilderClass = (await import(process.argv[2])).default
  const builder = new BuilderClass(...process.argv.slice(3))
  try {
    builder.build()
  } catch (err) {
    console.error('Build failed:', err)
  }
}

main()
\end{lstlisting}



\noindent We use the \texttt{import} function to dynamically load files in \chapref{unit-test} as well.
It only saves us a few lines of code in this case,
but we will use this idea of a general-purpose driver for larger programs in future chapters.


To work with our driver,
each version of our build manager must be a class that satisfies two requirements:

\begin{enumerate}

\item 

Its constructor must take a configuration file as an argument.



\item 

It must provide a \texttt{build} method that needs no arguments.



\end{enumerate}


\noindent The \texttt{build} method must create a graph from the configuration file,
check that it does not contain any \glossref{cycles},
and then run whatever commands are needed to update stale targets.
Just as we built a generic \texttt{Visitor}\index{Visitor pattern}\index{design pattern!Visitor} class in \chapref{page-templates},
we can build a generic base class for our build manager that does these steps in this order
without actually implementing any of them:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

class SkeletonBuilder {
  constructor (configFile) {
    this.configFile = configFile
  }

  build () {
    this.loadConfig()
    this.buildGraph()
    this.checkCycles()
    this.run()
  }

  loadConfig () {
    assert(false, 'not implemented')
  }

  buildGraph () {
    assert(false, 'not implemented')
  }

  checkCycles () {
    assert(false, 'not implemented')
  }

  run () {
    assert.fail('run method not implemented')
  }
}

export default SkeletonBuilder
\end{lstlisting}



This is an example of
the \glossref{Template Method}\index{Template Method pattern}\index{design pattern!Template Method} design pattern:
the parent class defines the order of the steps
and child classes fill them in
(\figref{build-manager-template-method}).
This design pattern ensures that every child does the same things in the same order,
even if the details of \emph{how} vary from case to case.

\figpdf{build-manager-template-method}{./build-manager/template-method.pdf}{The Template Method pattern in action.}{0.6}


We would normally implement all of the methods required by the \texttt{build} method at the same time;
here, we will write them them one-by-one to make the evolving code easier to follow.
The \texttt{loadConfig} method loads the configuration file
as the builder object is being constructed:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'
import fs from 'fs'
import yaml from 'js-yaml'

import SkeletonBuilder from './skeleton-builder.js'

class ConfigLoader extends SkeletonBuilder {
  loadConfig () {
    this.config = yaml.safeLoad(fs.readFileSync(this.configFile, 'utf-8'))

    assert(Array.isArray(this.config),
      'Configuration must be array')

    this.config.forEach(rule => {
      assert(('target' in rule) && (typeof rule.target === 'string'),
        `Rule ${JSON.stringify(rule)} does not string as 'target'`)

      assert(('depends' in rule) &&
        Array.isArray(rule.depends) &&
        rule.depends.every(dep => (typeof dep === 'string')),
        `Bad 'depends' for rule ${JSON.stringify(rule)}`)

      assert(('recipes' in rule) &&
        Array.isArray(rule.recipes) &&
        rule.recipes.every(recipe => (typeof recipe === 'string')),
        `Bad 'recipes' for rule ${JSON.stringify(rule)}`)
    })
  }
}

export default ConfigLoader
\end{lstlisting}



\noindent The first line does the loading;
the rest of the method checks that the rules are at least superficially plausible.
We need these checks because YAML is a generic file format
that doesn’t know anything about the extra requirements of our rules.
And as we first saw in \chapref{async-programming},
we have to specify that the character encoding of our file is UTF-8
so that JavaScript knows how to convert bytes into text.


The next step is to turn the configuration into a graph in memory.
We use the \hreffoot{\texttt{graphlib}}{https://www.npmjs.com/package/graphlib} module to manage nodes and links
rather than writing our own classes for graphs,
and store the recipe to rebuild a node in that node.
Two features of \texttt{graphlib} that took us a while to figure out are that:

\begin{enumerate}

\item 

links go \emph{from} the dependency \emph{to} the target,
    and



\item 

\texttt{setEdge} automatically adds nodes if they aren’t already present.



\end{enumerate}


\noindent \texttt{graphlib} provides implementations of some common graph algorithms,
including one to check for cycles,
so we might as well write that method at this point as well:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'
import graphlib from '@dagrejs/graphlib'

import ConfigLoader from './config-loader.js'

class GraphCreator extends ConfigLoader {
  buildGraph () {
    this.graph = new graphlib.Graph()
    this.config.forEach(rule => {
      this.graph.setNode(rule.target, {
        recipes: rule.recipes
      })
      rule.depends.forEach(dep => this.graph.setEdge(dep, rule.target))
    })
  }

  checkCycles () {
    const cycles = graphlib.alg.findCycles(this.graph)
    assert.strictEqual(cycles.length, 0,
      `Dependency graph contains cycles ${cycles}`)
  }
}

export default GraphCreator
\end{lstlisting}



We can now create something that displays our configuration when it runs
but does nothing else:


\begin{lstlisting}[frame=tblr]
import graphlib from '@dagrejs/graphlib'

import GraphCreator from './graph-creator.js'

class DisplayOnly extends GraphCreator {
  run () {
    console.log('Graph')
    console.log(graphlib.json.write(this.graph))
    console.log('Sorted')
    console.log(graphlib.alg.topsort(this.graph))
  }
}

export default DisplayOnly
\end{lstlisting}



If we run this with our three simple rules as input,
it shows the graph with \texttt{v} and \texttt{w} keys to represent the ends of the links:


\begin{lstlisting}[frame=shadowbox]
node driver.js ./display-only.js three-simple-rules.yml
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
Graph
{
  options: { directed: true, multigraph: false, compound: false },
  nodes: [
    { v: 'A', value: [Object] },
    { v: 'B', value: [Object] },
    { v: 'C', value: [Object] }
  ],
  edges: [ { v: 'B', w: 'A' }, { v: 'C', w: 'A' }, { v: 'C', w: 'B' } ]
}
Sorted
[ 'C', 'B', 'A' ]
\end{lstlisting}



Let’s write a quick test to make sure the cycle detector works as intended:


\begin{lstlisting}[frame=tblr]
- target: A
  depends:
  - B
  recipes:
  - "update A from B"
- target: B
  depends:
  - A
  recipes:
  - "update B from A"
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node driver.js ./display-only.js circular-rules.yml
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
Build failed: AssertionError [ERR_ASSERTION]: Dependency graph contains \
 cycles B,A
    at DisplayOnly.checkCycles \
     (/u/stjs/build-manager/graph-creator.js:19:12)
    at DisplayOnly.build \
     (/u/stjs/build-manager/skeleton-builder.js:11:10)
    at main (/u/stjs/build-manager/driver.js:5:13) {
  generatedMessage: false,
  code: 'ERR_ASSERTION',
  actual: 1,
  expected: 0,
  operator: 'strictEqual'
}
\end{lstlisting}


\section{How can we specify that a file is out-of-date?}\label{build-manager-timestamp}


The next step is to figure out which files are out-of-date.
Make does this by comparing the timestamps\index{timestamp!in build}\index{build!timestamp} of the files in question,
but this isn’t always reliable:
computers’ clocks may be slightly out of sync\index{clock synchronization (in build)}\index{build!clock synchronization},
which can produce a wrong answer on a networked filesystem,
and the operating system may only report file update times to the nearest millisecond
(which seemed very short in 1970 but seems very long today).


More modern build systems store a hash\index{hash code!in build}\index{build!hash code} of each file’s contents
and compare the current hash to the stored one to see if the file has changed.
Since we already looked at hashing in \chapref{file-backup},
we will use the timestamp approach here.
And instead of using a mock filesystem as we did in \chapref{file-backup},
we will simply load another configuration file that specifies fake timestamps for files:


\begin{lstlisting}[frame=tblr]
A: 2
B: 5
C: 8
\end{lstlisting}



Since we want to associate those timestamps with files,
we add a step to \texttt{buildGraph} to read the timestamp file and add information to the graph’s nodes:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'
import fs from 'fs'
import yaml from 'js-yaml'

import GraphCreator from './graph-creator.js'

class AddTimestamps extends GraphCreator {
  constructor (configFile, timesFile) {
    super(configFile)
    this.timesFile = timesFile
  }

  buildGraph () {
    super.buildGraph()
    this.addTimestamps()
  }

  addTimestamps () {
    const times = yaml.safeLoad(fs.readFileSync(this.timesFile, 'utf-8'))
    for (const node of Object.keys(times)) {
      assert(this.graph.hasNode(node),
             `Graph does not have node ${node}`)
      this.graph.node(node).timestamp = times[node]
    }
    const missing = this.graph.nodes().filter(
      n => !('timestamp' in this.graph.node(n))
    )
    assert.strictEqual(missing.length, 0,
      `Timestamp missing for node(s) ${missing}`)
  }

  run () {
    console.log(this.graph.nodes().map(
      n => `${n}: ${JSON.stringify(this.graph.node(n))}`
    ))
  }
}

export default AddTimestamps
\end{lstlisting}


\begin{callout}


\subsubsection*{Not quite what we were expecting}


The steps defined in \texttt{SkeletonBuilder.build} don’t change when we do this,
so people reading the code don’t have to change their mental model of what it does overall.
However,
if we had realized in advance that we were going to want to add timestamps from a file,
we would probably have added a step for that in the template method.
And if someone ever wants to inject a new step between building the graph and adding timestamps,
they will have to override \texttt{addTimestamps} and put their step at the top before calling \texttt{super.addTimestamps},
which will make the code a lot harder to understand.

\end{callout}


Before we move on,
let’s make sure that adding timestamps works as we want:


\begin{lstlisting}[frame=shadowbox]
node driver.js ./add-stamps.js three-simple-rules.yml add-stamps.yml
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  'A: {"recipes":["update A from B and C"],"timestamp":2}',
  'B: {"recipes":["update B from C"],"timestamp":5}',
  'C: {"recipes":[],"timestamp":8}'
]
\end{lstlisting}


\section{How can we update out-of-date files?}\label{build-manager-update}


To figure out which recipes to execute and in which order,
we set the pretended current time to the latest time of any file,
then look at each file in topological order.
If a file is older than any of its dependencies,
we update the file \emph{and} its pretended timestamp
to trigger an update of anything that depends on it.


We can pretend that updating a file always takes one unit of time,
so we advance our fictional clock by one for each build.
Using \texttt{graphlib.alg.topsort} to create the topological order,
we get this:


\begin{lstlisting}[frame=tblr]
import graphlib from '@dagrejs/graphlib'

import AddTimestamps from './add-stamps.js'

class UpdateOnTimestamps extends AddTimestamps {
  run () {
    const sorted = graphlib.alg.topsort(this.graph)
    const startTime = 1 + Math.max(...sorted.map(
      n => this.graph.node(n).timestamp))
    console.log(`${startTime}: START`)
    const endTime = sorted.reduce((currTime, node) => {
      if (this.isStale(node)) {
        console.log(`${currTime}: ${node}`)
        this.graph.node(node).recipes.forEach(
          a => console.log(`    ${a}`))
        this.graph.node(node).timestamp = currTime
        currTime += 1
      }
      return currTime
    }, startTime)
    console.log(`${endTime}: END`)
  }

  isStale (node) {
    return this.graph.predecessors(node).some(
      other => this.graph.node(other).timestamp >=
        this.graph.node(node).timestamp
    )
  }
}

export default UpdateOnTimestamps
\end{lstlisting}



The \texttt{run} method:

\begin{enumerate}

\item 

gets a sorted list of nodes;



\item 

sets the starting time to be one unit past the largest file time;
    and then



\item 

uses \texttt{Array.reduce}\index{Array.reduce} to operate on each node (i.e., each file) in order.
    If that file is stale,
    we print the steps we would run and then update the file’s timestamp.
    We only advance the notional current time when we do an update.



\end{enumerate}


\noindent To check if a file is stale,
we see if any of its dependencies currently have timestamps greater than or equal to its own.
When we run this,
it seems to do the right thing:


\begin{lstlisting}[frame=shadowbox]
node driver.js ./update-stamps.js three-simple-rules.yml add-stamps.yml
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
9: START
9: B
    update B from C
10: A
    update A from B and C
11: END
\end{lstlisting}


\section{How can we add generic build rules?}\label{build-manager-generic}


If our website has a hundred blog posts
or a hundred pages of documentation about particular JavaScript files,
we don’t want to have to write a hundred nearly-identical recipes.
Instead,
we want to be able to write generic \glossref{build rules}\index{build!rule}\index{rule (in build)} that say,
“Build all things of this kind the same way.”
These generic rules need to:

\begin{itemize}

\item 

a way to define a set of files;



\item 

a way to specify a generic rule;
    and



\item 

a way to fill in parts of that rule.



\end{itemize}


\noindent We will achieve this by overriding \texttt{buildGraph} to replace variables in recipes with values.
Once again,
object-oriented programming helps us change only what we need to change,
provided we divided our problem into sensible chunks in the first place.


Make provides \glossref{automatic variables}\index{automatic variable (in build)}\index{build!automatic variable}
with names like \texttt{\$<} and \texttt{\$@}
to represent the parts of a rule.
Ours will be more readable:
we will use \texttt{@TARGET} for the target,
\texttt{@DEPENDENCIES} for the dependencies (in order),
and \texttt{@DEP[1]}, \texttt{@DEP[2]}, and so on for specific dependencies
(\figref{build-manager-pattern-rules}).
Our variable expander looks like this:


\begin{lstlisting}[frame=tblr]
import UpdateOnTimestamps from './update-stamps.js'

class VariableExpander extends UpdateOnTimestamps {
  buildGraph () {
    super.buildGraph()
    this.expandVariables()
  }

  expandVariables () {
    this.graph.nodes().forEach(target => {
      try {
        const dependencies = this.graph.predecessors(target)
        const recipes = this.graph.node(target).recipes
        this.graph.node(target).recipes = recipes.map(act => {
          act = act
            .replace('@TARGET', target)
            .replace('@DEPENDENCIES', dependencies.join(' '))
          dependencies.forEach((dep, i) => {
            act = act.replace(`@DEP[${i}]`, dependencies[i])
          })
          return act
        })
      } catch (error) {
        console.error(`Cannot find ${target} in graph`)
        process.exit(1)
      }
    })
  }
}

export default VariableExpander
\end{lstlisting}


\figpdf{build-manager-pattern-rules}{./build-manager/pattern-rules.pdf}{Turning patterns rules into runnable commands.}{0.6}


The first thing we do is test that it works when there \emph{aren’t} any variables to expand
by running it on the same example we used previously:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
9: START
9: B
    update B from C
10: A
    update A from B C
11: END
\end{lstlisting}



\noindent This is perhaps the most important reason to create tests:
they tell us right away if something we have added or changed
has broken something that used to work.
That gives us a firm base to build on as we debug the new code.


\newpage


Now we need to add \glossref{pattern rules}\index{pattern rule (in build)}\index{build!pattern rule}.
Our first attempt at a rules file looks like this:


\begin{lstlisting}[frame=tblr]
- target: left.out
  depends: []
  recipes: []
  timestamp: 1
- target: left.in
  depends: []
  recipes: []
  timestamp: 2
- target: right.out
  depends: []
  recipes: []
  timestamp: 1
- target: right.in
  depends: []
  recipes: []
  timestamp: 3
- target: "%.out"
  depends:
  - "%.in"
  recipes:
  - "update @TARGET from @DEPENDENCIES"
\end{lstlisting}



\noindent and our first attempt at reading it extracts rules before expanding variables:


\begin{lstlisting}[frame=tblr]
import VariableExpander from './variable-expander.js'

class PatternUserAttempt extends VariableExpander {
  buildGraph () {
    super.buildGraph()
    this.extractRules()
    this.expandVariables()
  }

  extractRules () {
    this.rules = new Map()
    this.graph.nodes().forEach(target => {
      if (target.includes('%')) {
        const data = {
          recipes: this.graph.node(target).recipes
        }
        this.rules.set(target, data)
      }
    })
    this.rules.forEach((value, key) => {
      this.graph.removeNode(key)
    })
  }
}

export default PatternUserAttempt
\end{lstlisting}



However,
that doesn’t work:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
Build failed: AssertionError [ERR_ASSERTION]: Graph does not have node A
    at PatternUserAttempt.addTimestamps \
    (/u/stjs/build-manager/add-stamps.js:21:7)
    at PatternUserAttempt.buildGraph \
    (/u/stjs/build-manager/add-stamps.js:15:10)
    at PatternUserAttempt.buildGraph \
    (/u/stjs/build-manager/variable-expander.js:5:11)
    at PatternUserAttempt.buildGraph \
    (/u/stjs/build-manager/pattern-user-attempt.js:5:11)
    at PatternUserAttempt.build \
    (/u/stjs/build-manager/skeleton-builder.js:10:10)
    at main (/u/stjs/build-manager/driver.js:5:13) {
  generatedMessage: false,
  code: 'ERR_ASSERTION',
  actual: false,
  expected: true,
  operator: '=='
}
\end{lstlisting}



\noindent The problem is that our simple graph loader creates nodes for dependencies even if they aren’t targets.
As a result,
we wind up tripping over the lack of a node for \texttt{\%.in} before we get to extracting rules.

\begin{callout}


\subsubsection*{Errors become assertions}


When we first wrote \texttt{add-stamps.js},
it didn’t contain the assertion
that printed the error message shown above.
Once we tracked down our bug,
though,
we added the assertion to ensure we didn’t make the same mistake again,
and as \glossref{runnable documentation}\index{runnable documentation (assertions as)}\index{assertion!as runnable documentation}
to tell the next programmer more about the code.
Regular code tells the computer what to do;
assertions with meaningful error messages tell the reader why.

\end{callout}


We can fix our problem by rewriting the rule loader
to separate pattern rules from simple rules;
we can tell the two apart by checking if the rule’s dependencies include \texttt{\%}.
While we’re here,
we will enable timestamps as an optional field in the rules for testing purposes
rather than having them in a separate file:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'
import graphlib from '@dagrejs/graphlib'

import VariableExpander from './variable-expander.js'

class PatternUserRead extends VariableExpander {
  buildGraph () {
    this.buildGraphAndRules()
    this.expandVariables()
  }

  buildGraphAndRules () {
    this.graph = new graphlib.Graph()
    this.rules = new Map()
    this.config.forEach(rule => {
      if (rule.target.includes('%')) {
        const data = {
          recipes: rule.recipes,
          depends: rule.depends
        }
        this.rules.set(rule.target, data)
      } else {
        const timestamp = ('timestamp' in rule)
          ? rule.timestamp
          : null
        this.graph.setNode(rule.target, {
          recipes: rule.recipes,
          timestamp: timestamp
        })
        rule.depends.forEach(dep => {
          assert(!dep.includes('%'),
            'Cannot have "%" in a non-pattern rule')
          this.graph.setEdge(dep, rule.target)
        })
      }
    })
  }
}

export default PatternUserRead
\end{lstlisting}



Before we try to run this,
let’s add methods to show the state of our two internal data structures:


\begin{lstlisting}[frame=tblr]
import graphlib from '@dagrejs/graphlib'

import PatternUserRead from './pattern-user-read.js'

class PatternUserShow extends PatternUserRead {
  run () {
    console.log(JSON.stringify(this.toJSON(), null, 2))
  }

  toJSON () {
    return {
      graph: graphlib.json.write(this.graph),
      rules: Array.from(this.rules.keys()).map(key => {
        return { k: key, v: this.rules.get(key) }
      })
    }
  }
}

export default PatternUserShow
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node driver.js ./pattern-user-show.js pattern-rules.yml
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{
  "graph": {
    "options": {
      "directed": true,
      "multigraph": false,
      "compound": false
    },
    "nodes": [
      {
        "v": "left.out",
        "value": {
          "recipes": [],
          "timestamp": 1
        }
      },
      {
        "v": "left.in",
        "value": {
          "recipes": [],
          "timestamp": 2
        }
      },
      {
        "v": "right.out",
        "value": {
          "recipes": [],
          "timestamp": 1
        }
      },
      {
        "v": "right.in",
        "value": {
          "recipes": [],
          "timestamp": 3
        }
      }
    ],
    "edges": []
  },
  "rules": [
    {
      "k": "%.out",
      "v": {
        "recipes": [
          "update @TARGET from @DEPENDENCIES"
        ],
        "depends": [
          "%.in"
        ]
      }
    }
  ]
}
\end{lstlisting}



The output seems to be right,
so let’s try expanding rules \emph{after} building the graph and rules
but \emph{before} expanding variables:


\begin{lstlisting}[frame=tblr]
import PatternUserRead from './pattern-user-read.js'

class PatternUserRun extends PatternUserRead {
  buildGraph () {
    this.buildGraphAndRules()
    this.expandAllRules()
    this.expandVariables()
  }

  expandAllRules () {
    this.graph.nodes().forEach(target => {
      if (this.graph.predecessors(target).length > 0) {
        return
      }
      const data = this.graph.node(target)
      if (data.recipes.length > 0) {
        return
      }
      const rule = this.findRule(target)
      if (!rule) {
        return
      }
      this.expandRule(target, rule)
    })
  }

  findRule (target) {
    const pattern = `%.${target.split('.')[1]}`
    return this.rules.has(pattern)
      ? this.rules.get(pattern)
      : null
  }

  expandRule (target, rule) {
    const stem = target.split('.')[0]
    rule.depends
      .map(dep => dep.replace('%', stem))
      .forEach(dep => this.graph.setEdge(dep, target))
    const recipes = rule.recipes.map(act => act.replace('%', stem))
    const timestamp = this.graph.node(target).timestamp
    this.graph.setNode(target, {
      recipes: recipes,
      timestamp: timestamp
    })
  }
}

export default PatternUserRun
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
4: START
4: left.out
    update left.out from left.in
5: right.out
    update right.out from right.in
6: END
\end{lstlisting}


\section{What should we do next?}\label{build-manager-next}


We have added a lot of steps to our original template method,
which makes it a bit of a stretch to claim that the overall operation hasn’t changed.
Knowing what we know now,
we could go back and modify the original \texttt{SkeletonBuilder.build} method
to include those extra steps and provide do-nothing implementations.


The root of the problem is that we didn’t anticipate all the steps that would be involved
when we wrote our template method.
It typically takes a few child classes for this to settle down;
if it never does,
then Template Method\index{Template Method pattern}\index{design pattern!Template Method} is probably the wrong pattern for our situation.
This isn’t a failure in initial design:
we always learn about our problem as we try to capture it in code,
and if we know enough to anticipate 100\% of the issues that are going to come up,
it’s time to put what we’ve learned in a library for future use.

\section{Exercises}\label{build-manager-exercises}

\subsection*{Handle failure}

\begin{enumerate}

\item 

Modify the build manager to accommodate build steps that fail.



\item 

Write Mocha tests to check that this change works correctly.



\end{enumerate}

\subsection*{Dry run}


Add an option to the build manager to show what commands would be executed and why
if a build were actually run.
For example,
the output should display things like, “‘update A’ because A older than B”.

\subsection*{Change directories}


Modify the build manager so that:

\begin{lstlisting}[frame=tblr]
node build.js -C some/sub/directory rules.yml timestamps.yml
\end{lstlisting}


\noindent runs the build in the specified directory rather than the current directory.

\subsection*{Merge files}


Modify the build manager so that it can read multiple configuration files
and execute their combined rules.

\subsection*{Show recipes}


Add a method to build manager to display all unique recipes,
i.e.,
all of the commands it might execute if asked to rebuild everything.

\subsection*{Conditional execution}


Modify the build manager so that:

\begin{enumerate}

\item 

The user can pass \texttt{variable=true} and \texttt{variable=false} arguments on the command-line
    to define variables.



\item 

Rules can contain an \texttt{if: variable} field.



\item 

Those rules are only executed if the variable is defined and true.



\item 

Write Mocha tests to check that this works correctly.



\end{enumerate}

\subsection*{Define filesets}


Modify the build manager so that users can define sets of files:

\begin{lstlisting}[frame=tblr]
fileset:
  name: everything
  contains:
    - X
    - Y
    - Z
\end{lstlisting}


\noindent and then refer to them later:

\begin{lstlisting}[frame=tblr]
- target: P
  depends:
  - @everything
\end{lstlisting}

\subsection*{Globbing}


Modify the build manager so that it can dynamically construct a set of files:

\begin{lstlisting}[frame=tblr]
glob:
  name: allAvailableInputs
  pattern: "./*.in"
\end{lstlisting}


\noindent and then refer to them later:

\begin{lstlisting}[frame=tblr]
- target: P
  depends:
  - @allAvailableInputs
\end{lstlisting}

\subsection*{Use hashes}

\begin{enumerate}

\item 

Write a program called \texttt{build-init.js} that calculates a hash
    for every file mentioned in the build configuration
    and stores the hash along with the file’s name in \texttt{build-hash.json}.



\item 

Modify the build manager to compare the current hashes of files
    with those stored in \texttt{build-hash.json}
    in order to determine what is out of date,
    and to update \texttt{build-hash.json} each time it runs.



\end{enumerate}

\subsection*{Auxiliary functions}

\begin{enumerate}

\item 

Modify the builder manager so that it takes an extra argument \texttt{auxiliaries}
    containing zero or more named functions:

\begin{lstlisting}[frame=tblr]
const builder = new ExtensibleBuilder(configFile, timesFile, {
  slice: (node, graph) => simplify(node, graph, 1)
})
\end{lstlisting}



\item 

Modify the \texttt{run} method to call these functions
    before executing the rules for a node,
    and to only execute the rules if all of them return \texttt{true}.



\item 

Write Mocha tests to check that this works correctly.



\end{enumerate}

\chapter{Layout Engine}\label{layout-engine}


\noindent 
  Terms defined: \glossref{attribute}, \glossref{cache}, \glossref{confirmation bias}, \glossref{design by contract}, \glossref{easy mode}, \glossref{layout engine}, \glossref{Liskov Substitution Principle}, \glossref{query selector}, \glossref{signature}, \glossref{z-buffering}



You might be reading this as an HTML page,
an e-book (which is basically the same thing),
or on the printed page.
In all three cases,
a \glossref{layout engine}\index{layout engine} took some text and some layout instructions
and decided where to put each character and image.
We will build a small layout engine in this chapter
based on \hreffoot{Matt Brubeck’s}{https://limpet.net/mbrubeck/}\index{Brubeck, Matt} \hreffoot{tutorial}{https://limpet.net/mbrubeck/2014/08/08/toy-layout-engine-1.html}
to explore how browsers decide what to put where.


Our inputs will be a very small subset of HTML and an equally small subset of CSS.
We will create our own classes to represent these
instead of using those provided by various \hreffoot{Node}{https://nodejs.org/en/} libraries;
to translate the combination of HTML and CSS into text on the screen,
we will label each node in the DOM tree with the appropriate styles,
walk that tree to figure out where each visible element belongs,
and then draw the result as text on the screen.

\begin{callout}


\subsubsection*{Upside down}


The coordinate systems\index{coordinate system} for screens
puts (0, 0) in the upper left corner instead of the lower left.
X increases to the right as usual,
but Y increases as we go down, rather than up
(\figref{layout-engine-coordinate-system}).
This convention is a holdover from the days of teletype terminals
that printed lines on rolls of paper;
as \hreffoot{Mike Hoye}{http://exple.tive.org/blarg/}\index{Hoye, Mike} has \hreffoot{repeatedly observed}{http://exple.tive.org/blarg/2020/11/26/punching-holes/},
the past is all around us.

\end{callout}

\figpdf{layout-engine-coordinate-system}{./layout-engine/coordinate-system.pdf}{Coordinate system with (0, 0) in the upper left corner.}{0.6}

\section{How can we size rows and columns?}\label{layout-engine-size}


Let’s start on \glossref{easy mode}
without margins, padding, line-wrapping, or other complications.
Everything we can put on the screen is represented as a rectangular cell,
and every cell is either a row, a column, or a block.
A block has a fixed width and height:


\begin{lstlisting}[frame=tblr]
export class Block {
  constructor (width, height) {
    this.width = width
    this.height = height
  }

  getWidth () {
    return this.width
  }

  getHeight () {
    return this.height
  }
}
\end{lstlisting}



A row arranges one or more cells horizontally;
its width is the sum of the widths of its children,
while its height is the height of its tallest child
(\figref{layout-engine-sizing}):


\begin{lstlisting}[frame=tblr]
export class Row {
  constructor (...children) {
    this.children = children
  }

  getWidth () {
    let result = 0
    for (const child of this.children) {
      result += child.getWidth()
    }
    return result
  }

  getHeight () {
    let result = 0
    for (const child of this.children) {
      result = Math.max(result, child.getHeight())
    }
    return result
  }
}
\end{lstlisting}


\figpdf{layout-engine-sizing}{./layout-engine/sizing.pdf}{Calculating sizes of blocks with fixed width and height.}{0.6}


Finally,
a column arranges one or more cells vertically;
its width is the width of its widest child
and its height is the sum of the heights of its children.
(Here and elsewhere we use the abbreviation \texttt{col} when referring to columns.)


\begin{lstlisting}[frame=tblr]
export class Col {
  constructor (...children) {
    this.children = children
  }

  getWidth () {
    let result = 0
    for (const child of this.children) {
      result = Math.max(result, child.getWidth())
    }
    return result
  }

  getHeight () {
    let result = 0
    for (const child of this.children) {
      result += child.getHeight()
    }
    return result
  }
}
\end{lstlisting}



Rows and columns nest inside one another:
a row cannot span two or more columns,
and a column cannot cross the boundary between two rows.
Any time we have a structure with that property
we can represent it as a tree of nested objects.
Given such a tree,
we can calculate the width and height of each cell every time we need to.
This is simple but inefficient:
we could calculate both width and height at the same time
and \glossref{cache}\index{cache!calculated values} those values to avoid recalculation,
but we called this “easy mode” for a reason.


As simple as it is,
this code could still contain errors (and did during development),
so we write some \hreffoot{Mocha}{https://mochajs.org/}\index{Mocha} tests to check that it works as desired
before trying to build anything more complicated:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import {
  Block,
  Row,
  Col
} from '../easy-mode.js'

describe('lays out in easy mode', () => {
  it('lays out a single unit block', async () => {
    const fixture = new Block(1, 1)
    assert.strictEqual(fixture.getWidth(), 1)
    assert.strictEqual(fixture.getHeight(), 1)
  })

  it('lays out a large block', async () => {
    const fixture = new Block(3, 4)
    assert.strictEqual(fixture.getWidth(), 3)
    assert.strictEqual(fixture.getHeight(), 4)
  })

  it('lays out a row of two blocks', async () => {
    const fixture = new Row(
      new Block(1, 1),
      new Block(2, 4)
    )
    assert.strictEqual(fixture.getWidth(), 3)
    assert.strictEqual(fixture.getHeight(), 4)
  })

  it('lays out a column of two blocks', async () => {
    const fixture = new Col(
      new Block(1, 1),
      new Block(2, 4)
    )
    assert.strictEqual(fixture.getWidth(), 2)
    assert.strictEqual(fixture.getHeight(), 5)
  })

  it('lays out a grid of rows of columns', async () => {
    const fixture = new Col(
      new Row(
        new Block(1, 2),
        new Block(3, 4)
      ),
      new Row(
        new Block(5, 6),
        new Col(
          new Block(7, 8),
          new Block(9, 10)
        )
      )
    )
    assert.strictEqual(fixture.getWidth(), 14)
    assert.strictEqual(fixture.getHeight(), 22)
  })
})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test /u/stjs
> mocha */test/test-*.js "-g" "easy mode"



  lays out in easy mode
    ✓ lays out a single unit block
    ✓ lays out a large block
    ✓ lays out a row of two blocks
    ✓ lays out a column of two blocks
    ✓ lays out a grid of rows of columns


  5 passing (7ms)
\end{lstlisting}


\section{How can we position rows and columns?}\label{layout-engine-position}


Now that we know how big each cell is
we can figure out where to put it.
Suppose we start with the upper left corner of the browser:
upper because we lay out the page top-to-bottom
and left because we are doing left-to-right layout.
If the cell is a block, we place it there.
If the cell is a row, on the other hand,
we get its height
and then calculate its lower edge as y1 = y0 + height.
We then place the first child’s lower-left corner at (x0, y1),
the second child’s at (x0 + width0, y1), and so on
(\figref{layout-engine-layout}).
Similarly,
if the cell is a column
we place the first child at (x0, y0),
the next at (x0, y0 + height0),
and so on.

\figpdf{layout-engine-layout}{./layout-engine/layout.pdf}{Laying out rows and columns of fixed-size blocks.}{0.6}


To save ourselves some testing we will derive the classes that know how to do layout
from the classes we wrote before.
Our blocks are:


\begin{lstlisting}[frame=tblr]
export class PlacedBlock extends Block {
  constructor (width, height) {
    super(width, height)
    this.x0 = null
    this.y0 = null
  }

  place (x0, y0) {
    this.x0 = x0
    this.y0 = y0
  }

  report () {
    return [
      'block', this.x0, this.y0,
      this.x0 + this.width,
      this.y0 + this.height
    ]
  }
}
\end{lstlisting}



\noindent while our columns are:


\begin{lstlisting}[frame=tblr]
export class PlacedCol extends Col {
  constructor (...children) {
    super(...children)
    this.x0 = null
    this.y1 = null
  }

  place (x0, y0) {
    this.x0 = x0
    this.y0 = y0
    let yCurrent = this.y0
    this.children.forEach(child => {
      child.place(x0, yCurrent)
      yCurrent += child.getHeight()
    })
  }

  report () {
    return [
      'col', this.x0, this.y0,
      this.x0 + this.getWidth(),
      this.y0 + this.getHeight(),
      ...this.children.map(child => child.report())
    ]
  }
}
\end{lstlisting}



\noindent and our rows are:


\begin{lstlisting}[frame=tblr]
export class PlacedRow extends Row {
  constructor (...children) {
    super(...children)
    this.x0 = null
    this.y0 = null
  }

  place (x0, y0) {
    this.x0 = x0
    this.y0 = y0
    const y1 = this.y0 + this.getHeight()
    let xCurrent = x0
    this.children.forEach(child => {
      const childY = y1 - child.getHeight()
      child.place(xCurrent, childY)
      xCurrent += child.getWidth()
    })
  }

  report () {
    return [
      'row', this.x0, this.y0,
      this.x0 + this.getWidth(),
      this.y0 + this.getHeight(),
      ...this.children.map(child => child.report())
    ]
  }
}
\end{lstlisting}



Once again,
we write and run some tests to check that everything is doing what it’s supposed to:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import {
  PlacedBlock as Block,
  PlacedCol as Col,
  PlacedRow as Row
} from '../placed.js'

describe('places blocks', () => {
  it('places a single unit block', async () => {
    const fixture = new Block(1, 1)
    fixture.place(0, 0)
    assert.deepStrictEqual(
      fixture.report(),
      ['block', 0, 0, 1, 1]
    )
  })

  it('places a large block', async () => {
    const fixture = new Block(3, 4)
    fixture.place(0, 0)
    assert.deepStrictEqual(
      fixture.report(),
      ['block', 0, 0, 3, 4]
    )
  })

  it('places a row of two blocks', async () => {
    const fixture = new Row(
      new Block(1, 1),
      new Block(2, 4)
    )
    fixture.place(0, 0)
    assert.deepStrictEqual(
      fixture.report(),
      ['row', 0, 0, 3, 4,
        ['block', 0, 3, 1, 4],
        ['block', 1, 0, 3, 4]
      ]
    )
  })

})
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test /u/stjs
> mocha */test/test-*.js "-g" "places blocks"

  places blocks
    ✓ places a single unit block
    ✓ places a large block
    ✓ places a row of two blocks
    ✓ places a column of two blocks
    ✓ places a grid of rows of columns


  5 passing (8ms)
\end{lstlisting}


\section{How can we render elements?}\label{layout-engine-render}


We drew the blocks on a piece of graph paper
in order to figure out the expected answers for the tests shown above.
We can do something similar in software by creating a “screen” of space characters
and then having each block draw itself in the right place.
If we do this starting at the root of the tree,
child blocks will overwrite the markings made by their parents,
which will automatically produce the right appearance
(\figref{layout-engine-draw-over}).
(A more sophisticated version of this called \glossref{z-buffering}
keeps track of the visual depth of each pixel
in order to draw things in three dimensions.)

\figpdf{layout-engine-draw-over}{./layout-engine/draw-over.pdf}{Render blocks by drawing child nodes on top of parent nodes.}{0.6}


Our pretended screen is just an array of arrays of characters:


\begin{lstlisting}[frame=tblr]
const makeScreen = (width, height) => {
  const screen = []
  for (let i = 0; i < height; i += 1) {
    screen.push(new Array(width).fill(' '))
  }
  return screen
}
\end{lstlisting}



We will use successive lower-case characters to show each block,
i.e.,
the root block will draw itself using the letter a,
while its children will use b, c, and so on.


\begin{lstlisting}[frame=tblr]
const draw = (screen, node, fill = null) => {
  fill = nextFill(fill)
  node.render(screen, fill)
  if ('children' in node) {
    node.children.forEach(child => {
      fill = draw(screen, child, fill)
    })
  }
  return fill
}

const nextFill = (fill) => {
  return (fill === null)
    ? 'a'
    : String.fromCharCode(fill.charCodeAt() + 1)
}
\end{lstlisting}



To teach each kind of cell how to render itself,
we have to derive a new class from each of the ones we have
and give the new class a \texttt{render} method with the same \glossref{signature}\index{signature!of function}\index{function signature}:


\begin{lstlisting}[frame=tblr]
import {
  PlacedBlock,
  PlacedCol,
  PlacedRow
} from './placed.js'

// [keep]
export class RenderedBlock extends PlacedBlock {
  render (screen, fill) {
    drawBlock(screen, this, fill)
  }
}

export class RenderedCol extends PlacedCol {
  render (screen, fill) {
    drawBlock(screen, this, fill)
  }
}

export class RenderedRow extends PlacedRow {
  render (screen, fill) {
    drawBlock(screen, this, fill)
  }
}

const drawBlock = (screen, node, fill) => {
  for (let ix = 0; ix < node.getWidth(); ix += 1) {
    for (let iy = 0; iy < node.getHeight(); iy += 1) {
      screen[node.y0 + iy][node.x0 + ix] = fill
    }
  }
}
// [/keep]
\end{lstlisting}



\noindent These \texttt{render} methods do exactly the same thing,
so we have each one call a shared function that does the actual work.
If we were building a real layout engine,
a cleaner solution would be to go back and create a class called \texttt{Cell} with this \texttt{render} method,
then derive our \texttt{Block}, \texttt{Row}, and \texttt{Col} classes from that.
In general,
if two or more classes need to be able to do something,
we should add a method to do that to their lowest common ancestor.


Our simpler tests are a little easier to read once we have rendering in place,
though we still had to draw things on paper to figure out our complex ones:


\begin{lstlisting}[frame=tblr]
  it('renders a grid of rows of columns', async () => {
    const fixture = new Col(
      new Row(
        new Block(1, 2),
        new Block(3, 4)
      ),
      new Row(
        new Block(1, 2),
        new Col(
          new Block(3, 4),
          new Block(2, 3)
        )
      )
    )
    fixture.place(0, 0)
    assert.deepStrictEqual(
      render(fixture),
      [
        'bddd',
        'bddd',
        'cddd',
        'cddd',
        'ehhh',
        'ehhh',
        'ehhh',
        'ehhh',
        'eiig',
        'fiig',
        'fiig'
      ].join('\n')
    )
  })
\end{lstlisting}



\noindent The fact that we find our own tests difficult to understand
is a sign that we should do more testing.
It would be very easy for us to get a wrong result
and convince ourselves that it was actually correct;
\glossref{confirmation bias}\index{confirmation bias} of this kind
is very common in software development.

\section{How can we wrap elements to fit?}\label{layout-engine-fit}


One of the biggest differences between a browser and a printed page
is that the text in the browser wraps itself automatically as the window is resized.
(The other, these days, is that the printed page doesn’t spy on us,
though someone is undoubtedly working on that.)


To add wrapping to our layout engine,
suppose we fix the width of a row.
If the total width of the children is greater than the row’s width,
the layout engine needs to wrap the children around.
This assumes that columns can be made as big as they need to be,
i.e.,
that we can grow vertically to make up for limited space horizontally.
It also assumes that all of the row’s children are no wider than the width of the row;
we will look at what happens when they’re not in the exercises.


Our layout engine manages wrapping by transforming the tree.
The height and width of blocks are fixed,
so they become themselves.
Columns become themselves as well,
but since they have children that might need to wrap,
the class representing columns needs a new method:


\begin{lstlisting}[frame=tblr]
export class WrappedBlock extends PlacedBlock {
  wrap () {
    return this
  }
}

export class WrappedCol extends PlacedCol {
  wrap () {
    const children = this.children.map(child => child.wrap())
    return new PlacedCol(...children)
  }
}
\end{lstlisting}



Rows do all the hard work.
Each original row is replaced with a new row that contains a single column with one or more rows,
each of which is one “line” of wrapped cells
(\figref{layout-engine-wrap}).
This replacement is unnecessary when everything will fit on a single row,
but it’s easiest to write the code that does it every time;
we will look at making this more efficient in the exercises.

\figpdf{layout-engine-wrap}{./layout-engine/wrap.pdf}{Wrapping rows by introducing a new row and column.}{0.6}


Our new wrappable row’s constructor takes a fixed width followed by the children
and returns that fixed width when asked for its size:


\begin{lstlisting}[frame=tblr]
export class WrappedRow extends PlacedRow {
  constructor (width, ...children) {
    super(...children)
    assert(width >= 0,
      'Need non-negative width')
    this.width = width
  }

  getWidth () {
    return this.width
  }

}
\end{lstlisting}



\noindent Wrapping puts the row’s children into buckets,
then converts the buckets to a row of a column of rows:


\begin{lstlisting}[frame=tblr]
  wrap () {
    const children = this.children.map(child => child.wrap())
    const rows = []
    let currentRow = []
    let currentX = 0

    children.forEach(child => {
      const childWidth = child.getWidth()
      if ((currentX + childWidth) <= this.width) {
        currentRow.push(child)
        currentX += childWidth
      } else {
        rows.push(currentRow)
        currentRow = [child]
        currentX = childWidth
      }
    })
    rows.push(currentRow)

    const newRows = rows.map(row => new PlacedRow(...row))
    const newCol = new PlacedCol(...newRows)
    return new PlacedRow(newCol)
  }
\end{lstlisting}



Once again we bring forward all the previous tests
and write some new ones to test the functionality we’ve added:


\begin{lstlisting}[frame=tblr]
  it('wrap a row of two blocks that do not fit on one row', async () => {
    const fixture = new Row(
      3,
      new Block(2, 1),
      new Block(2, 1)
    )
    const wrapped = fixture.wrap()
    wrapped.place(0, 0)
    assert.deepStrictEqual(
      wrapped.report(),
      ['row', 0, 0, 2, 2,
        ['col', 0, 0, 2, 2,
          ['row', 0, 0, 2, 1,
            ['block', 0, 0, 2, 1]
          ],
          ['row', 0, 1, 2, 2,
            ['block', 0, 1, 2, 2]
          ]
        ]
      ]
    )
  })
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test /u/stjs
> mocha */test/test-*.js "-g" "wraps blocks"



  wraps blocks
    ✓ wraps a single unit block
    ✓ wraps a large block
    ✓ wrap a row of two blocks that fit on one row
    ✓ wraps a column of two blocks
    ✓ wraps a grid of rows of columns that all fit on their row
    ✓ wrap a row of two blocks that do not fit on one row
    ✓ wrap multiple blocks that do not fit on one row


  7 passing (10ms)
\end{lstlisting}


\begin{callout}


\subsubsection*{The Liskov Substitution Principle}


We are able to re-use tests like this because of
the \glossref{Liskov Substitution Principle}\index{Liskov Substitution Principle}\index{software design!Liskov Substitution Principle},
which states that
it should be possible to replace objects in a program
with objects of derived classes
without breaking anything.
In order to satisfy this principle,
new code must handle the same set of inputs as the old code,
though it may be able to process more inputs as well.
Conversely,
its output must be a subset of what the old code produced
so that whatever is downstream from it won’t be surprised.
Thinking in these terms leads to a methodology called
\glossref{design by contract}\index{design by contract}\index{software design!design by contract}.

\end{callout}

\section{What subset of CSS will we support?}\label{layout-engine-css}


It’s finally time to style pages that contain text.
Our final subset of HTML has rows, columns, and text blocks as before.
Each text block has one or more lines of text;
the number of lines determines the block’s height
and the length of the longest line determines its width.


Rows and columns can have \glossref{attributes} just as they can in real HTML,
and each attribute must have a single value in quotes.
Rows no longer take a fixed width:
instead,
we will specify that with our little subset of CSS\index{CSS}.
Together,
these three classes are just over 40 lines of code:


\begin{lstlisting}[frame=tblr]
export class DomBlock extends WrappedBlock {
  constructor (lines) {
    super(
      Math.max(...lines.split('\n').map(line => line.length)),
      lines.length
    )
    this.lines = lines
    this.tag = 'text'
    this.rules = null
  }

  findRules (css) {
    this.rules = css.findRules(this)
  }
}

export class DomCol extends WrappedCol {
  constructor (attributes, ...children) {
    super(...children)
    this.attributes = attributes
    this.tag = 'col'
    this.rules = null
  }

  findRules (css) {
    this.rules = css.findRules(this)
    this.children.forEach(child => child.findRules(css))
  }
}

export class DomRow extends WrappedRow {
  constructor (attributes, ...children) {
    super(0, ...children)
    this.attributes = attributes
    this.tag = 'row'
    this.rules = null
  }

  findRules (css) {
    this.rules = css.findRules(this)
    this.children.forEach(child => child.findRules(css))
  }
}
\end{lstlisting}



We will use regular expressions to parse HTML
(though as we explained in \chapref{regex-parser},
\hreffoot{this is a sin}{https://stackoverflow.com/questions/1732348/regex-match-open-tags-except-xhtml-self-contained-tags/1732454\#1732454}).
The main body of our parser is:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import {
  DomBlock,
  DomCol,
  DomRow
} from './micro-dom.js'

const TEXT_AND_TAG = /^([^<]*)(<[^]+?>)(.*)$/ms
const TAG_AND_ATTR = /<(\w+)([^>]*)>/
const KEY_AND_VALUE = /\s*(\w+)="([^"]*)"\s*/g

const parseHTML = (text) => {
  const chunks = chunkify(text.trim())
  assert(isElement(chunks[0]),
    'Must have enclosing outer node')
  const [node, remainder] = makeNode(chunks)
  assert(remainder.length === 0,
    'Cannot have dangling content')
  return node
}

const chunkify = (text) => {
  const raw = []
  while (text) {
    const matches = text.match(TEXT_AND_TAG)
    if (!matches) {
      break
    }
    raw.push(matches[1])
    raw.push(matches[2])
    text = matches[3]
  }
  if (text) {
    raw.push(text)
  }
  const nonEmpty = raw.filter(chunk => (chunk.length > 0))
  return nonEmpty
}

const isElement = (chunk) => {
  return chunk && (chunk[0] === '<')
}


export default parseHTML
\end{lstlisting}



\noindent while the two functions that do most of the work are:


\begin{lstlisting}[frame=tblr]
const makeNode = (chunks) => {
  assert(chunks.length > 0,
    'Cannot make nodes without chunks')

  if (!isElement(chunks[0])) {
    return [new DomBlock(chunks[0]), chunks.slice(1)]
  }

  const node = makeOpening(chunks[0])
  const closing = `</${node.tag}>`

  let remainder = chunks.slice(1)
  let child = null
  while (remainder && (remainder[0] !== closing)) {
    [child, remainder] = makeNode(remainder)
    node.children.push(child)
  }

  assert(remainder && (remainder[0] === closing),
    `Node with tag ${node.tag} not closed`)
  return [node, remainder.slice(1)]
}
\end{lstlisting}



\noindent and:


\begin{lstlisting}[frame=tblr]
const makeOpening = (chunk) => {
  const outer = chunk.match(TAG_AND_ATTR)
  const tag = outer[1]
  const attributes = [...outer[2].trim().matchAll(KEY_AND_VALUE)]
    .reduce((obj, [all, key, value]) => {
      obj[key] = value
      return obj
    }, {})
  let Cls = null
  if (tag === 'col') {
    Cls = DomCol
  } else if (tag === 'row') {
    Cls = DomRow
  }
  assert(Cls !== null,
    `Unrecognized tag name ${tag}`)
  return new Cls(attributes)
}
\end{lstlisting}



The next step is to define a generic class for CSS rules
with a subclass for each type of rule.
From highest precedence to lowest,
the three types of rules we support identify specific nodes via their ID,
classes of nodes via their \texttt{class} attribute,
and types of nodes via their element name.
We keep track of which rules take precedence over which through the simple expedient of numbering the classes:


\begin{lstlisting}[frame=tblr]
export class CssRule {
  constructor (order, selector, styles) {
    this.order = order
    this.selector = selector
    this.styles = styles
  }
}
\end{lstlisting}



An ID rule’s \glossref{query selector}\index{query selector} is written as \texttt{\#name}
and matches HTML like \texttt{<tag id="name">...</tag>} (where \texttt{tag} is \texttt{row} or \texttt{col}):


\begin{lstlisting}[frame=tblr]
export class IdRule extends CssRule {
  constructor (selector, styles) {
    assert(selector.startsWith('#') && (selector.length > 1),
      `ID rule ${selector} must start with # and have a selector`)
    super(IdRule.ORDER, selector.slice(1), styles)
  }

  match (node) {
    return ('attributes' in node) &&
      ('id' in node.attributes) &&
      (node.attributes.id === this.selector)
  }
}
IdRule.ORDER = 0
\end{lstlisting}



A class rule’s query selector is written as \texttt{.kind} and matches HTML like \texttt{<tag class="kind">...</tag>}.
Unlike real CSS,
we only allow one class per node:


\begin{lstlisting}[frame=tblr]
export class ClassRule extends CssRule {
  constructor (selector, styles) {
    assert(selector.startsWith('.') && (selector.length > 1),
      `Class rule ${selector} must start with . and have a selector`)
    super(ClassRule.ORDER, selector.slice(1), styles)
  }

  match (node) {
    return ('attributes' in node) &&
      ('class' in node.attributes) &&
      (node.attributes.class === this.selector)
  }
}
ClassRule.ORDER = 1
\end{lstlisting}



Finally,
tag rules just have the name of the type of node they apply to without any punctuation:


\begin{lstlisting}[frame=tblr]
export class TagRule extends CssRule {
  constructor (selector, styles) {
    super(TagRule.ORDER, selector, styles)
  }

  match (node) {
    return this.selector === node.tag
  }
}
TagRule.ORDER = 2
\end{lstlisting}



We could build yet another parser to read a subset of CSS and convert it to objects,
but this chapter is long enough,
so we will write our rules as JSON:

\begin{lstlisting}[frame=tblr]
{
  'row': { width: 20 },
  '.kind': { width: 5 },
  '#name': { height: 10 }
}
\end{lstlisting}


\noindent and build a class that converts this representation to a set of objects:


\begin{lstlisting}[frame=tblr]
export class CssRuleSet {
  constructor (json, mergeDefaults = true) {
    this.rules = this.jsonToRules(json)
  }

  jsonToRules (json) {
    return Object.keys(json).map(selector => {
      assert((typeof selector === 'string') && (selector.length > 0),
        'Require non-empty string as selector')
      if (selector.startsWith('#')) {
        return new IdRule(selector, json[selector])
      }
      if (selector.startsWith('.')) {
        return new ClassRule(selector, json[selector])
      }
      return new TagRule(selector, json[selector])
    })
  }

  findRules (node) {
    const matches = this.rules.filter(rule => rule.match(node))
    const sorted = matches.sort((left, right) => left.order - right.order)
    return sorted
  }
}
\end{lstlisting}



Our CSS ruleset class also has a method for finding the rules for a given DOM node.
This method relies on the precedence values we defined for our classes
in order to sort them
so that we can find the most specific.


Here’s our final set of tests:


\begin{lstlisting}[frame=tblr]
  it('styles a tree of nodes with multiple rules', async () => {
    const html = [
      '<col id="name">',
      '<row class="kind">first\nsecond</row>',
      '<row>third\nfourth</row>',
      '</col>'
    ]
    const dom = parseHTML(html.join(''))
    const rules = new CssRuleSet({
      '.kind': { height: 3 },
      '#name': { height: 5 },
      row: { width: 10 }
    })
    dom.findRules(rules)
    assert.deepStrictEqual(dom.rules, [
      new IdRule('#name', { height: 5 })
    ])
    assert.deepStrictEqual(dom.children[0].rules, [
      new ClassRule('.kind', { height: 3 }),
      new TagRule('row', { width: 10 })
    ])
    assert.deepStrictEqual(dom.children[1].rules, [
      new TagRule('row', { width: 10 })
    ])
  })
\end{lstlisting}



If we were going on,
we would override the cells’ \texttt{getWidth} and \texttt{getHeight} methods to pay attention to styles.
We would also decide what to do with cells that don’t have any styles defined:
use a default,
flag it as an error,
or make a choice based on the contents of the child nodes.
We will explore these possibilities in the exercises.

\begin{callout}


\subsubsection*{Where it all started}


This chapter’s topic was one of the seeds from which this entire book grew
(the other being debuggers discussed in \chapref{debugger}).
After struggling with CSS\index{CSS!struggles with} for several years,
I began wondering whether it really had to be so complicated.
That question led to others,
which eventually led to all of this.
The moral is,
be careful what you ask.

\end{callout}

\section{Exercises}\label{layout-engine-exercises}

\subsection*{Refactoring the node classes}


Refactor the classes used to represent blocks, rows, and columns so that:

\begin{enumerate}

\item 

They all derive from a common parent.



\item 

All common behavior is defined in that parent (if only with placeholder methods).



\end{enumerate}

\subsection*{Handling rule conflicts}


Modify the rule lookup mechanism so that if two conflicting rules are defined,
the one that is defined second takes precedence.
For example,
if there are two definitions for \texttt{row.bold},
whichever comes last in the JSON representation of the CSS wins.

\subsection*{Handling arbitrary tags}


Modify the existing code to handle arbitrary HTML elements.

\begin{enumerate}

\item 

The parser should recognize \texttt{<anyTag>...</anyTag>}.



\item 

Instead of separate classes for rows and columns,
    there should be one class \texttt{Node} whose \texttt{tag} attribute identifies its type.



\end{enumerate}

\subsection*{Recycling nodes}


Modify the wrapping code so that new rows and columns are only created if needed.
For example,
if a row of width 10 contains a text node with the string “fits”,
a new row and column are \emph{not} inserted.

\subsection*{Rendering a clear background}


Modify the rendering code so that only the text in block nodes is shown,
i.e.,
so that the empty space in rows and columns is rendered as spaces.

\subsection*{Clipping text}

\begin{enumerate}

\item 

Modify the wrapping and rendering so that
    if a block of text is too wide for the available space
    the extra characters are clipped.
    For example,
    if a column of width 5 contains a line “unfittable”,
    only “unfit” appears.



\item 

Extend your solution to break lines on spaces as needed
    in order to avoid clipping.



\end{enumerate}

\subsection*{Bidirectional rendering}


Modify the existing software to do either left-to-right or right-to-left rendering
upon request.

\subsection*{Equal sizing}


Modify the existing code to support elastic columns,
i.e.,
so that all of the columns in a row are automatically sized to have the same width.
If the number of columns does not divide evenly into the width of the row,
allocate the extra space as equally as possible from left to right.

\subsection*{Padding elements}


Modify the existing code so that:

\begin{enumerate}

\item 

Authors can define a \texttt{padding} attribute for row and column elements.



\item 

When the node is rendered, that many blank spaces are added on all four sides of the contents.



\end{enumerate}


\noindent For example, the HTML \texttt{<row>text</row>} would render as:

\begin{lstlisting}[frame=tblr]
+------+
|      |
| text |
|      |
+------+
\end{lstlisting}


\noindent where the lines show the outer border of the rendering.

\subsection*{Drawing borders}

\begin{enumerate}

\item 

Modify the existing code so that elements may specify \texttt{border: true} or \texttt{border: false}
    (with the latter being the default).
    If an element’s \texttt{border} property is \texttt{true},
    it is drawn with a dashed border.
    For example,
    if the \texttt{border} property of \texttt{row} is \texttt{true},
    then \texttt{<row>text</row>} is rendered as:

\begin{lstlisting}[frame=tblr]
+----+
|text|
+----+
\end{lstlisting}



\item 

Extend your solution so that if two adjacent cells both have borders,
    only a single border is drawn.
    For example,
    if the \texttt{border} property of \texttt{col} is \texttt{true},
    then:

\begin{lstlisting}[frame=tblr]
<row><col>left</col><col>right</col></row>
\end{lstlisting}


\noindent is rendered as:

\begin{lstlisting}[frame=tblr]
+----+-----+
|left|right|
+----+-----+
\end{lstlisting}



\end{enumerate}

\chapter{File Interpolator}\label{file-interpolator}


\noindent 
  Terms defined: \glossref{header file}, \glossref{literate programming}, \glossref{loader}, \glossref{sandbox}, \glossref{search path}, \glossref{shell variable}



Many of the examples in these lessons are too long
to show comfortably in one block of code on a printed page,
so we needed a way to break them up.
As an experiment,
we wrote a custom \glossref{module loader}\index{module loader}
that reads a source file containing specially-formatted comments
and then reads and inserts the files specified in those comments
before running the code
(\figref{file-interpolator-conceptual}).
Modern programming languages don’t work this way,
but C\index{C} and C++\index{C++} do this
with \glossref{header files}\index{header file!in C and C++},
and static site generators\index{static site generator!header file}\index{header file!static site generator}
(\chapref{page-templates}) do this to share fragments of HTML.

\figpdf{file-interpolator-conceptual}{./file-interpolator/conceptual.pdf}{Including fragments of code to create runnable programs.}{0.6}


The special comments in our source files contain
the text to put in the displayed version
and file to include when loading:


\begin{lstlisting}[frame=tblr]
class Something {
  /*+ constructor + constructor.js +*/

  /*+ a long method + long_method.js +*/

  /*+ another method + another_method.js +*/
}
\end{lstlisting}



We got this to work,
but decided to use a different approach in this book.
The stumbling block was that the style-checking tool \hreffoot{ESLint}{https://eslint.org/}\index{ESLint}
didn’t know what to make of our inclusions,
so we would either have to modify it or build a style checker of our own.
(We will actually do that in \chapref{style-checker},
but we won’t go nearly as far as ESLint.)


Despite being a dead end,
the inclusion tool is a good way to show
how JavaScript turns source code into something it can execute.
We need to be able to do this in the next couple of chapters,
so we might as well tackle it now.

\section{How can we evaluate JavaScript dynamically?}\label{file-interpolator-dynamic}


We want to display files as they are on the web and in print,
but interpolate the files referenced in special comments
when we load things with \texttt{import}.
To do this,
we need to understand the lifecycle of a JavaScript program.
When we ask for a file,
\hreffoot{Node}{https://nodejs.org/en/} reads the text,
translates it into runnable instructions,
and runs those instructions.
We can do the second and third steps whenever we want using a function called \texttt{eval},
which takes a string as input and executes it as if it were part of the program
(\figref{file-interpolator-eval}).

\figpdf{file-interpolator-eval}{./file-interpolator/eval.pdf}{\texttt{eval} vs. normal translation and execution.}{0.6}

\begin{callout}


\subsubsection*{This is not a good idea}


\texttt{eval}\index{eval!insecurity of} is a security risk:
arbitrary code can do arbitrary things,
so if we take a string typed in by a user and execute it without any checks
it could email our bookmark list to villains all over the world,
erase our hard drive,
or do anything else that code can do (which is pretty much anything).
Browsers do their best to run code in a \glossref{sandbox}\index{sandbox (for safe execution)} for safety,
but Node doesn’t,
so it’s up to us to be (very) careful.

\end{callout}


To see \texttt{eval} in action,
let’s evaluate an expression:


\begin{lstlisting}[frame=tblr]
console.log(eval('2 + 2'))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
4
\end{lstlisting}



\noindent Notice that the input to \texttt{eval} is \emph{not} \texttt{2 + 2},
but rather a string containing the digit 2,
a space,
a plus sign,
another space,
and another 2.
When we call \texttt{eval},
it translates this string
using exactly the same parser that Node uses for our program
and immediately runs the result.


We can make the example a little more interesting
by constructing the string dynamically:


\begin{lstlisting}[frame=tblr]
const x = 1
const y = 3
const z = 5
for (const name of ['x', 'y', 'z', 'oops']) {
  const expr = `${name} + 1`
  console.log(name, '+ 1 =', eval(expr))
}
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
x + 1 = 2
y + 1 = 4
z + 1 = 6
undefined:1
oops + 1
^

ReferenceError: oops is not defined
    at eval (eval at <anonymous> \
    (/u/stjs/file-interpolator/eval-loop.js:7:30), <anonymous>:1:1)
    at /u/stjs/file-interpolator/eval-loop.js:7:30
    at ModuleJob.run (internal/modules/esm/module_job.js:152:23)
    at async Loader.import (internal/modules/esm/loader.js:166:24)
    at async Object.loadESM (internal/process/esm_loader.js:68:5)
\end{lstlisting}



\noindent The first time the loop runs the string is \texttt{{\textquotesingle}x + 1{\textquotesingle}};
since there’s a variable called \texttt{x} in scope,
\texttt{eval} does the addition and we print the result.
The same thing happens for the variables \texttt{y} and \texttt{z},
but we get an error when we try to evaluate the string \texttt{{\textquotesingle}oops + 1{\textquotesingle}}
because there is no variable in scope called \texttt{oops}.


\texttt{eval} can use whatever variables are in scope when it’s called,
but what happens to any variables it defines?
This example creates a variable called \texttt{x} and runs \texttt{console.log} to display it,
but as the output shows,
\texttt{x} is local to the \texttt{eval} call
just as variables created inside a function
only exist during a call to that function:


\begin{lstlisting}[frame=tblr]
const code = `
  const x = 'hello'
  console.log('x in eval is', x)
`

eval(code)
console.log('typeof x after eval', typeof x)
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
x in eval is hello
typeof x after eval undefined
\end{lstlisting}



However,
\texttt{eval} can modify variables defined outside the text being evaluated
in the same way that a function can modify global variables:


\begin{lstlisting}[frame=tblr]
let x = 'original'
eval('x = "modified"')
console.log('x after eval is', x)
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
x after eval is modified
\end{lstlisting}



\noindent This means that
if the text we give to \texttt{eval} modifies a structure that is defined outside the text,
that change outlives the call to \texttt{eval}:


\begin{lstlisting}[frame=tblr]
const seen = {}

for (const name of ['x', 'y', 'z']) {
  const expr = `seen["${name}"] = "${name.toUpperCase()}"`
  eval(expr)
}

console.log(seen)
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{ x: 'X', y: 'Y', z: 'Z' }
\end{lstlisting}



The examples so far have all evaluated strings embedded in the program itself,
but \texttt{eval} doesn’t care where its input comes from.
Let’s move the code that does the modifying into \texttt{to-be-loaded.js}:


\begin{lstlisting}[frame=tblr]
// Modify a global structure defined by whoever loads us.
Seen.from_loaded_file = 'from loaded file'
\end{lstlisting}



\noindent This doesn’t work on its own because \texttt{Seen} isn’t defined:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
/u/stjs/file-interpolator/to-be-loaded.js:3
Seen.from_loaded_file = 'from loaded file'
^

ReferenceError: Seen is not defined
    at /u/stjs/file-interpolator/to-be-loaded.js:3:1
    at ModuleJob.run (internal/modules/esm/module_job.js:152:23)
    at async Loader.import (internal/modules/esm/loader.js:166:24)
    at async Object.loadESM (internal/process/esm_loader.js:68:5)
\end{lstlisting}



\noindent But if we read the file and \texttt{eval} the text \emph{after} defining \texttt{Seen},
it does what we want:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'

const Seen = {}

const filename = process.argv[2]
const content = fs.readFileSync(filename, 'utf-8')
console.log('before eval, Seen is', Seen)
eval(content)
console.log('after eval, Seen is', Seen)
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node does-the-loading.js to-be-loaded.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
before eval, Seen is {}
after eval, Seen is { from_loaded_file: 'from loaded file' }
\end{lstlisting}


\section{How can we manage files?}\label{file-interpolator-manage}


The source files in this book are small enough
that we don’t have to worry about reading them repeatedly,
but we would like to avoid re-reading things unnecessarily
in large systems or when there might be network delays.
The usual approach is to create a cache\index{cache!of loaded files}
using the Singleton pattern\index{Singleton pattern}\index{design pattern!Singleton}
that we first met in \chapref{unit-test}.
Whenever we want to read a file,
we check to see if it’s already in the cache
(\figref{file-interpolator-cache}).
If it is,
we use that copy;
if not,
we read it and add it to the cache
using the file path as a lookup key.


We can write a simple cache in just a few lines of code:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'

class Cache {
  constructor () {
    this.loaded = new Map()
  }

  need (name) {
    if (this.loaded.has(name)) {
      console.log(`returning cached value for ${name}`)
      return this.loaded.get(name)
    }
    console.log(`loading ${name}`)
    const content = fs.readFileSync(name, 'utf-8')
    const result = eval(content)
    this.loaded.set(name, result)
    return result
  }
}

const cache = new Cache()

export default (name) => {
  return cache.need(name)
}
\end{lstlisting}


\figpdfhere{file-interpolator-cache}{./file-interpolator/cache.pdf}{Using the Singleton pattern to implement a cache of loaded files.}{0.6}


Since we are using \texttt{eval}, though,
we can’t rely on \texttt{export} to make things available to the rest of the program.
Instead,
we rely on the fact that the result of an \texttt{eval} call is the value of
the last expression evaluated.
Since a variable name on its own evaluates to the variable’s value,
we can create a function and then use its name
to “export” it from the evaluated file:


\begin{lstlisting}[frame=tblr]
// Define.
const report = (message) => {
  console.log(`report in import-01.js with message "${message}"`)
}

// Export.
report
\end{lstlisting}



To test our program,
we load the implementation of the cache using \texttt{import},
then use it to load and evaluate another file.
This example expects that “other file” to define a function,
which we call in order to show that everything is working:


\begin{lstlisting}[frame=tblr]
import need from './need-simple.js'

const imported = need('./import-simple.js')
imported('called from test-simple.js')
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node test-simple.js
\end{lstlisting}


\section{How can we find files?}\label{file-interpolator-find}


Each of the files included in our examples is in the same directory as the file including it,
but in C/C++ or a page templating system
we might include a particular file in several different places.
We don’t want to have to put all of our files in a single directory,
so we need a way specify where to look for files that are being included.


One option is to use relative paths,
but another option is to give our program
a list of directories to look in.
This is called a \glossref{search path}\index{search path},
and many programs use them,
including Node itself.
By convention,
a search path is written as a colon-separated list of directories on Unix
or using semi-colons on Windows.
If the path to an included file starts with \texttt{./},
we look for it locally;
if not,
we go through the directories in the search path in order
until we find a file with a matching name
(\figref{file-interpolator-search-path}).

\figpdf{file-interpolator-search-path}{./file-interpolator/search-path.pdf}{Using a colon-separated list of directories as a search path.}{0.6}

\begin{callout}


\subsubsection*{That’s just how it is}


The rules about search paths in the paragraph above are a convention:
somebody did it this way years ago
and (almost) everyone has imitated it since.
We could implement search paths some other way,
but as with configuration file formats,
variable naming conventions,
and many other things,
the last thing the world needs is more innovation.

\end{callout}


Since the cache is responsible for finding files,
it should also handle the search path.
The outline of the class stays the same:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'
import path from 'path'

class Cache {
  constructor () {
    this.loaded = new Map()
    this.constructSearchPath()
  }

  need (fileSpec) {
    if (this.loaded.has(fileSpec)) {
      console.log(`returning cached value for ${fileSpec}`)
      return this.loaded.get(fileSpec)
    }
    console.log(`loading value for ${fileSpec}`)
    const filePath = this.find(fileSpec)
    const content = fs.readFileSync(filePath, 'utf-8')
    const result = eval(content)
    this.loaded.set(fileSpec, result)
    return result
  }

}

const cache = new Cache()

export default (fileSpec) => {
  return cache.need(fileSpec)
}
\end{lstlisting}



To get the search path,
we look for the \glossref{shell variable}\index{shell variable (for storing search path)}\index{search path!shell variable} \texttt{NEED\_PATH}.
(Writing shell variables’ names in upper case is another convention.)
If \texttt{NEED\_PATH} exists,
we split it on colons to create a list of directories:


\begin{lstlisting}[frame=tblr]
  constructSearchPath () {
    this.searchPath = []
    if ('NEED_PATH' in process.env) {
      this.searchPath = process.env.NEED_PATH
        .split(':')
        .filter(x => x.length > 0)
    }
  }
\end{lstlisting}



When we need to find a file we first check to see if the path is local.
If it’s not,
we try the directories in the search path in order:


\begin{lstlisting}[frame=tblr]
  constructSearchPath () {
    this.searchPath = []
    if ('NEED_PATH' in process.env) {
      this.searchPath = process.env.NEED_PATH
        .split(':')
        .filter(x => x.length > 0)
    }
  }
\end{lstlisting}



To test this,
we put the file to import in a subdirectory called \texttt{modules}:


\begin{lstlisting}[frame=tblr]
// Define.
const report = (message) => {
  console.log(`in LEFT with message "${message}"`)
}

// Export.
report
\end{lstlisting}



\noindent and then put the file doing the importing in the current directory:


\begin{lstlisting}[frame=tblr]
import need from './need-path.js'
const imported = need('imported-left.js')
imported('called from test-import-left.js')
\end{lstlisting}



We now need to set the variable \texttt{NEED\_PATH}.
There are many ways to do this in shell;
if we only need the variable to exist for a single command,
the simplest is to write it as:

\begin{lstlisting}[frame=tblr]
NAME=value command
\end{lstlisting}


\noindent right before the command (on the same line).
Here’s the shell command that runs our test case
using \texttt{\$PWD} to get the current working directory:


\begin{lstlisting}[frame=shadowbox]
NEED_PATH=$PWD/modules/ node test-import-left.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
loading value for imported-left.js
trying /u/stjs/file-interpolator/modules/imported-left.js for \
imported-left.js
in LEFT with message "called from test-import-left.js"
\end{lstlisting}



Now let’s create a second importable file in the \texttt{modules} directory:


\begin{lstlisting}[frame=tblr]
// Define.
const report = (message) => {
  console.log(`in RIGHT with message "${message}"`)
}

// Export.
report
\end{lstlisting}



\noindent and load that twice to check that caching works:


\begin{lstlisting}[frame=tblr]
import need from './need-path.js'

const imported = need('imported-right.js')
imported('called from test-import-right.js')

const alsoImported = need('imported-right.js')
alsoImported('called from test-import-right.js')
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
loading value for imported-right.js
trying /u/stjs/file-interpolator/modules/imported-right.js for \
imported-right.js
in RIGHT with message "called from test-import-right.js"
returning cached value for imported-right.js
in RIGHT with message "called from test-import-right.js"
\end{lstlisting}


\section{How can we interpolate pieces of code?}\label{file-interpolator-interpolate}


Interpolating files is straightforward once we have this machinery in place.
We modify \texttt{Cache.find} to return a directory and a file path,
then add an \texttt{interpolate} method to replace special comments:


\begin{lstlisting}[frame=tblr]
class Cache {
  // ...
  interpolate (fileDir, outer) {
    return outer.replace(Cache.INTERPOLATE_PAT,
                         (match, comment, filename) => {
      filename = filename.trim()
      const filePath = path.join(fileDir, filename)
      if (!fs.existsSync(filePath)) {
        throw new Error(`Cannot find ${filePath}`)
      }
      const inner = fs.readFileSync(filePath, 'utf-8')
      return inner
    })
  }
  // ...
}
Cache.INTERPOLATE_PAT = /\/\*\+(.+?)\+(.+?)\+\*\//g
\end{lstlisting}



We can now have a file like this:


\begin{lstlisting}[frame=tblr]
class Example {
  constructor (msg) {
    this.constructorMessage = msg
  }
  /*+ top method + import-interpolate-topmethod.js +*/
  /*+ bottom method + import-interpolate-bottommethod.js +*/
}

Example
\end{lstlisting}



\noindent and subfiles like this:


\begin{lstlisting}[frame=tblr]
topMethod (msg) {
  this.bottomMethod(`(topMethod ${msg})`)
}
\end{lstlisting}



\noindent and this:


\begin{lstlisting}[frame=tblr]
bottomMethod (msg) {
  console.log(`(bottomMethod ${msg})`)
}
\end{lstlisting}



Let’s test it:


\begin{lstlisting}[frame=shadowbox]
node test-import-interpolate.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
(bottomMethod (topMethod called from test-import-interpolate.js))
\end{lstlisting}



When this program runs, its lifecycle\index{lifecycle!of file interpolation} is:

\begin{enumerate}

\item Node starts to run \texttt{test-import-interpolate.js}.

\item It sees the \texttt{import} of \texttt{need-interpolate} so it reads and evaluates that code.

\item Doing this creates a singleton cache object.

\item The program then calls \texttt{need({\textquotesingle}./import-interpolate.js{\textquotesingle})}.

\item This checks the cache: nope, nothing there.

\item So it loads \texttt{import-interpolate.js}.

\item It finds two specially-formatted comments in the text...

\item ...so it loads the file described by each one and inserts the text in place of the comment.

\item Now that it has the complete text, it calls \texttt{eval}...

\item ...and stores the result of \texttt{eval} (which is a class) in the cache.

\item It also returns that class.

\item We then create an instance of that class and call its method.

\end{enumerate}


\noindent This works,
but as we said in the introduction we decided not to use it
because it didn’t play well with other tools.
No piece of software exists in isolation;
when we evaluate a design,
we always have to ask how it fits into everything else we have.

\section{What did we do instead?}\label{file-interpolator-instead}


Rather than interpolating file fragments,
we extract or erase parts of regular JavaScript files
based on specially-formatted comments
like the \texttt{<fragment>...</fragment>} pair shown below.

\begin{lstlisting}[frame=tblr]
class Example {
  constructor (name) {
    this.name = name
  }

  // <fragment>
  fragment (message) {
    console.log(`${name}: ${message}`)
  }
  // </fragment>
}
\end{lstlisting}


The code that selects the part of the file we want to display
is part of our page templating system.
It re-extracts code for display every time the web version of this site is built,
which ensures that we always shows what’s in the current version of our examples.
However,
this system doesn’t automatically update the description of the code:
if we write, “It does X,”
then modify the code to do Y,
our lesson can be inconsistent.
\glossref{Literate programming}\index{literate programming} was invented
to try to prevent this from happening,
but it never really caught on—unfortunately,
most programming systems that describe themselves as “literate” these days
only implement part of \hreffoot{Donald Knuth’s}{https://www-cs-faculty.stanford.edu/~knuth/}\index{Knuth, Donald} original vision.

\section{Exercises}\label{file-interpolator-exercises}

\subsection*{Security concerns}

\begin{enumerate}

\item 

Write a function \texttt{loadAndRun} that reads a file, evaluates it, and returns the result.



\item 

Create a file \texttt{trust-me.js} that prints “nothing happening here” when it is evaluated,
    but also deletes everything in the directory called \texttt{target}.



\item 

Write tests for this using [\texttt{mock-fs}][node-mock-fs].



\end{enumerate}


\noindent Please be careful doing this exercise.

\subsection*{Loading functions}


Write a function that reads a file containing single-argument functions like this:

\begin{lstlisting}[frame=tblr]
addOne: (x) => x + 1
halve: (x) => x / 2
array: (x) => Array(x).fill(0)
\end{lstlisting}


\noindent and returns an object containing callable functions.

\subsection*{Registering functions}


Write a function that loads one or more files containing function definitions like this:

\begin{lstlisting}[frame=tblr]
const double = (x) => {
  return 2 * x
}

EXPORTS.append(double)
\end{lstlisting}


\noindent and returns a list containing all the loaded functions.

\subsection*{Indenting inclusions}


Modify the file inclusion system
so that inclusions are indented by the same amount as the including comment.
For example,
if the including file is:

\begin{lstlisting}[frame=tblr]
const withLogging = (args) => {
  /*+ logging call + logging.js +*/
}

withLogging
\end{lstlisting}


\noindent and the included file is:

\begin{lstlisting}[frame=tblr]
console.log('first message')
console.log('second message')
\end{lstlisting}


\noindent then the result will be:

\begin{lstlisting}[frame=tblr]
const withLogging = (args) => {
  console.log('first message')
  console.log('second message')
}

withLogging
\end{lstlisting}


\noindent i.e., all lines of the inclusion will be indented to match the first.

\subsection*{Interpolating from subdirectories}


Modify the file interpolator so that snippets can be included from sub-directories using relative paths.

\subsection*{Recursive search for inclusions}

\begin{enumerate}

\item 

Modify the file interpolator so that it searches recursively
    through all subdirectories of the directories on the search path
    to find inclusions.



\item 

Explain why this is a bad idea.



\end{enumerate}

\subsection*{Defining variables}


Modify the file inclusion system so that users can pass in a \texttt{Map} containing name-value pairs
and have these interpolated into the text of the files being loaded.
To interpolate a value,
the included file must use \texttt{@@name@@}.

\subsection*{Specifying markers}


Modify the file inclusion system so that the user can override the inclusion comment markers.
For example, the user should be able to specify that \texttt{/*!} and \texttt{!*/} be used to mark inclusions.
(This is often used in tutorials that need to show the inclusion markers without them being interpreted.)

\subsection*{Recursive inclusions}


Modify the file interpolator to support recursive includes,
i.e.,
to handle inclusion markers in files that are being included.
Be sure to check for the case of infinite includes.

\subsection*{Slicing files}


Write a function that reads a JavaScript source file
containing specially-formatted comments like the ones shown below
and extracts the indicated section.

\begin{lstlisting}[frame=tblr]
const toBeLeftOut = (args) => {
  console.log('this should not appear')
}

// <keepThis>
const toBeKept = (args) => {
  console.log('only this function should appear')
}
// </keepThis>
\end{lstlisting}


Users should be able to specify any tag they want,
and if that tag occurs multiple times,
all of the sections marked with that tag should be kept.
(This is the approach we took for this book instead of file interpolation.)

\chapter{Module Loader}\label{module-loader}


\noindent 
  Terms defined: \glossref{absolute path}, \glossref{alias}, \glossref{circular dependency}, \glossref{closure}, \glossref{directed graph}, \glossref{encapsulate}, \glossref{immediately-invoked function expression}, \glossref{inner function}, \glossref{Least Recently Used cache}, \glossref{namespace}, \glossref{plugin architecture}



\chapref{file-interpolator} showed how to use \texttt{eval} to load code dynamically.
We can use this to build our own version of JavaScript’s \texttt{require} function.
Our function will take the name of a source file as an argument
and return whatever that file exports.
The key requirement for such a function is to avoid accidentally overwriting things:
if we just \texttt{eval} some code and it happens to assign to a variable called \texttt{x},
anything called \texttt{x} already in our program might be overwritten.
We therefore need a way to \glossref{encapsulate}\index{encapsulation}\index{software design!encapsulation} the contents of what we’re loading.
Our approach is based on \cite{Casciaro2020},
which contains a lot of other useful information as well.

\section{How can we implement namespaces?}\label{module-loader-namespace}


A \glossref{namespace}\index{namespace} is a collection of names in a program
that are isolated from other namespaces.
Most modern languages provide namespaces as a built-in feature
so that programmers don’t accidentally step on each other’s toes.
JavaScript doesn’t,
so we have to implement them ourselves.


We can do this using \glossref{closures}\index{closure}.
Every function is a namespace:
variables defined inside the function are distinct from variables defined outside it
(\figref{module-loader-closures}).
If we create the variables we want to manage inside a function,
then defined another function inside the first
and return that \glossref{inner function}\index{inner function}\index{function!inner},
that inner function will be the only thing with references to those variables.

\figpdfhere{module-loader-closures}{./module-loader/closures.pdf}{Using closures to create private variables.}{0.6}


For example,
let’s create a function that always appends the same string to its argument:


\begin{lstlisting}[frame=tblr]
const createAppender = (suffix) => {
  const appender = (text) => {
    return text + suffix
  }
  return appender
}

const exampleFunction = createAppender(' and that')
console.log(exampleFunction('this'))
console.log('suffix is', suffix)
\end{lstlisting}



\noindent When we run it,
the value that was assigned to the parameter \texttt{suffix} still exists
but can only be reached by the inner function:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
this and that
/u/stjs/module-loader/manual-namespacing.js:10
console.log('suffix is', suffix)
                         ^

ReferenceError: suffix is not defined
    at /u/stjs/module-loader/manual-namespacing.js:10:26
    at ModuleJob.run (internal/modules/esm/module_job.js:152:23)
    at async Loader.import (internal/modules/esm/loader.js:166:24)
    at async Object.loadESM (internal/process/esm_loader.js:68:5)
\end{lstlisting}



We could require every module to define a setup function like this for users to call,
but thanks to \texttt{eval} we can wrap the file’s contents in a function and call it automatically.
To do this we will create something called
an \glossref{immediately-invoked function expression}\index{immediately-invoked function expression} (IIFE).
The syntax \texttt{() => \{...\}} defines a function.
If we put the definition in parentheses and then put another pair of parentheses right after it:

\begin{lstlisting}[frame=tblr]
(() => {...})()
\end{lstlisting}


\noindent we have code that defines a function of no arguments and immediately calls it.
We can use this trick to achieve the same effect as the previous example in one step:


\begin{lstlisting}[frame=tblr]
const contents = (() => {
  const privateValue = 'private value'
  const publicValue = 'public value'
  return { publicValue }
})()

console.log(`contents.publicValue is ${contents.publicValue}`)
console.log(`contents.privateValue is ${contents.privateValue}`)
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
contents.publicValue is public value
contents.privateValue is undefined
\end{lstlisting}


\begin{callout}


\subsubsection*{Unconfusing the parser}


The extra parentheses around the original definition force the parser to evaluate things in the right order;
if we write:

\begin{lstlisting}[frame=tblr]
() => {...}()
\end{lstlisting}


\noindent then JavaScript interprets it as a function definition followed by an empty expression
rather than an immediate call to the function just defined.

\end{callout}

\section{How can we load a module?}\label{module-loader-load}


We want the module we are loading to export names by assigning to \texttt{module.exports} just as \texttt{require} does,
so we need to provide an object called \texttt{module} and create a IIFE.
(We will handle the problem of the module loading other modules later.)
Our \texttt{loadModule} function takes a filename and returns a newly created module object;
the parameter to the function we build and \texttt{eval} must be called \texttt{module} so that we can assign to \texttt{module.exports}.
For clarity,
we call the object we pass in \texttt{result} in \texttt{loadModule}.


\begin{lstlisting}[frame=tblr]
import fs from 'fs'

const loadModule = (filename) => {
  const source = fs.readFileSync(filename, 'utf-8')
  const result = {}
  const fullText = `((module) => {${source}})(result)`
  console.log(`full text for eval:\n${fullText}\n`)
  eval(fullText)
  return result.exports
}

export default loadModule
\end{lstlisting}


\figpdf{module-loader-iife-a}{./module-loader/iife-a.pdf}{Using IIFEs to encapsulate modules and get their exports (part 1).}{0.6}

\figpdf{module-loader-iife-b}{./module-loader/iife-b.pdf}{Using IIFEs to encapsulate modules and get their exports (part 2).}{0.6}


\figref{module-loader-iife-a} and \figref{module-loader-iife-b} show the structure of our loader so far.
We can use this code as a test:


\begin{lstlisting}[frame=tblr]
const publicValue = 'public value'

const privateValue = 'private value'

const publicFunction = (caller) => {
  return `publicFunction called from ${caller}`
}

module.exports = { publicValue, publicFunction }
\end{lstlisting}



\noindent and this short program to load the test and check its exports:


\begin{lstlisting}[frame=tblr]
import loadModule from './load-module-only.js'

const result = loadModule(process.argv[2])
console.log(`result.publicValue is ${result.publicValue}`)
console.log(`result.privateValue is ${result.privateValue}`)
console.log(result.publicFunction('main'))
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node test-load-module-only.js small-module.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
full text for eval:
((module) => {const publicValue = 'public value'

const privateValue = 'private value'

const publicFunction = (caller) => {
  return `publicFunction called from ${caller}`
}

module.exports = { publicValue, publicFunction }
})(result)

result.publicValue is public value
result.privateValue is undefined
publicFunction called from main
\end{lstlisting}


\section{Do we need to handle circular dependencies?}\label{module-loader-circular}


What if the code we are loading loads other code?
We can visualize the network of who requires whom as a \glossref{directed graph}\index{directed graph}:
if X requires Y,
we draw an arrow from X to Y.
Unlike the directed \emph{acyclic} graphs\index{directed acyclic graph} we met in \chapref{build-manager},
though,
these graphs can contain cycles:
we say a \glossref{circular dependency}\index{circular dependency} exists
if X depends on Y and Y depends on X
either directly or indirectly.
This may seem nonsensical,
but can easily arise with \glossref{plugin architectures}\index{plugin architecture}\index{software design!plugin architecture}:
the file containing the main program loads an extension,
and that extension calls utility functions defined in the file containing the main program.


Most compiled languages can handle circular dependencies easily:
they compile each module into low-level instructions,
then link those to resolve dependencies before running anything
(\figref{module-loader-circularity}).
But interpreted languages usually run code as they’re loading it,
so if X is in the process of loading Y and Y tries to call X,
X may not (fully) exist yet.

\figpdf{module-loader-circularity}{./module-loader/circularity.pdf}{Testing circular imports.}{0.6}


Circular dependencies work in \hreffoot{Python}{https://www.python.org/}\index{Python},
but only sort of.
Let’s create two files called \texttt{major.py} and \texttt{minor.py}:


\begin{lstlisting}[frame=tblr]
# major.py

import minor

def top():
    print("top")
    minor.middle()

def bottom():
    print("bottom")

top()
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
# minor.py

import major

def middle():
    print("middle")
    major.bottom()
\end{lstlisting}



Loading fails when we run \texttt{major.py} from the command line:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
top
Traceback (most recent call last):
  File "major.py", line 3, in <module>
    import minor
  File "/u/stjs/module-loader/checking/minor.py", line 3, in <module>
    import major
  File "/u/stjs/module-loader/checking/major.py", line 12, in <module>
    top()
  File "/u/stjs/module-loader/checking/major.py", line 7, in top
    minor.middle()
AttributeError: module 'minor' has no attribute 'middle'
\end{lstlisting}



\noindent but works in the interactive interpreter:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
$ python
>>> import major
top
middle
bottom
\end{lstlisting}



The equivalent test in JavaScript also has two files:


\begin{lstlisting}[frame=tblr]
// major.js
const { middle } = require('./minor')

const top = () => {
  console.log('top')
  middle()
}

const bottom = () => {
  console.log('bottom')
}

top()

module.exports = { top, bottom }
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
// minor.js
const { bottom } = require('./major')

const middle = () => {
  console.log('middle')
  bottom()
}

module.exports = { middle }
\end{lstlisting}



\noindent It fails on the command line:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
top
middle
/u/stjs/module-loader/checking/minor.js:6
  bottom()
  ^

TypeError: bottom is not a function
    at middle (/u/stjs/module-loader/checking/minor.js:6:3)
    at top (/u/stjs/module-loader/checking/major.js:6:3)
    at Object.<anonymous> (/u/stjs/module-loader/checking/major.js:13:1)
    at Module._compile (internal/modules/cjs/loader.js:1063:30)
    at Object.Module._extensions..js \
 (internal/modules/cjs/loader.js:1092:10)
    at Module.load (internal/modules/cjs/loader.js:928:32)
    at Function.Module._load (internal/modules/cjs/loader.js:769:14)
    at Function.executeUserEntryPoint [as runMain] \
 (internal/modules/run_main.js:72:12)
    at internal/main/run_main_module.js:17:47
\end{lstlisting}



\noindent and also fails in the interactive interpreter
(which is more consistent):


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
$ node
> require('./major')
top
middle
/u/stjs/module-loader/checking/minor.js:6
  bottom()
  ^

TypeError: bottom is not a function
    at middle (/u/stjs/module-loader/checking/minor.js:6:3)
    at top (/u/stjs/module-loader/checking/major.js:6:3)
    at Object.<anonymous> (/u/stjs/module-loader/checking/major.js:13:1)
    at Module._compile (internal/modules/cjs/loader.js:1063:30)
    at Object.Module._extensions..js \
 (internal/modules/cjs/loader.js:1092:10)
    at Module.load (internal/modules/cjs/loader.js:928:32)
    at Function.Module._load (internal/modules/cjs/loader.js:769:14)
    at Module.require (internal/modules/cjs/loader.js:952:19)
    at require (internal/modules/cjs/helpers.js:88:18)
    at [stdin]:1:1
\end{lstlisting}



We therefore won’t try to handle circular dependencies.
However,
we will detect them and generate a sensible error message.

\begin{callout}


\subsubsection*{\texttt{import} vs. \texttt{require}}


Circular dependencies work JavaScript’s \texttt{import} syntax
because we can analyze files to determine what needs what,
get everything into memory,
and then resolve dependencies.
We can’t do this with \texttt{require}-based code
because someone might create an \glossref{alias}\index{alias!during import}\index{import!alias}
and call \texttt{require} through that
or \texttt{eval} a string that contains a \texttt{require} call.
(Of course, they can also do these things with the function version of \texttt{import}.)

\end{callout}

\section{How can a module load another module?}\label{module-loader-subload}


While we’re not going to handle circular dependencies,
modules do need to be able to load other modules.
To enable this,
we need to provide the module with a function called \texttt{require}
that it can call as it loads.
As in \chapref{file-interpolator},
this function checks a cache\index{cache!of loaded files}
to see if the file being asked for has already been loaded.
If not, it loads it and saves it;
either way, it returns the result.


Our cache needs to be careful about how it identifies files
so that it can detect duplicates loading attempts that use different names.
For example,
suppose that \texttt{major.js} loads \texttt{subdir/first.js} and \texttt{subdir/second.js}.
When \texttt{subdir/second.js} loads \texttt{./first.js},
our system needs to realize that it already has that file
even though the path looks different.
We will use \glossref{absolute paths} as cache keys
so that every file has a unique, predictable key.


To reduce confusion,
we will call our function \texttt{need} instead of \texttt{require}.
In order to make the cache available to modules while they’re loading,
we will make it a property of \texttt{need}.
(Remember,
a function is just another kind of object in JavaScript;
every function gets several properties automatically,
and we can always add more.)
Since we’re using the built-in \texttt{Map} class as a cache,
the entire implementation of \texttt{need} is just 15 lines long:


\begin{lstlisting}[frame=tblr]
import path from 'path'

import loadModule from './load-module.js'

const need = (name) => {
  const absPath = path.resolve(name)
  if (!need.cache.has(absPath)) {
    const contents = loadModule(absPath, need)
    need.cache.set(absPath, contents)
  }
  return need.cache.get(absPath)
}
need.cache = new Map()

export default need
\end{lstlisting}



We now need to modify \texttt{loadModule} to take our function \texttt{need} as a parameter.
(Again, we’ll have our modules call \texttt{need({\textquotesingle}something.js{\textquotesingle})} instead of \texttt{require({\textquotesingle}something{\textquotesingle})} for clarity.)
Let’s test it with the same small module that doesn’t need anything else to make sure we haven’t broken anything:


\begin{lstlisting}[frame=tblr]
import need from './need.js'

const small = need('small-module.js')
console.log(`small.publicValue is ${small.publicValue}`)
console.log(`small.privateValue is ${small.privateValue}`)
console.log(small.publicFunction('main'))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
full text for eval:
((module, need) => {
const publicValue = 'public value'

const privateValue = 'private value'

const publicFunction = (caller) => {
  return `publicFunction called from ${caller}`
}

module.exports = { publicValue, publicFunction }

})(result, need)

small.publicValue is public value
small.privateValue is undefined
publicFunction called from main
\end{lstlisting}



What if we test it with a module that \emph{does} load something else?


\begin{lstlisting}[frame=tblr]
import need from './need'

const small = need('small-module.js')

const large = (caller) => {
  console.log(`large from ${caller}`)
  small.publicFunction(`${caller} to large`)
}

export default large
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
import need from './need.js'

const large = need('large-module.js')
console.log(large.large('main'))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
full text for eval:
((module, need) => {
import need from './need'

const small = need('small-module.js')

const large = (caller) => {
  console.log(`large from ${caller}`)
  small.publicFunction(`${caller} to large`)
}

export default large

})(result, need)

undefined:2
import need from './need'
^^^^^^

SyntaxError: Cannot use import statement outside a module
    at loadModule (/u/stjs/module-loader/load-module.js:8:8)
    at need (/u/stjs/module-loader/need.js:8:22)
    at /u/stjs/module-loader/test-need-large-module.js:3:15
    at ModuleJob.run (internal/modules/esm/module_job.js:152:23)
    at async Loader.import (internal/modules/esm/loader.js:166:24)
    at async Object.loadESM (internal/process/esm_loader.js:68:5)
\end{lstlisting}



This doesn’t work because \texttt{import} only works at the top level of a program,
not inside a function.
Our system can therefore only run loaded modules by \texttt{need}ing them:


\begin{lstlisting}[frame=tblr]
const small = need('small-module.js')

const large = (caller) => {
  return small.publicFunction(`large called from ${caller}`)
}

module.exports = large
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
import need from './need.js'

const large = need('large-needless.js')
console.log(large('main'))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
full text for eval:
((module, need) => {
const small = need('small-module.js')

const large = (caller) => {
  return small.publicFunction(`large called from ${caller}`)
}

module.exports = large

})(result, need)

full text for eval:
((module, need) => {
const publicValue = 'public value'

const privateValue = 'private value'

const publicFunction = (caller) => {
  return `publicFunction called from ${caller}`
}

module.exports = { publicValue, publicFunction }

})(result, need)

publicFunction called from large called from main
\end{lstlisting}


\begin{callout}


\subsubsection*{“It’s so deep it’s meaningless”}


The programs we have written in this chapter are harder to understand
than most of the programs in earlier chapters
because they are so abstract.
Reading through them,
it’s easy to get the feeling that everything is happening somewhere else.
Programmers’ tools are often like this:
there’s always a risk of confusing the thing in the program
with the thing the program is working on.
Drawing pictures of data structures can help,
and so can practicing with closures
(which are one of the most powerful ideas in programming),
but a lot of the difficulty is irreducible,
so don’t feel bad if it takes you a while to wrap your head around it.

\end{callout}

\section{Exercises}\label{module-loader-exercises}

\subsection*{Counting with closures}


Write a function \texttt{makeCounter} that returns a function
that produces the next integer in sequence starting from zero each time it is called.
Each function returned by \texttt{makeCounter} must count independently, so:

\begin{lstlisting}[frame=tblr]
left = makeCounter()
right = makeCounter()
console.log(`left ${left()`)
console.log(`right ${right()`)
console.log(`left ${left()`)
console.log(`right ${right()`)
\end{lstlisting}


\noindent must produce:

\begin{lstlisting}[frame=tblr]
left 0
right 0
left 1
right `
\end{lstlisting}

\subsection*{Objects and namespaces}


A JavaScript object stores key-value pairs,
and the keys in one object are separate from the keys in another.
Why doesn’t this provide the same level of safety as a closure?

\subsection*{Testing module loading}


Write tests for \texttt{need.js} using Mocha and \texttt{mock-fs}.

\subsection*{Using \texttt{module} as a name}


What happens if we define the variable \texttt{module} in \texttt{loadModule}
so that it is in scope when \texttt{eval} is called
rather than creating a variable called \texttt{result} and passing that in:

\begin{lstlisting}[frame=tblr]
const loadModule = (filename) => {
  const source = fs.readFileSync(filename, 'utf-8')
  const module = {}
  const fullText = `(() => {${source}})()`
  eval(fullText)
  return module.exports
}
\end{lstlisting}

\subsection*{Implementing a search path}


Add a search path to \texttt{need.js} so that if a module isn’t found locally,
it will be looked for in each directory in the search path in order.

\subsection*{Using a setup function}


Rewrite the module loader so that every module has a function called \texttt{setup}
that must be called after loading it to create its exports
rather than using \texttt{module.exports}.

\subsection*{Handling errors while loading}

\begin{enumerate}

\item 

Modify \texttt{need.js} so that it does something graceful
    if an exception is thrown while a module is being loaded.



\item 

Write unit tests for this using Mocha.



\end{enumerate}

\subsection*{Refactoring circularity}


Suppose that \texttt{main.js} contains this:


\begin{lstlisting}[frame=tblr]
const PLUGINS = []

const plugin = require('./plugin')

const main = () => {
  PLUGINS.forEach(p => p())
}

const loadPlugin = (plugin) => {
  PLUGINS.push(plugin)
}

module.exports = {
  main,
  loadPlugin
}
\end{lstlisting}



\noindent and \texttt{plugin.js} contains this:


\begin{lstlisting}[frame=tblr]
const { loadPlugin } = require('./main')

const printMessage = () => {
  console.log('running plugin')
}

loadPlugin(printMessage)
\end{lstlisting}



\noindent Refactor this code so that it works correctly while still using \texttt{require} rather than \texttt{import}.

\subsection*{An LRU cache}


A \glossref{Least Recently Used (LRU) cache}
reduces access time while limiting the amount of memory used
by keeping track of the N items that have been used most recently.
For example,
if the cache size is 3 and objects are accessed in the order shown in the first column,
the cache’s contents will be as shown in the second column:


\vspace{\baselineskip}
\begin{tabular}{lll}
\textbf{\underline{Item}} & \textbf{\underline{Action}} & \textbf{\underline{Cache After Access}} \\
A & read A & [A] \\
A & get A from cache & [A] \\
B & read B & [B, A] \\
A & get A from cache & [A, B] \\
C & read C & [C, A, B] \\
D & read D & [D, C, A] \\
B & read B & [B, D, C] \\
\end{tabular}

\vspace{\baselineskip}

\begin{enumerate}

\item 

Implement a function \texttt{cachedRead} that takes the number of entries in the cache as an argument
    and returns a function that uses an LRU cache
    to either read files or return cached copies.



\item 

Modify \texttt{cachedRead} so that the number of items in the cache
    is determined by their combined size
    rather than by the number of files.



\end{enumerate}

\subsection*{Make functions safe for renaming}


Our implementation of \texttt{need} implemented the cache as a property of the function itself.

\begin{enumerate}

\item 

How can this go wrong?
    (Hint: thing about aliases.)



\item 

Modify the implementation to solve this problem using a closure.



\end{enumerate}

\chapter{Style Checker}\label{style-checker}


\noindent 
  Terms defined: \glossref{abstract syntax tree}, \glossref{Adapter pattern}, \glossref{column-major storage}, \glossref{dynamic lookup}, \glossref{generator function}, \glossref{intrinsic complexity}, \glossref{Iterator pattern}, \glossref{linter}, \glossref{Markdown}, \glossref{row-major storage}, \glossref{walk (a tree)}



Programmers argue endlessly about the best way to format their programs,
but everyone agrees that the most important thing is to be consistent\index{coding style!importance of consistency}
\cite{Binkley2012,Johnson2019}.
Since checking rules by hand is tedious,
most programmers use tools to compare code against various rules and report any violations.
Programs that do this are often called \glossref{linters}\index{linter}\index{coding style!linter}
in honor of an early one for C\index{C} named \texttt{lint}
(because it looked for fluff in source code).


In this chapter we will build a simple linter of our own inspired by \hreffoot{ESLint}{https://eslint.org/}\index{ESLint},
which we use to check the code in this book.
Our tool will parse source code to create a data structure,
then go through that data structure and apply rules for each part of the program.
It will also introduce us to one of the key ideas of this book,
which is that source code is just another kind of data.

\begin{callout}


\subsubsection*{Don’t define your own style}


Just as the world doesn’t need more file format (\chapref{regex-parser})
it also doesn’t need more programming styles,
or more arguments among programmers about whether there should be spaces before curly braces or not.
\hreffoot{Standard JS}{https://standardjs.com/}\index{Standard JS} may not do everything exactly the way you want,
but adopting it increases the odds that other programmers will be able to read your code at first glance.

\end{callout}

\section{How can we parse JavaScript to create an AST?}\label{style-checker-ast}


A parser for a simple language like arithmetic or JSON is relatively easy to write.
A parser for a language as complex as JavaScript is much more work,
so we will use one called \hreffoot{Acorn}{https://github.com/acornjs/acorn}\index{Acorn} instead.
Acorn takes a string containing source code as input
and produces an \glossref{abstract syntax tree}\index{abstract syntax tree} (AST)
whose nodes store information about what’s in the program
(\figref{style-checker-parse-tree}).
An AST is for a program what the DOM\index{Document Object Model} is for HTML:
an in-memory representation that is easy for software to inspect and manipulate.


ASTs can be quite complex—for example,
the JSON representation of the AST for a single constant declaration
is 84 lines long:


\begin{lstlisting}[frame=tblr]
import acorn from 'acorn'

const ast = acorn.parse('const x = 0', { locations: true })
console.log(JSON.stringify(ast, null, 2))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{
  "type": "Program",
  "start": 0,
  "end": 11,
  "loc": {
    "start": {
      "line": 1,
      "column": 0
    },
    "end": {
...
            "value": 0,
            "raw": "0"
          }
        }
      ],
      "kind": "const"
    }
  ],
  "sourceType": "script"
}
\end{lstlisting}


\figpdfhere{style-checker-parse-tree}{./style-checker/parse-tree.pdf}{The parse tree of a simple program.}{0.6}


Acorn’s output is in \hreffoot{Esprima}{https://esprima.org/} format\index{Esprima format}
(so-called because it was originally defined by a tool with that name).
The format’s specification is very detailed,
but we can usually figure out most of what we need by inspection.
For example,
here is the output for a 15-line program:


\begin{lstlisting}[frame=tblr]
import acorn from 'acorn'

const program = `const value = 2

const double = (x) => {
  const y = 2 * x
  return y
}

const result = double(value)
console.log(result)
`

const ast = acorn.parse(program, { locations: true })
console.log(JSON.stringify(ast, null, 2))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{
  "type": "Program",
  "start": 0,
  "end": 122,
  "loc": {
    "start": {
      "line": 1,
      "column": 0
    },
    "end": {
...
          "line": 1,
          "column": 0
        },
        "end": {
          "line": 1,
          "column": 15
        }
      },
      "declarations": [
...480 more lines...
\end{lstlisting}



\noindent Yes, it really is almost 500 lines long...

\section{How can we find things in an AST?}\label{style-checker-search}


If we want to find functions, variables, or anything else in an AST
we need to \glossref{walk the tree}\index{walk a tree},
i.e.,
to visit each node in turn.
The \hreffoot{\texttt{acorn-walk}}{https://www.npmjs.com/package/acorn-walk} library will do this for us
using the Visitor design pattern\index{Visitor pattern}\index{design pattern!Visitor} we first saw in \chapref{page-templates}
If we provide a function to act on nodes of type \texttt{Identifier},
\texttt{acorn-walk} will call that function each time it finds an identifier.
We can use other options to say that we want to record the locations of nodes (i.e., their line numbers)
and to collect comments in an array called \texttt{onComment}.
Our function can do whatever we want;
for demonstration purposes we will add nodes to an array called \texttt{state}
and report them all at the end
(\figref{style-checker-walk-tree}).


\begin{lstlisting}[frame=tblr]
import acorn from 'acorn'
import walk from 'acorn-walk'

const program = `// Constant
const value = 2

// Function
const double = (x) => {
  const y = 2 * x
  return y
}

// Main body
const result = double(value)
console.log(result)
`

const options = {
  locations: true,
  onComment: []
}
const ast = acorn.parse(program, options)

const state = []
walk.simple(ast, {
  Identifier: (node, state) => {
    state.push(node)
  }
}, null, state)

state.forEach(node => console.log(
  `identifier ${node.name} on line ${node.loc.start.line}`
))
const comments = options.onComment.map(
  node => node.loc.start.line
).join(', ')
console.log(`comments on lines ${comments}`)
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
identifier x on line 6
identifier y on line 7
identifier double on line 11
identifier value on line 11
identifier console on line 12
identifier result on line 12
comments on lines 1, 4, 10
\end{lstlisting}


\begin{callout}


\subsubsection*{There’s more than one way to do it}


\texttt{walk.simple} takes four arguments:

\begin{enumerate}

\item 

The root node of the AST, which is used as the starting point.



\item 

An object containing callback functions for handling various kinds of nodes.



\item 

Another object that specifies what algorithm to use—we have set this to \texttt{null}
    to use the default because
    we don’t particularly care about the order in which the nodes are processed.



\item 

Something we want passed in to each of the node handlers,
    which in our case is the \texttt{state} array.
    If our node handling functions don’t require any extra data
    from one call to the next
    we can leave this out;
    if we want to accumulate information across calls,
    this argument acts as the Visitor’s memory.



\end{enumerate}


\noindent Any general-purpose implementation of the Visitor pattern
is going to need these four things,
but as we will see below,
we can implement them in different ways.

\end{callout}

\figpdfhere{style-checker-walk-tree}{./style-checker/walk-tree.pdf}{Walking a tree to perform an operation at each node.}{0.6}

\section{How can we apply checks?}\label{style-checker-apply}


We don’t just want to collect nodes:
we want to check their properties against a set of rules.
One way to do this would be to call \texttt{walk.simple} once for each rule,
passing it a function that checks just that rule.
Another way—the one we’ll use—is to write a generic function\index{software design!generic function}
that checks a rule and records any nodes that don’t satisfy it,
and then call that function once for each rule inside our \texttt{Identifier} handler.
This may seem like extra work,
but it ensures that all of our rule-checkers store their results in the same way,
which in turn means that we can write one reporting function
and be sure it will handle everything.


The function  \texttt{applyCheck} takes the current state (where we are accumulating rule violations),
a label that identifies this rule (so that violations of it can be stored together),
the node,
and a logical value telling it whether the node passed the test or not.
If the node failed the test
we make sure that \texttt{state} contains a list with the appropriate label
and then append this node to it.
This “create storage space on demand” pattern
is widely used but doesn’t have a well-known name.


\begin{lstlisting}[frame=tblr]
const applyCheck = (state, label, node, passes) => {
  if (!passes) {
    if (!(label in state)) {
      state[label] = []
    }
    state[label].push(node)
  }
}
\end{lstlisting}



We can now put a call to \texttt{applyCheck} inside the handler for \texttt{Identifier}:


\begin{lstlisting}[frame=tblr]
const ast = acorn.parse(program, { locations: true })

const state = {}
walk.simple(ast, {
  Identifier: (node, state) => {
    applyCheck(state, 'name_length', node, node.name.length >= 4)
  }
}, null, state)

state.name_length.forEach(
  node => console.log(`${node.name} at line ${node.loc.start.line}`))
\end{lstlisting}



\noindent We can’t just use \texttt{applyCheck} as the handler for \texttt{Identifier}
because \texttt{walk.simple} wouldn’t know how to call it.
This is a (very simple) example of the \glossref{Adapter}\index{Adapter pattern}\index{design pattern!Adapter} design pattern:
we write a function or class to connect the code we want to call
to the already-written code that is going to call it.


The output for the same sample program as before is:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
x at line 6
y at line 7
\end{lstlisting}



\noindent The exercises will ask why the parameter \texttt{x} doesn’t show up
as a violation of our rule
that variables’ names must be at least four characters long.

\section{How does the AST walker work?}\label{style-checker-walker}


The AST walker uses the Visitor pattern,
but how does it actually work?
We can build our own by defining a class with methods that walk the tree,
take action depending on the kind of node,
and then go through the children of that node (if any).
The user can then derive a class of their own from this
and override the set of action methods they’re interested in.


One key difference between our implementation and \texttt{acorn-walk}‘s is that
our methods don’t need to take \texttt{state} as a parameter
because it’s contained in the object that they’re part of.
That simplifies the methods—one less parameter—but it does mean that
anyone who wants to use our visitor has to derive a class,
which is a bit more complicated than writing a function.
This tradeoff is a sign that managing state is part of the problem’s
\glossref{intrinsic complexity}\index{intrinsic complexity}:
we can move it around,
but we can’t get rid of it.


The other difference between our visitor and \texttt{acorn-walk} is that
our class uses \glossref{dynamic lookup}\index{dynamic lookup}
(a form of introspection\index{introspection!of methods})
to look up a method
with the same name as the node type in the object.
While we normally refer to a particular method of an object using \texttt{object.method},
we can also look them up by asking for \texttt{object[name]}
in the same way that we would look up any other property of any other object.
Our completed class looks like this:


\begin{lstlisting}[frame=tblr]
class Walker {
  // Construct a new AST tree walker.
  constructor (ast) {
    this.ast = ast
  }

  // Walk the tree.
  walk (accumulator) {
    this.stack = []
    this._walk(this.ast, accumulator)
    return accumulator
  }

  // Act on node and then on children.
  _walk (node, accumulator) {
    if (node && (typeof node === 'object') && ('type' in node)) {
      this._doNode(node, accumulator)
      this._doChildren(node, accumulator)
    }
  }

  // Handle a single node by lookup.
  _doNode (node, accumulator) {
    if (node.type in this) {
      this[node.type](node, accumulator)
    }
  }

  // Recurse for anything interesting within the node.
  _doChildren (node, accumulator) {
    this.stack.push(node)
    for (const key in node) {
      if (Array.isArray(node[key])) {
        node[key].forEach(child => {
          this._walk(child, accumulator)
        })
      } else if (typeof node[key] === 'object') {
        this._walk(node[key], accumulator)
      }
    }
    this.stack.pop(node)
  }

  // Is the current node a child of some other type of node?
  _childOf (nodeTypes) {
    return this.stack &&
      nodeTypes.includes(this.stack.slice(-1)[0].type)
  }
}
\end{lstlisting}



\noindent The code we need to use it is:


\begin{lstlisting}[frame=tblr]
import acorn from 'acorn'


// Walk to accumulate variable and parameter definitions.
class VariableWalker extends Walker {
  Identifier (node, accumulator) {
    if (this._childOf(['ArrowFunctionExpression',
      'VariableDeclarator'])) {
      accumulator.push(node.name)
    }
  }
}

// Test.
const program = `const value = 2

const double = (x) => {
  const y = 2 * x
  return y
}

const result = double(value)
console.log(result)
`

const ast = acorn.parse(program, { locations: true })
const walker = new VariableWalker(ast)
const accumulator = []
walker.walk(accumulator)
console.log('definitions are', accumulator)
\end{lstlisting}



\noindent and its output is:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
definitions are [ 'value', 'double', 'x', 'y', 'result' ]
\end{lstlisting}



We think this approach to implementing the Visitor pattern is easier to understand and extend
than one that relies on callbacks,
but that could just be a reflection of our background and experience.
As with code style,
the most important thing is consistency:
if we implement Visitor using classes in one place,
we should implement it that way everywhere.

\section{How else could the AST walker work?}\label{style-checker-alternatives}


A third approach to this problem uses
the \glossref{Iterator}\index{Iterator pattern}\index{design pattern!Iterator} design pattern.
Instead of taking the computation to the nodes,
an iterator returns the elements of a structure for processing
(\figref{style-checker-iterator}).
One way to think about it is that Visitor encapsulates recursion,
while Iterator turns everything into a loop.


We can implement the Iterator pattern in JavaScript using
\glossref{generator functions}\index{generator function}\index{Iterator pattern!generator function}.
If we declare a function using \texttt{function *} (with an asterisk) instead of \texttt{function}
then we can use the \texttt{yield} keyword to return a value and suspend processing to be resumed later.
The result of \texttt{yield} is a two-part structure with a value and a flag showing whether or not processing is done:


\begin{lstlisting}[frame=tblr]
function * threeWords () {
  yield 'first'
  yield 'second'
  yield 'third'
}

const gen = threeWords()

console.log(gen.next())
console.log(gen.next())
console.log(gen.next())
console.log(gen.next())
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{ value: 'first', done: false }
{ value: 'second', done: false }
{ value: 'third', done: false }
{ value: undefined, done: true }
\end{lstlisting}


\figpdfhere{style-checker-iterator}{./style-checker/iterator.pdf}{Finding nodes in the tree using the Iterator pattern.}{0.6}


As another example,
this generator takes a string and produces its vowels one by one:


\begin{lstlisting}[frame=tblr]
function * getVowels (text) {
  for (const char of text) {
    if ('AEIOUaeiou'.includes(char)) {
      yield char
    }
  }
}

const test = 'this is a test'
const gen = getVowels(test)
let current = gen.next()
while (!current.done) {
  console.log(current.value)
  current = gen.next()
}
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
i
i
a
e
\end{lstlisting}



\noindent A generator function doesn’t actually generate anything;
instead,
it creates an object that we can then ask for values repeatedly.
This gives us a way to have several generators in play at the same time.


Instead of a \texttt{while} loop it is much more common to use \texttt{for...of},
which knows how to work with generators:


\begin{lstlisting}[frame=tblr]
for (const vowel of getVowels(test)) {
  console.log(vowel)
}
\end{lstlisting}



Finally,
just as \texttt{function *} says “this function is a generator”,
\texttt{yield *} says “yield the values from a nested generator one by one”.
We can use it to walk irregular structures like nested arrays:


\begin{lstlisting}[frame=tblr]
function * getNodes (here) {
  if (typeof here === 'string') {
    yield here
  } else if (Array.isArray(here)) {
    for (const child of here) {
      yield * getNodes(child)
    }
  } else {
    throw new Error(`unknown type "${typeof here}"`)
  }
}

const nested = ['first', ['second', 'third']]
for (const value of getNodes(nested)) {
  console.log(value)
}
\end{lstlisting}



Let’s use generators to count the number of expressions of various types in a program.
The generator function that visits each node is:


\begin{lstlisting}[frame=tblr]
function * getNodes (node) {
  if (node && (typeof node === 'object') && ('type' in node)) {
    yield node
    for (const key in node) {
      if (Array.isArray(node[key])) {
        for (const child of node[key]) {
          yield * getNodes(child)
        }
      } else if (typeof node[key] === 'object') {
        yield * getNodes(node[key])
      }
    }
  }
}
\end{lstlisting}



\noindent and the program that uses it is:


\begin{lstlisting}[frame=tblr]
const ast = acorn.parse(program, { locations: true })
const result = {}
for (const node of getNodes(ast)) {
  if (node.type === 'BinaryExpression') {
    if (node.operator in result) {
      result[node.operator] += 1
    } else {
      result[node.operator] = 1
    }
  }
}
console.log('counts are', result)
\end{lstlisting}



When we run it with our usual test program as input, we get:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
counts are { '*': 2, '+': 1 }
\end{lstlisting}



Generators are a clean solution to many hard problems,
but we find it more difficult to check variable identifiers using generators
than using the class-based Visitor approach
because we want to accumulate violations to report later.
Again,
this could be a reflection of what we’re used to rather than anything intrinsic;
as with coding style,
the most important thing is to be consistent.

\section{What other kinds of analysis can we do?}\label{style-checker-analysis}


As one final example,
consider the problem of keeping track of which methods are defined where
in a deeply-nested class hierarchy.
(This problem comes up in some of the later chapters in this book:
we wrote so many classes that incrementally extended their predecessors for pedagogical purposes
that we lost track of what was defined where.)
To create a table of method definitions,
we first need to find the ancestors of the last class in the hierarchy:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'
import acorn from 'acorn'
import fs from 'fs'
import path from 'path'
import walk from 'acorn-walk'

class FindAncestors {
  find (dirname, filename, className) {
    return this.traceAncestry(dirname, filename, className, [])
  }

  traceAncestry (dirname, filename, className, accum) {
    const fullPath = path.join(dirname, filename)
    const program = fs.readFileSync(fullPath, 'utf-8')
    const options = { locations: true, sourceType: 'module' }
    const ast = acorn.parse(program, options)
    const classDef = this.findClassDef(filename, ast, className)
    accum.push({ filename, className, classDef })
    const ancestorName = this.getAncestor(classDef)
    if (ancestorName === null) {
      return accum
    }
    const ancestorFile = this.findImport(filename, ast, ancestorName)
    return this.traceAncestry(dirname, ancestorFile, ancestorName, accum)
  }

}

export default FindAncestors
\end{lstlisting}



Finding class definitions is a straightforward extension of what we have already done:


\newpage


\begin{lstlisting}[frame=tblr]
  findClassDef (filename, ast, className) {
    const state = []
    walk.simple(ast, {
      ClassDeclaration: (node, state) => {
        if ((node.id.type === 'Identifier') &&
            (node.id.name === className)) {
          state.push(node)
        }
      }
    }, null, state)
    assert(state.length === 1,
      `No definition for ${className} in ${filename}`)
    return state[0]
  }
\end{lstlisting}



To test this code, we start with the last of these three short files:


\begin{lstlisting}[frame=tblr]
class Upper {
  constructor () {
    this.name = 'upper'
  }

  report () {
    console.log(this.modify(this.name))
  }

  modify (text) {
    return text.toUpperCase()
  }
}

module.exports = Upper
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
import Upper from './upper.js'

class Middle extends Upper {
  constructor () {
    super()
    this.range = 'middle'
  }

  modify (text) {
    return `** ${super.modify(text)} **`
  }
}

export default Middle
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
import Middle from './middle.js'

class Lower extends Middle {
  report () {
    console.log(this.additional())
  }

  additional () {
    return 'lower'
  }
}

export default Lower
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
Lower in lower.js
Middle in ./middle.js
Upper in ./upper.js
\end{lstlisting}



Good: we can recover the chain of inheritance\index{chain of inheritance}.
Finding method definitions is also straightforward:


\begin{lstlisting}[frame=tblr]
import FindAncestors from './find-ancestors.js'

class FindMethods extends FindAncestors {
  find (dirname, filename, className) {
    const classes = super.find(dirname, filename, className)
    classes.forEach(record => {
      record.methods = this.findMethods(record.classDef)
    })
    return classes
  }

  findMethods (classDef) {
    return classDef.body.body
      .filter(item => item.type === 'MethodDefinition')
      .map(item => {
        if (item.kind === 'constructor') {
          return 'constructor'
        } else if (item.kind === 'method') {
          return item.key.name
        } else {
          return null
        }
      })
      .filter(item => item !== null)
  }
}

export default FindMethods
\end{lstlisting}



And finally,
we can print a \glossref{Markdown}\index{Markdown}-formatted table
showing which methods are defined in which class:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
| method | Upper | Middle | Lower |
| ---- | ---- | ---- | ---- |
| additional | . | . | X |
| constructor | X | X | . |
| modify | X | X | . |
| report | X | . | X |
\end{lstlisting}



\noindent which renders as:


\vspace{\baselineskip}
\begin{tabular}{llll}
\textbf{\underline{method}} & \textbf{\underline{Upper}} & \textbf{\underline{Middle}} & \textbf{\underline{Lower}} \\
additional & . & . & X \\
constructor & X & X & . \\
modify & X & X & . \\
report & X & . & X \\
\end{tabular}

\vspace{\baselineskip}


This may seem rather pointless for our toy example,
but it proves its worth when we are looking at something like
the virtual machine we will build in \chapref{virtual-machine},
which has a more complex method definition table:


\vspace{\baselineskip}
\begin{tabular}{lllll}
\textbf{\underline{method}} & \textbf{\underline{DebuggerBase}} & \textbf{\underline{DebuggerInteractive}} & \textbf{\underline{DebuggerTest}} & \textbf{\underline{DebuggerExit}} \\
clear & . & X & . & . \\
constructor & X & X & X & . \\
exit & . & X & . & X \\
getCommand & . & X & . & . \\
handle & . & X & . & . \\
help & . & X & . & . \\
input & . & X & X & . \\
interact & . & X & . & . \\
list & . & X & . & . \\
message & X & . & X & . \\
next & . & X & . & . \\
print & . & X & . & . \\
run & . & X & . & . \\
setTester & . & . & X & . \\
setVM & X & . & . & . \\
stop & . & X & . & . \\
variables & . & X & . & . \\
\end{tabular}

\vspace{\baselineskip}

\section{Exercises}\label{style-checker-exercises}

\subsection*{Function length}


Derive a class from \texttt{Walker} that reports the length in lines of each function defined in the code being checked.

\subsection*{Expression depth}


Derive a class from \texttt{Walker} that reports how deep each top-level expression in the source code is.
For example,
the depth of \texttt{1 + 2 * 3} is 2,
while the depth of \texttt{max(1 + 2 + 3)} is 3
(one level for the function call, one for the first addition, and one for the nested addition).

\subsection*{Downward and upward}


Modify \texttt{Walker} so that users can specify
one action to take at a node on the way down the tree
and a separate action to take on the way up.
(Hint: require users to specify \texttt{Nodename\_downward} and/or \texttt{Nodename\_upward} methods in their class,
then use string concatenation to construct method names while traversing the tree.)

\subsection*{Aggregating across files}


Create a command-line program called \texttt{sniff.js}
that checks for style violations in any number of source files.
The first command-line argument to \texttt{sniff.js} must be a JavaScript source file
that exports a class derived from \texttt{Walker} called \texttt{Check}
that implements the checks the user wants.
The other command-line arguments must be the names of JavaScript source files to be checked:


\begin{lstlisting}[frame=shadowbox]
node sniff.js my-check.js source-1.js source-2.js
\end{lstlisting}


\subsection*{Finding assertions}


Write a program \texttt{find-assertions.js} that finds all calls to \texttt{assert} or \texttt{assert.something}
and prints the assertion message (if any).

\subsection*{Finding a missing parameter}

\begin{enumerate}

\item 

Why doesn’t the parameter \texttt{x} show up as a rule violation
    in the example where we check name lengths?



\item 

Modify the example so that it does.



\end{enumerate}

\subsection*{Finding nested indexes}


Write a tool that finds places where nested indexing is used,
i.e.,
where the program contains expression like \texttt{arr[table[i]]}.

\subsection*{Dynamic lookup}

\begin{enumerate}

\item 

Write a function \texttt{dynamicExecution} that takes an object,
    the name of a method,
    and zero or more parameters as arguments
    and calls that method on that object:

\begin{lstlisting}[frame=tblr]
dynamicExecution(obj, 'meth', 1, 'a')
// same as obj.meth(1, 'a')
\end{lstlisting}



\item 

What \emph{doesn’t} this work for?



\end{enumerate}

\subsection*{Generators and arrays}

\begin{enumerate}

\item 

Write a generator that takes a two-dimensional table represented as an array of arrays
    and returns the values in \glossref{column-major} order.



\item 

Write another generator that takes a similar table
    and returns the values in \glossref{row-major} order.



\end{enumerate}

\subsection*{Generators and identifiers}


Rewrite the tool to check identifier lengths using a generator.

\chapter{Code Generator}\label{code-generator}


\noindent 
  Terms defined: \glossref{byte code}, \glossref{code coverage (in testing)}, \glossref{compiler}, \glossref{Decorator pattern}, \glossref{macro}, \glossref{nested function}, \glossref{two hard problems in computer science}



We’ve been writing tests since \chapref{unit-test},
but how much of our code do they actually check?
One way to find out is to use a \glossref{code coverage}\index{code coverage} tool
like \hreffoot{Istanbul}{https://istanbul.js.org/}\index{Istanbul}
that watches a program while it executes
and keeps track of which lines have run and which haven’t.
Making sure that each line is tested at least once doesn’t guarantee that the code is bug-free,
but any code that \emph{isn’t} run shouldn’t be trusted.


Our code coverage tool will keep track of which functions have and haven’t been called.
Rather than rewriting \hreffoot{Node}{https://nodejs.org/en/} to keep track of this for us,
we will modify the functions themselves
by parsing the code with \hreffoot{Acorn}{https://github.com/acornjs/acorn}\index{Acorn},
inserting the instructions we need into the AST\index{abstract syntax tree},
and then turning the AST back into code.

\begin{callout}


\subsubsection*{Simple usually isn’t}


At first glance it would be a lot simpler
to use regular expressions to find every line that looks like the start of a function definition
and insert a line right after each one
to record the information we want.
Of course,
some people split function headers across several lines if they have lots of parameters,
and there might be things that look like function definitions embedded in comments or strings.
It doesn’t take long before our simple solution turns into
a poorly-implemented parser for a subset of JavaScript that no-one else understands.
Using a full-blown parser and working with the AST is almost always less work.

\end{callout}

\section{How can we replace a function with another function?}\label{code-generator-replace}


The first thing we need is a way to wrap up an arbitrary function call.
If we declare a function in JavaScript with a parameter like \texttt{...args},
all of the “extra” arguments in the call that don’t line up with regular parameters
are stuffed into the variable \texttt{args}
(\figref{code-generator-spread}).
We can also call a function by putting values in a variable
and using \texttt{func(...var)} to spread\index{spread!function arguments} those values out.
There’s nothing special about the names \texttt{args} and \texttt{vars}:
what matters is the ellipsis \texttt{...}

\figpdf{code-generator-spread}{./code-generator/spread.pdf}{Using ...args to capture and spread parameters.}{0.6}


We can use \texttt{...args} to capture all of the arguments to a function call
and forward them to another function.
Let’s start by creating functions with a varying number of parameters
that run to completion or throw an exception,
then run them to make sure they do what we want:


\newpage


\begin{lstlisting}[frame=tblr]
let zero = () => console.log('zero')
let one = (first) => console.log(`one(${first})`)
let two = (first, second) => console.log(`two(${first}, ${second})`)

let error = () => {
  console.log('error')
  throw new Error('from error')
  console.log('should not reach this')
}

const runAll = (title) => {
  console.log(title)
  zero()
  one(1)
  two(1, 2)
  try {
    error()
  } catch (error) {
    console.log(`caught ${error} as expected`)
  }
  console.log()
}

runAll('first time')
\end{lstlisting}



We can now write a function that takes a function as an input
and creates a new function that handles all of the errors in the original function:


\begin{lstlisting}[frame=tblr]
const replace = (func) => {
  return (...args) => {
    console.log('before')
    try {
      const result = func(...args)
      console.log('after')
      return result
    } catch (error) {
      console.log('error')
      throw error
    }
  }
}

zero = replace(zero)
one = replace(one)
two = replace(two)
error = replace(error)

runAll('second time')
\end{lstlisting}



Let’s try it out:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
first time
zero
one(1)
two(1, 2)
error
caught Error: from error as expected

second time
before
zero
after
before
one(1)
after
before
two(1, 2)
after
before
error
error
caught Error: from error as expected
\end{lstlisting}



This is an example of the \glossref{Decorator}\index{Decorator pattern}\index{design pattern!Decorator} design pattern.
A decorator is a function whose job is to modify the behavior of other functions
in some general ways.
Decorators are built in to some languages (like \hreffoot{Python}{https://www.python.org/}\index{Python}),
and we can add them in most others as we have done here.

\section{How can we generate JavaScript?}\label{code-generator-generate}


We could use a decorator to replace every function in our program
with one that keeps track of whether or not it was called,
but it would be tedious to apply the decorator to every one of our functions by hand.
What we really want is a way to do this automatically for everything,
and for that we need to parse and generate code.

\begin{callout}


\subsubsection*{Other ways to do it}


A third way to achieve what we want is
to let the system turn code into runnable instructions
and then modify those instructions.
This approach is often used in compiled languages like \hreffoot{Java}{https://en.wikipedia.org/wiki/Java\_(programming\_language)}\index{Java},
where the \glossref{byte code} produced by the \glossref{compiler} is saved in files
in order to be run.
We can’t do this here because Node compiles and runs code in a single step.

\end{callout}


Our tool will parse the JavaScript with Acorn to create an AST,
modify the AST,
and then use a library called \hreffoot{Escodegen}{https://github.com/estools/escodegen/}\index{Escodegen} to turn the AST back into JavaScript.
To start,
let’s look at the AST for a simple function definition,
which is 75 lines of pretty-printed JSON:


\begin{lstlisting}[frame=tblr]
import acorn from 'acorn'

const text = `const func = (param) => {
  return param + 1
}`

const ast = acorn.parse(text, { sourceType: 'module' })
console.log(JSON.stringify(ast, null, 2))
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{
  "type": "Program",
  "start": 0,
  "end": 46,
  "body": [
    {
      "type": "VariableDeclaration",
      "start": 0,
      "end": 46,
      "declarations": [
        {
          "type": "VariableDeclarator",
          "start": 6,
          "end": 46,
          "id": {
            "type": "Identifier",
            "start": 6,
            "end": 10,
            "name": "func"
          },
          "init": {
            "type": "ArrowFunctionExpression",
            "start": 13,
            "end": 46,
            "id": null,
            "expression": false,
            "generator": false,
            "async": false,
            "params": [
              {
                "type": "Identifier",
                "start": 14,
                "end": 19,
                "name": "param"
              }
            ],
            "body": {
              "type": "BlockStatement",
              "start": 24,
              "end": 46,
              "body": [
                {
                  "type": "ReturnStatement",
                  "start": 28,
                  "end": 44,
                  "argument": {
                    "type": "BinaryExpression",
                    "start": 35,
                    "end": 44,
                    "left": {
                      "type": "Identifier",
                      "start": 35,
                      "end": 40,
                      "name": "param"
                    },
                    "operator": "+",
                    "right": {
                      "type": "Literal",
                      "start": 43,
                      "end": 44,
                      "value": 1,
                      "raw": "1"
                    }
                  }
                }
              ]
            }
          }
        }
      ],
      "kind": "const"
    }
  ],
  "sourceType": "module"
}
\end{lstlisting}



After inspecting a few nodes,
we can try to create some of our own and turn them into code.
Here,
for example,
we have the JSON representation of the expression \texttt{40+2}:


\begin{lstlisting}[frame=tblr]
import escodegen from 'escodegen'

const result = escodegen.generate({
  type: 'BinaryExpression',
  operator: '+',
  left: { type: 'Literal', value: 40 },
  right: { type: 'Literal', value: 2 }
})
console.log(result)
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
40 + 2
\end{lstlisting}


\section{How can we count how often functions are executed?}\label{code-generator-count}


Our tool will find all the function declaration nodes in the program
and insert a node to increment an entry in a global variable called \texttt{\_\_counters}.
(Prefixing the name with two underscores doesn’t guarantee that
we won’t accidentally clobber a variable in the user’s program with the same name,
but hopefully it makes that less likely.)
Our test case is:


\begin{lstlisting}[frame=tblr]
const TEXT = `
const funcOuter = (param) => {
  return param + 1
}
const funcInner = (param) => {
  return param + 1
}
for (const i of [1, 3, 5]) {
  funcOuter(funcInner(i) + funcInner(i))
}
`
\end{lstlisting}



\noindent and the main function of our program is:


\begin{lstlisting}[frame=tblr]
const main = () => {
  const ast = acorn.parse(TEXT, { sourceType: 'module' })

  const allNodes = []
  walk.simple(ast, {
    VariableDeclarator: (node, state) => {
      if (node.init && (node.init.type === 'ArrowFunctionExpression')) {
        state.push(node)
      }
    }
  }, null, allNodes)

  const names = {}
  allNodes.forEach(node => insertCounter(names, node))
  console.log(initializeCounters(names))
  console.log(escodegen.generate(ast))
  console.log(reportCounters())
}
\end{lstlisting}



To insert a count we call \texttt{insertCounter}
to record the function’s name and modify the node:


\begin{lstlisting}[frame=tblr]
const insertCounter = (names, node) => {
  const name = node.id.name
  names[name] = 0

  const body = node.init.body.body
  const increment =
    acorn.parse(`__counters['${name}'] += 1`, { sourceType: 'module' })
  body.unshift(increment)
}
\end{lstlisting}



\noindent Notice how we don’t try to build the nodes by hand,
but instead construct the string we need,
use Acorn\index{Acorn} to parse that,
and use the result.
Doing this saves us from embedding multiple lines of JSON in our program
and also ensures that if a newer version of Acorn decides to generate a different AST,
our program will do the right thing automatically.


Finally,
we need to add a couple of helper functions\index{helper function}:


\begin{lstlisting}[frame=tblr]
const initializeCounters = (names) => {
  const body = Object.keys(names).map(n => `'${n}': 0`).join(',\n')
  return 'const __counters = {\n' + body + '\n}'
}

const reportCounters = () => {
  return 'console.log(__counters)'
}
\end{lstlisting}



\noindent and run it to make sure it all works:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
const __counters = {
'funcOuter': 0,
'funcInner': 0
}
const funcOuter = param => {
        __counters['funcOuter'] += 1;
    return param + 1;
};
const funcInner = param => {
        __counters['funcInner'] += 1;
    return param + 1;
};
for (const i of [
        1,
        3,
        5
    ]) {
    funcOuter(funcInner(i) + funcInner(i));
}
console.log(__counters)
\end{lstlisting}


\begin{callout}


\subsubsection*{Too simple to be safe}


Our simple approach to naming counters doesn’t work if functions can have the same names,
which they can if we use modules or \glossref{nested functions}\index{nested function}\index{function!nested}.
One way to solve this would be to manufacture a label from the function’s name
and the line number in the source code;
another would be to keep track of which functions are nested within which
and concatenate their names to produce a unique key.
Problems like this are why people say that naming things
is one of the \glossref{two hard problems}\index{two hard problems in computer science} in computer science.

\end{callout}

\section{How can we time function execution?}\label{code-generator-time}


Now that we have a way to insert code into functions
we can use it to do many other things.
For example,
we can find out how long it takes functions to run
by wrapping them up in code that records the start and end time of each call.
As before,
we find the nodes of interest and decorate them,
then stitch the result together with a bit of bookkeeping:


\begin{lstlisting}[frame=tblr]
const timeFunc = (text) => {
  const ast = acorn.parse(text, { sourceType: 'module' })
  const allNodes = gatherNodes(ast)
  allNodes.forEach(node => wrapFuncDef(node))
  return [
    initializeCounters(allNodes),
    escodegen.generate(ast),
    reportCounters()
  ].join('\n')
}
\end{lstlisting}



Gathering nodes is straightforward:


\begin{lstlisting}[frame=tblr]
const gatherNodes = (ast) => {
  const allNodes = []
  walk.simple(ast, {
    VariableDeclarator: (node, state) => {
      if (node.init && (node.init.type === 'ArrowFunctionExpression')) {
        state.push(node)
      }
    }
  }, null, allNodes)
  return allNodes
}
\end{lstlisting}



\noindent as is wrapping the function definition:


\begin{lstlisting}[frame=tblr]
const wrapFuncDef = (originalAst) => {
  const name = originalAst.id.name
  const wrapperAst = makeWrapperAst(name)
  wrapperAst.init.body.body[0].declarations[0].init = originalAst.init
  originalAst.init = wrapperAst.init
}
\end{lstlisting}



The only big difference is how we make the wrapper function.
We create it with a placeholder for the original function
so that we have a spot in the AST to insert the actual code:


\begin{lstlisting}[frame=tblr]
const timeFunc = (text) => {
  const ast = acorn.parse(text, { sourceType: 'module' })
  const allNodes = gatherNodes(ast)
  allNodes.forEach(node => wrapFuncDef(node))
  return [
    initializeCounters(allNodes),
    escodegen.generate(ast),
    reportCounters()
  ].join('\n')
}
\end{lstlisting}



Let’s run one last test:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
const __counters = {
'assignment': 0,
'readFile': 0
}
const assignment = (...originalArgs) => {
    const originalFunc = range => {
        let j = 0;
        for (let i = 0; i < range; i += 1) {
            j = i;
        }
    };
    const startTime = Date.now();
    try {
        const result = originalFunc(...originalArgs);
        const endTime = Date.now();
        __counters['assignment'] += endTime - startTime;
        return result;
    } catch (error) {
        const endTime = Date.now();
        __counters['assignment'] += endTime - startTime;
        throw error;
    }
};
const readFile = (...originalArgs) => {
    const originalFunc = (range, filename) => {
        for (let i = 0; i < range; i += 1) {
            fs.readFileSync(filename, 'utf-8');
        }
    };
    const startTime = Date.now();
    try {
        const result = originalFunc(...originalArgs);
        const endTime = Date.now();
        __counters['readFile'] += endTime - startTime;
        return result;
    } catch (error) {
        const endTime = Date.now();
        __counters['readFile'] += endTime - startTime;
        throw error;
    }
};
const numLoops = 100000;
assignment(numLoops);
readFile(numLoops, 'index.md');
console.log(__counters)
OUTPUT
{ assignment: 1, readFile: 3879 }
\end{lstlisting}



Source-to-source translation is widely used in JavaScript:
tools like \hreffoot{Babel}{https://babeljs.io/}\index{Babel} use it to transform modern features like \texttt{async} and \texttt{await}
(\chapref{async-programming})
into code that older browsers can understand.
The technique is so powerful that it is built into languages like Scheme,
which allow programmers to add new syntax to the language
by defining \glossref{macros}\index{macro}.
Depending on how carefully they are used,
macros can make programs elegant, incomprehensible, or both.

\section{Exercises}\label{code-generator-exercises}

\subsection*{JSON to JavaScript}


Write a tool that uses \hreffoot{Escodegen}{https://github.com/estools/escodegen/}
to translate simple expressions written in JSON into runnable JavaScript.
For example, the tool should translate:

\begin{lstlisting}[frame=tblr]
['+', 3, ['*', 5, 'a']]
\end{lstlisting}


\noindent into:

\begin{lstlisting}[frame=tblr]
3 + (5 * a)
\end{lstlisting}

\subsection*{JavaScript to HTML}


Write a function that takes nested JavaScript function calls for generating HTML like this:

\begin{lstlisting}[frame=tblr]
div(h1('title'), p('explanation'))
\end{lstlisting}


\noindent and turns them into HTML like this:

\begin{lstlisting}[frame=tblr]
<div><h1>title</h1><p>explanation</p></div>
\end{lstlisting}

\subsection*{Handling modules}


Modify the code that counts the number of times a function is called
to handle functions with the same name from different modules.

\subsection*{Tracking calls}


Write a decorator that takes a function as its argument
and returns a new function that behaves exactly the same way
except that it keeps track of who called it.

\begin{enumerate}

\item 

The program contains a stack where decorated functions push and pop their names
    as they are called and as they exit.



\item 

Each time a function is called
    it adds a record to an array to record its name and the name at the top of the stack
    (i.e., the most-recently-called decorated function).



\end{enumerate}

\subsection*{Counting classical function definitions}


Modify the code generator to handle functions declared with the \texttt{function} keyword
as well as functions declared using \texttt{=>}.

\subsection*{Recording input file size}

\begin{enumerate}

\item 

Write a program that replaces all calls to \texttt{fs.readFileSync}
    with calls to \texttt{readFileSyncCount}.



\item 

Write the function \texttt{readFileSyncCount} to read and return a file using \texttt{fs.readFileSync}
    but to also record the file’s name and size in bytes.



\item 

Write a third function \texttt{reportInputFileSizes} that reports
    what files were read and how large they were.



\item 

Write tests for these functions using Mocha and \texttt{mock-fs}.



\end{enumerate}

\subsection*{Checking argument types}


Write a tool that modifies functions to check the types of their arguments at run-time.

\begin{enumerate}

\item 

Each function is replaced by a function that passes all of its arguments to \texttt{checkArgs}
    along with the function’s name,
    then continues with the function’s original operation.



\item 

The first time \texttt{checkArgs} is called for a particular function
    it records the actual types of the arguments.



\item 

On subsequent calls, it checks that the argument types match those of the first call
    and throws an exception if they do not.



\end{enumerate}

\subsection*{Two-dimensional arrays}


The function \texttt{make2D} takes a row length and one or more values
and creates a two-dimensional array from those values:

\begin{lstlisting}[frame=tblr]
make2D(2, 'a', 'b', 'c', 'd')
// produces [['a', 'b'], ['c', 'd']]
\end{lstlisting}


\noindent Write a function that searches code to find calls to \texttt{make2D}
and replaces them with inline arrays-of-arrays.
This function only has to work for calls with a fixed row length,
i.e., it does \emph{not} have to handle \texttt{make2D(N, {\textquotesingle}a{\textquotesingle}, {\textquotesingle}b{\textquotesingle})}.

\subsection*{From require to import}


Write a function that searches code for simple calls to \texttt{require}
and replaces them with calls to \texttt{import}.
This function only needs to work for the simplest case;
for example, if the input is:

\begin{lstlisting}[frame=tblr]
const name = require('module')
\end{lstlisting}


\noindent then the output is:

\begin{lstlisting}[frame=tblr]
import name from 'module'
\end{lstlisting}

\subsection*{Removing empty constructors}


Write a function that removes empty constructors from class definitions.
For example, if the input is:

\begin{lstlisting}[frame=tblr]
class Example {
  constructor () {
  }

  someMethod () {
    console.log('some method')
  }
}
\end{lstlisting}


\noindent then the output should be:

\begin{lstlisting}[frame=tblr]
class Example {
  someMethod () {
    console.log('some method')
  }
}
\end{lstlisting}

\chapter{Documentation Generator}\label{doc-generator}


\noindent 
  Terms defined: \glossref{accumulator}, \glossref{block comment}, \glossref{deprecation}, \glossref{doc comment}, \glossref{line comment}, \glossref{slug}



Many programmers believe they’re more likely to write documentation and keep it up to date
if it is close to the code.
Tools that extract specially-formatted comments from code and turn them into documentation
have been around since at least the 1980s;
many are used for JavaScript,
including \hreffoot{JSDoc}{https://jsdoc.app/}\index{JSDoc} and \hreffoot{ESDoc}{https://esdoc.org/}\index{ESDoc}.
This chapter will use what we learned in \chapref{code-generator} about parsing source code
to build a simple documentation generator of our own.

\section{How can we extract documentation comments?}\label{doc-generator-extract}


We will use \hreffoot{Acorn}{https://github.com/acornjs/acorn}\index{Acorn} once again to parse our source files.
This time we will use the parser’s \texttt{onComment} option,
giving it an array to fill in.
For the moment we won’t bother to assign the AST\index{abstract syntax tree} produced by parsing to a variable
because we are just interested in the comments:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'
import acorn from 'acorn'

const text = fs.readFileSync(process.argv[2], 'utf-8')
const options = {
  sourceType: 'module',
  locations: true,
  onComment: []
}
acorn.parse(text, options)
console.log(JSON.stringify(options.onComment, null, 2))
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
// double-slash comment
/* slash-star comment */
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  {
    "type": "Line",
    "value": " double-slash comment",
    "start": 0,
    "end": 23,
    "loc": {
      "start": {
        "line": 1,
        "column": 0
      },
      "end": {
        "line": 1,
        "column": 23
      }
    }
  },
  {
    "type": "Block",
    "value": " slash-star comment ",
    "start": 24,
    "end": 48,
    "loc": {
      "start": {
        "line": 2,
        "column": 0
      },
      "end": {
        "line": 2,
        "column": 24
      }
    }
  }
]
\end{lstlisting}



There is more information here than we need,
so let’s slim down the JSON that we extract:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'
import acorn from 'acorn'

const text = fs.readFileSync(process.argv[2], 'utf-8')
const options = {
  sourceType: 'module',
  locations: true,
  onComment: []
}
acorn.parse(text, options)
const subset = options.onComment.map(entry => {
  return {
    type: entry.type,
    value: entry.value,
    start: entry.loc.start.line,
    end: entry.loc.end.line
  }
})
console.log(JSON.stringify(subset, null, 2))
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node extract-comments-subset.js two-kinds-of-comment.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  {
    "type": "Line",
    "value": " double-slash comment",
    "start": 1,
    "end": 1
  },
  {
    "type": "Block",
    "value": " slash-star comment ",
    "start": 2,
    "end": 2
  }
]
\end{lstlisting}


\figpdfhere{doc-generator-comments}{./doc-generator/comments.pdf}{How line comments and block comments are distinguished and represented.}{0.6}


Acorn distinguishes two kinds of comments (\figref{doc-generator-comments}).
\glossref{Line comments}\index{line comment}\index{comment!line} cannot span multiple lines;
if one line comment occurs immediately after another,
Acorn reports two comments:


\begin{lstlisting}[frame=tblr]
//
// multi-line double-slash comment
//
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node extract-comments-subset.js multi-line-double-slash-comment.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  {
    "type": "Line",
    "value": "",
    "start": 1,
    "end": 1
  },
  {
    "type": "Line",
    "value": " multi-line double-slash comment",
    "start": 2,
    "end": 2
  },
  {
    "type": "Line",
    "value": "",
    "start": 3,
    "end": 3
  }
]
\end{lstlisting}



\glossref{Block comments}\index{block comment}\index{comment!block},
on the other hand,
can span any number of lines.
We don’t need to prefix each line with \texttt{*} but most people do for readability:


\begin{lstlisting}[frame=tblr]
/*
 * multi-line slash-star comment
 */
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node extract-comments-subset.js multi-line-slash-star-comment.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  {
    "type": "Block",
    "value": "\n * multi-line slash-star comment\n ",
    "start": 1,
    "end": 3
  }
]
\end{lstlisting}



By convention,
we use block comments that start with \texttt{/**} for documentation.
The first two characters are recognized by the parser as “start of comment”,
so the first character in the extracted text is \texttt{*}:


\begin{lstlisting}[frame=tblr]
/**
 * doc comment
 */
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  {
    "type": "Block",
    "value": "*\n * doc comment\n ",
    "start": 1,
    "end": 3
  }
]
\end{lstlisting}


\section{What input will we try to handle?}\label{doc-generator-input}


We will use \hreffoot{Markdown}{https://en.wikipedia.org/wiki/Markdown}\index{Markdown} for formatting our documentation.
The \glossref{doc comments}\index{doc comment}\index{comment!doc} for function definitions look like this:


\begin{lstlisting}[frame=tblr]
/**
 * # Demonstrate documentation generator.
 */

import util from './util-plain'

/**
 * ## `main`: Main driver.
 */
const main = () => {
  // Parse arguments.
  // Process input stream.
}

/**
 * ## `parseArgs`: Parse command line.
 * - `args` (`string[]`): arguments to parse.
 * - `defaults` (`Object`): default values.
 *
 * Returns: program configuration object.
 */
const parseArgs = (args, defaults) => {
  // body would go here
}

/**
 * ## `process`: Transform data.
 * - `input` (`stream`): where to read.
 * - `output` (`stream`): where to write.
 * - `op` (`class`): what to do.
 *    Use @BaseProcessor unless told otherwise.
 */
const process = (input, output, op = util.BaseProcessor) => {
  // body would go here
}
\end{lstlisting}



\noindent while the ones for class definitions look like this:


\begin{lstlisting}[frame=tblr]
/**
 * # Utilities to demonstrate doc generator.
 */

/**
 * ## `BaseProcessor`: General outline.
 */
class BaseProcessor {
  /**
   * ### `constructor`: Build processor.
   */
  constructor () {
    // body would go here
  }

  /**
   * ### `run`: Pass input to output.
   * - `input` (`stream`): where to read.
   * - `output` (`stream`): where to write.
   */
  run (input, output) {
    // body would go here
  }
}

export default BaseProcessor
\end{lstlisting}



The doc comments are unpleasant at the moment:
they repeat the function and method names from the code,
we have to create titles ourselves,
and we have to remember the back-quotes for formatting code.
We will fix some of these problems once we have a basic tool up and running.


The next step in doing that is to translate Markdown into HTML.
There are many Markdown parsers\index{Markdown!parser} in JavaScript;
after experimenting with a few,
we decided to use \hreffoot{\texttt{markdown-it}}{https://markdown-it.github.io/}
along with the \hreffoot{\texttt{markdown-it-anchor}}{https://www.npmjs.com/package/markdown-it-anchor} extension
that creates HTML anchors for headings.
The main program gets all the doc comments from all of the input files,
converts the Markdown to HTML,
and displays that:


\begin{lstlisting}[frame=tblr]
const STYLE = 'width: 40rem; padding-left: 0.5rem; border: solid;'
const HEAD = `<html><body style="${STYLE}">`
const FOOT = '</body></html>'

const main = () => {
  const allComments = getAllComments(process.argv.slice(2))
  const md = new MarkdownIt({ html: true })
    .use(MarkdownAnchor, { level: 1, slugify: slugify })
  const html = md.render(allComments)
  console.log(HEAD)
  console.log(html)
  console.log(FOOT)
}
\end{lstlisting}



To get all the comments,
we extract comments from all the files,
remove the leading \texttt{*} characters (which aren’t part of the documentation),
and then join the results after stripping off extraneous blanks:


\begin{lstlisting}[frame=tblr]
const getAllComments = (allFilenames) => {
  return allFilenames
    .map(filename => {
      const comments = extractComments(filename)
      return { filename, comments }
    })
    .map(({ filename, comments }) => {
      comments = comments.map(comment => removePrefix(comment))
      return { filename, comments }
    })
    .map(({ filename, comments }) => {
      const combined = comments
        .map(comment => comment.stripped)
        .join('\n\n')
      return `# ${filename}\n\n${combined}`
    })
    .join('\n\n')
}
\end{lstlisting}



Extracting the comments from a single file is done as before:


\begin{lstlisting}[frame=tblr]
const extractComments = (filename) => {
  const text = fs.readFileSync(filename, 'utf-8')
  const options = {
    sourceType: 'module',
    locations: true,
    onComment: []
  }
  acorn.parse(text, options)
  const subset = options.onComment
    .filter(entry => entry.type === 'Block')
    .map(entry => {
      return {
        type: entry.type,
        value: entry.value,
        start: entry.start,
        end: entry.end
      }
    })
  return subset
}
\end{lstlisting}



\noindent and removing the prefix \texttt{*} characters is a matter of splitting the text into lines,
removing the leading spaces and asterisks,
and putting the lines back together:


\begin{lstlisting}[frame=tblr]
const removePrefix = (comment) => {
  comment.stripped = comment.value
    .split('\n')
    .slice(0, -1)
    .map(line => line.replace(/^ *\/?\* */, ''))
    .map(line => line.replace('*/', ''))
    .join('\n')
    .trim()
  return comment
}
\end{lstlisting}



One thing that isn’t in this file (because we’re going to use it in later versions)
is the function \texttt{slugify}.
A \glossref{slug}\index{slug (unique identifier)} is a short string that identifies a header or a web page;
the name comes from the era of newspapers,
where a slug was a short name used to identify an article while it was in production.
Our \texttt{slugify} function strips unnecessary characters out of a title,
adds hyphens,
and generally makes it something you might see in a URL:


\begin{lstlisting}[frame=tblr]
const slugify = (text) => {
  return encodeURIComponent(
    text.split(' ')[0]
      .replace(/.js$/, '')
      .trim()
      .toLowerCase()
      .replace(/[^ \w]/g, '')
      .replace(/\s+/g, '-')
  )
}

export default slugify
\end{lstlisting}



Let’s run this generator and see what it produces
(\figref{doc-generator-process-plain} and \figref{doc-generator-mapping}):


\begin{lstlisting}[frame=shadowbox]
node process-plain.js example-plain.js util-plain.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html><body style="width: 40rem; padding-left: 0.5rem; border: solid;">
<h1 id="exampleplain">example-plain.js</h1>
<h1 id="demonstrate">Demonstrate documentation generator.</h1>
<h2 id="main"><code>main</code>: Main driver.</h2>
<h2 id="parseargs"><code>parseArgs</code>: Parse command line.</h2>
<ul>
<li><code>args</code> (<code>string[]</code>): arguments to parse.</li>
<li><code>defaults</code> (<code>Object</code>): default values.</li>
</ul>
<p>Returns: program configuration object.</p>
<h2 id="process"><code>process</code>: Transform data.</h2>
<ul>
<li><code>input</code> (<code>stream</code>): where to read.</li>
<li><code>output</code> (<code>stream</code>): where to write.</li>
<li><code>op</code> (<code>class</code>): what to do.
Use @BaseProcessor unless told otherwise.</li>
</ul>
<h1 id="utilplain">util-plain.js</h1>
<h1 id="utilities">Utilities to demonstrate doc generator.</h1>
<h2 id="baseprocessor"><code>BaseProcessor</code>: General outline.</h2>
<h3 id="constructor"><code>constructor</code>: Build processor.</h3>
<h3 id="run"><code>run</code>: Pass input to output.</h3>
<ul>
<li><code>input</code> (<code>stream</code>): where to read.</li>
<li><code>output</code> (<code>stream</code>): where to write.</li>
</ul>

</body></html>
\end{lstlisting}


\figpdf{doc-generator-mapping}{./doc-generator/mapping.pdf}{How comments in code map to documentation in HTML.}{0.6}

\figpdf{doc-generator-process-plain}{./doc-generator/process-plain.pdf}{The page produced by the documentation generator.}{0.4}


It works,
but there is a double \texttt{h1} header for each file (the filename and the title comment),
the anchor IDs are hard to read,
there are no cross-references,
and so on.
Some of the visual issues can be resolved with CSS\index{CSS},
and we can change our input format to make processing easier
as long as it also makes authoring easier.
However,
anything that is written twice will eventually be wrong in one place or another,
so our first priority is to remove duplication.

\section{How can we avoid duplicating names?}\label{doc-generator-dup}


If a comment is the first thing in a file,
we want to use it as title text;
this will save us having to write an explicit level-1 title in a comment.
For each other comment,
we can extract the name of the function or method
from the node on the line immediately following the doc comment.
This allows us to write much tidier comments:


\begin{lstlisting}[frame=tblr]
/**
 * Overall file header.
 */

/**
 * Double the input.
 */
const double = (x) => 2 * x

/**
 * Triple the input.
 */
function triple (x) {
  return 3 * x
}

/**
 * Define a class.
 */
class Example {
  /**
   * Method description.
   */
  someMethod () {
  }
}
\end{lstlisting}



To extract and display information from nodes immediately following doc comments
we must find all the block comments,
record the last line of each,
and then search the AST to find nodes that are on lines
immediately following any of those trailing comment lines.
(We will assume for now that there are no blank lines between the comment
and the start of the class or function.)
The main program finds the comments as usual,
creates a set containing the line numbers we are looking for,
then searches for the nodes we want:


\begin{lstlisting}[frame=tblr]
const main = () => {
  const options = {
    sourceType: 'module',
    locations: true,
    onComment: []
  }
  const text = fs.readFileSync(process.argv[2], 'utf-8')
  const ast = acorn.parse(text, options)
  const comments = options.onComment
    .filter(entry => entry.type === 'Block')
    .map(entry => {
      return {
        value: entry.value,
        start: entry.loc.start.line,
        end: entry.loc.end.line
      }
    })
  const targets = new Set(comments.map(comment => comment.end + 1))
  const nodes = []
  findFollowing(ast, targets, nodes)
  console.log(nodes.map(node => condense(node)))
}
\end{lstlisting}



The recursive search is straightforward as well—we delete line numbers from the target set
and add nodes to the \glossref{accumulator}\index{Accumulator pattern}\index{design pattern!Accumulator} as we find matches:


\begin{lstlisting}[frame=tblr]
const findFollowing = (node, targets, accum) => {
  if ((!node) || (typeof node !== 'object') || (!('type' in node))) {
    return
  }

  if (targets.has(node.loc.start.line)) {
    accum.push(node)
    targets.delete(node.loc.start.line)
  }

  for (const key in node) {
    if (Array.isArray(node[key])) {
      node[key].forEach(child => findFollowing(child, targets, accum))
    } else if (typeof node[key] === 'object') {
      findFollowing(node[key], targets, accum)
    }
  }
}
\end{lstlisting}



Finally,
we use a function called \texttt{condense} to get the name we want out of the AST we have:


\begin{lstlisting}[frame=tblr]
const condense = (node) => {
  const result = {
    type: node.type,
    start: node.loc.start.line
  }
  switch (node.type) {
    case 'VariableDeclaration':
      result.name = node.declarations[0].id.name
      break
    case 'FunctionDeclaration':
      result.name = node.id.name
      break
    case 'ClassDeclaration':
      result.name = node.id.name
      break
    case 'MethodDefinition':
      result.name = node.key.name
      break
    default:
      assert.fail(`Unknown node type ${node.type}`)
      break
  }
  return result
}
\end{lstlisting}



\noindent We need this because we get a different structure with:

\begin{lstlisting}[frame=tblr]
const name = function() => {
}
\end{lstlisting}


\noindent than we get with:

\begin{lstlisting}[frame=tblr]
function name() {
}
\end{lstlisting}


When we run this on our test case we get:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  { type: 'VariableDeclaration', start: 8, name: 'double' },
  { type: 'FunctionDeclaration', start: 13, name: 'triple' },
  { type: 'ClassDeclaration', start: 20, name: 'Example' },
  { type: 'MethodDefinition', start: 24, name: 'someMethod' }
]
\end{lstlisting}



We can use this to create better output (\figref{doc-generator-fill-in-headers}):


\begin{lstlisting}[frame=tblr]
import MarkdownIt from 'markdown-it'
import MarkdownAnchor from 'markdown-it-anchor'

import getComments from './get-comments.js'
import getDefinitions from './get-definitions.js'
import fillIn from './fill-in.js'
import slugify from './slugify.js'

const STYLE = 'width: 40rem; padding-left: 0.5rem; border: solid;'
const HEAD = `<html><body style="${STYLE}">`
const FOOT = '</body></html>'

const main = () => {
  const filenames = process.argv.slice(2)
  const allComments = getComments(filenames)
  const allDefinitions = getDefinitions(filenames)
  const combined = []
  for (const [filename, comments] of allComments) {
    const definitions = allDefinitions.get(filename)
    const text = fillIn(filename, comments, definitions)
    combined.push(text)
  }
  const md = new MarkdownIt({ html: true })
    .use(MarkdownAnchor, { level: 1, slugify: slugify })
  const html = md.render(combined.join('\n\n'))
  console.log(HEAD)
  console.log(html)
  console.log(FOOT)
}

main()
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
<html><body style="width: 40rem; padding-left: 0.5rem; border: solid;">
<h1 id="fillinheadersinput">fill-in-headers-input.js</h1>
<p>Demonstrate documentation generator.</p>
<h2 id="main">main</h2>
<p>Main driver.</p>
<h2 id="parseargs">parseArgs</h2>
<p>Parse command-line arguments.</p>
<ul>
<li><code>args</code> (<code>string[]</code>): arguments to parse.</li>
<li><code>defaults</code> (<code>Object</code>): default values.</li>
</ul>
<blockquote>
<p>Program configuration object.</p>
</blockquote>
<h2 id="baseprocessor">BaseProcessor</h2>
<p>Default processing class.</p>
<h3 id="constructor">constructor</h3>
<p>Build base processor.</p>
<h3 id="run">run</h3>
<p>Pass input to output.</p>
<ul>
<li><code>input</code> (<code>stream</code>): where to read.</li>
<li><code>output</code> (<code>stream</code>): where to write.</li>
</ul>

</body></html>
\end{lstlisting}


\figpdf{doc-generator-fill-in-headers}{./doc-generator/fill-in-headers.pdf}{Filling in headers when generating documentation.}{0.4}

\section{Code is Data}\label{doc-generator-note}


We haven’t made this point explicitly in a while,
so we will repeat it here:
code is just another kind of data\index{code!as data},
and we can process it just like we would process any other data.
Parsing code to produce an AST is no different from parsing HTML to produce DOM;
in both cases we are simply transforming a textual representation that’s easy for people to author
into a data structure that’s easy for a program to manipulate.
Pulling things out of that data to create a report
is no different from pulling numbers out of a hospital database to report monthly vaccination rates.


Treating code as data enables us to do routine programming tasks with a single command,
which in turn gives us more time to think about the tasks that we can’t (yet) automate.
Doing this is the foundation of a tool-based approach to software engineering;
as the mathematician Alfred North Whitehead\index{Whitehead, Alfred North} once wrote,
“Civilization advances by extending the number of important operations which we can perform without thinking about them.”

\section{Exercises}\label{doc-generator-exercises}

\subsection*{Building an index}


Modify the documentation generator to produce an alphabetical index of all classes and methods found.
Index entries should be hyperlinks to the documentation for the corresponding item.

\subsection*{Documenting exceptions}


Extend the documentation generator to allow people to document the exceptions that a function throws.

\subsection*{Deprecation warning}


Add a feature to the documentation generator
to allow authors to mark functions and methods as \glossref{deprecation}
(i.e., to indicate that while they still exist,
they should not be used because they are being phased out).

\subsection*{Usage examples}


Enhance the documentation generator so that
if a horizontal rule \texttt{---} appears in a documentation comment,
the text following is typeset as usage example.
(A doc comment may contain several usage examples.)

\subsection*{Unit testing}


Write unit tests for the documentation generator using Mocha.

\subsection*{Summarizing functions}


Modify the documentation generator so that line comments inside a function that use \texttt{//*}
are formatted as a bullet list in the documentation for that function.

\subsection*{Cross referencing}


Modify the documentation generator so that
the documentation for one class or function
can include Markdown links to other classes or functions.

\subsection*{Data types}


Modify the documentation generator to allow authors to define new data types
in the same way as \hreffoot{JSDoc}{https://jsdoc.app/}.

\subsection*{Inline parameter documentation}


Some documentation generators put the documentation for a parameter
on the same line as the parameter:

\begin{lstlisting}[frame=tblr]
/**
 * Transform data.
 */
function process(
  input,  /*- {stream} where to read */
  output, /*- {stream} where to write */
  op      /*- {Operation} what to do */
){
  // body would go here
}
\end{lstlisting}


\noindent Modify the documentation generator to handle this.

\subsection*{Tests as documentation}


The \hreffoot{doctest}{https://docs.python.org/3/library/doctest.html} library for Python
allows programmers to embed unit tests as documentation in their programs.
Write a tool that:

\begin{enumerate}

\item 

Finds functions that start with a block comment.



\item 

Extracts the code and output from those blocks comments
    and turns them into assertions.



\end{enumerate}


\noindent For example, given this input:

\begin{lstlisting}[frame=tblr]
const findIncreasing = (values) => {
  /**
   * > findIncreasing([])
   * []
   * > findIncreasing([1])
   * [1]
   * > findIncreasing([1, 2])
   * [1, 2]
   * > findIncreasing([2, 1])
   * [2]
   */
}
\end{lstlisting}


\noindent the tool would produce:

\begin{lstlisting}[frame=tblr]
assert.deepStrictEqual(findIncreasing([]), [])
assert.deepStrictEqual(findIncreasing([1]), [1])
assert.deepStrictEqual(findIncreasing([1, 2]), [1, 2])
assert.deepStrictEqual(findIncreasing([2, 1]), [2])
\end{lstlisting}

\chapter{Module Bundler}\label{module-bundler}


\noindent 
  Terms defined: \glossref{entry point}, \glossref{module bundler}, \glossref{transitive closure}



JavaScript\index{JavaScript!hurried design of} was designed in a hurry 25 years ago to make web pages interactive.
Nobody realized it would become one of the most popular programming languages in the world,
so it didn’t include support for things that large programs need.
One of those things was a way to turn a set of easy-to-edit source files
into a single easy-to-load file
so that browsers could get what they needed with one request.


A \glossref{module bundler}\index{module bundler}
finds all the files that an application depends on
and combines them into a single loadable file
(\figref{module-bundler-bundling}).
This file is much more efficient to load:
it’s the same number of bytes but just one network request.
(See \tblref{systems-programming-times} for a reminder of why this is important.)
Bundling files also tests that dependencies actually resolve
so that the application has at least a chance of being able to run.

\figpdf{module-bundler-bundling}{./module-bundler/bundling.pdf}{Combining multiple modules into one.}{0.6}


Bundling requires an \glossref{entry point}\index{entry point (of module)}\index{module!entry point},
i.e.,
a place to start searching for dependencies.
Given that,
it finds all dependencies,
combines them into one file,
and ensures they can find each other correctly once loaded.
The sections below go through these steps one by one.

\section{What will we use as test cases?}\label{module-bundler-tests}


Our first test case is a single file that doesn’t require anything:


\begin{lstlisting}[frame=tblr]
const main = () => {
  console.log('in main')
}

module.exports = main
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
in main
\end{lstlisting}



\noindent For our second test,
\texttt{main.js} requires \texttt{other.js}:


\begin{lstlisting}[frame=tblr]
const other = require('./other')

const main = () => {
  console.log(other('main'))
}

module.exports = main
\end{lstlisting}



\noindent and \texttt{other.js} doesn’t require anything:


\begin{lstlisting}[frame=tblr]
const other = require('./other')

const main = () => {
  console.log(other('main'))
}

module.exports = main
\end{lstlisting}



\noindent The output we expect is:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
other called from main
\end{lstlisting}


\begin{callout}


\subsubsection*{Why \texttt{require}?}


Our tests cases use the old-style \texttt{require}\index{require vs. import}\index{import vs. require} function
and assign things that are to be visible outside the module to \texttt{module.exports}
rather than using \texttt{import} and \texttt{export}.
We tried writing the chapter using the latter,
but kept stumbling over whether we were talking about \texttt{import} in Node’s module loader
or the \texttt{import} we were building.
This kind of confusion is common when building programming tools;
we hope that splitting terminology as we have will help.

\end{callout}


Our third test case has multiple inclusions in multiple directories
and is shown in \figref{module-bundler-complicated}:

\begin{itemize}

\item \texttt{./main} requires all four of the files below.

\item \texttt{./top-left} doesn’t require anything.

\item \texttt{./top-right} requires \texttt{top-left} and \texttt{bottom-right}.

\item \texttt{./subdir/bottom-left} also requires \texttt{top-left} and \texttt{bottom-right}.

\item \texttt{./subdir/bottom-right} doesn’t require anything.

\end{itemize}

\figpdf{module-bundler-complicated}{./module-bundler/complicated.pdf}{Dependencies in large module bundler test case.}{0.6}


\noindent The main program is:


\begin{lstlisting}[frame=tblr]
// main.js

const topLeft = require('./top-left')
const topRight = require('./top-right')
const bottomLeft = require('./subdir/bottom-left')
const bottomRight = require('./subdir/bottom-right')

const main = () => {
  const functions = [topLeft, topRight, bottomLeft, bottomRight]
  functions.forEach(func => {
    console.log(`${func('main')}`)
  })
}

module.exports = main
\end{lstlisting}



\noindent and the other four files use \texttt{require} and \texttt{module.exports} to get what they need.
The output we expect is:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
topLeft from main
topRight from main with topLeft from topRight and bottomRight from \
 topRight
bottomLeft from main with topLeft from bottomLeft and bottomRight from \
 bottomLeft
bottomRight from main
\end{lstlisting}



We do not handle circular dependencies
because \texttt{require} itself doesn’t (\chapref{module-loader}).

\section{How can we find dependencies?}\label{module-bundler-find}


To get all the dependencies for one source file,
we parse it and extract all of the calls to \texttt{require}.
The code to do this is relatively straightforward given what we know about \hreffoot{Acorn}{https://github.com/acornjs/acorn}\index{Acorn}:


\begin{lstlisting}[frame=tblr]
import acorn from 'acorn'
import fs from 'fs'
import walk from 'acorn-walk'

const getRequires = (filename) => {
  const entryPointFile = filename
  const text = fs.readFileSync(entryPointFile, 'utf-8')
  const ast = acorn.parse(text)
  const requires = []
  walk.simple(ast, {
    CallExpression: (node, state) => {
      if ((node.callee.type === 'Identifier') &&
          (node.callee.name === 'require')) {
        state.push(node.arguments[0].value)
      }
    }
  }, null, requires)
  return requires
}

export default getRequires
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
import getRequires from './get-requires.js'

const result = getRequires(process.argv[2])
console.log(result)
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node test-get-requires.js simple/main.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[ './other' ]
\end{lstlisting}


\begin{callout}


\subsubsection*{An unsolvable problem}


The dependency finder shown above gives the right answer for reasonable JavaScript programs,
but not all JavaScript is reasonable.
Suppose creates an alias for \texttt{require} and uses that to load other files:

\begin{lstlisting}[frame=tblr]
const req = require
const weWillMissThis = req('./other-file')
\end{lstlisting}


We could try to trace variable assignments to catch cases like these,
but someone could still fool us by writing this:

\begin{lstlisting}[frame=tblr]
const clever = eval(`require`)
const weWillMissThisToo = clever('./other-file')
\end{lstlisting}


\emph{There is no general solution to this problem}
other than running the code to see what it does.
If you would like to understand why not,
and learn about a pivotal moment in the history of computing,
we highly recommend \cite{Petzold2008}.

\end{callout}


To get all of the dependencies a bundle needs
we need to find the \glossref{transitive closure}\index{transitive closure} of the entry point’s dependencies,
i.e.,
the requirements of the requirements and so on recursively.
Our algorithm for doing this uses two sets:
\texttt{pending},
which contains the things we haven’t looked at yet,
and \texttt{seen},
which contains the things we have
(\figref{module-bundler-transitive-closure}).
\texttt{pending} initially contains the entry point file and \texttt{seen} is initially empty.
We keep taking items from \texttt{pending} until it is empty.
If the current thing is already in \texttt{seen} we do nothing;
otherwise we get its dependencies and add them to either \texttt{seen} or \texttt{pending}.

\figpdf{module-bundler-transitive-closure}{./module-bundler/transitive-closure.pdf}{Implementing transitive closure using two sets.}{0.6}


Finding dependencies is complicated by the fact that we can load something under different names,
such as \texttt{./subdir/bottom-left} from \texttt{main} but \texttt{./bottom-left} from \texttt{./subdir/bottom-right}.
As with the module loader in \chapref{module-loader},
we use absolute paths as unique identifiers.
Our code is also complicated by the fact that JavaScript’s \texttt{Set} class doesn’t have an equivalent of \texttt{Array.pop},
so we will actually maintain the “set” of pending items as a list.
The resulting code is:


\begin{lstlisting}[frame=tblr]
import path from 'path'

import getRequires from './get-requires.js'

const transitiveClosure = (entryPointPath) => {
  const pending = [path.resolve(entryPointPath)]
  const filenames = new Set()
  while (pending.length > 0) {
    const candidate = path.resolve(pending.pop())
    if (filenames.has(candidate)) {
      continue
    }
    filenames.add(candidate)
    const candidateDir = path.dirname(candidate)
    getRequires(candidate)
      .map(raw => path.resolve(path.join(candidateDir, `${raw}.js`)))
      .filter(cooked => !filenames.has(cooked))
      .forEach(cooked => pending.push(cooked))
  }
  return [...filenames]
}

export default transitiveClosure
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
import transitiveClosure from './transitive-closure-only.js'

const result = transitiveClosure(process.argv[2])
console.log(JSON.stringify(result, null, 2))
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node test-transitive-closure-only.js full/main.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
[
  "/u/stjs/module-bundler/full/main.js",
  "/u/stjs/module-bundler/full/subdir/bottom-right.js",
  "/u/stjs/module-bundler/full/subdir/bottom-left.js",
  "/u/stjs/module-bundler/full/top-left.js",
  "/u/stjs/module-bundler/full/top-right.js"
]
\end{lstlisting}



This works,
but it isn’t keeping track of the mapping from required names within files to absolute paths,
so when one of the files in our bundle tries to access something,
we might not know what it’s after.
The fix is to modify transitive closure to construct and return a two-level structure.
The primary keys are the absolute paths to the files being required,
while sub-keys are the paths they refer to when loading things
(\figref{module-bundler-structure}).

\figpdf{module-bundler-structure}{./module-bundler/structure.pdf}{Data structure used to map names to absolute paths.}{0.6}


Adding this takes our transitive closure code from
23 lines
to 28 lines:


\begin{lstlisting}[frame=tblr]
import path from 'path'
import getRequires from './get-requires.js'

const transitiveClosure = (entryPointPath) => {
  const mapping = {}
  const pending = [path.resolve(entryPointPath)]
  const filenames = new Set()
  while (pending.length > 0) {
    const candidate = path.resolve(pending.pop())
    if (filenames.has(candidate)) {
      continue
    }
    filenames.add(candidate)
    mapping[candidate] = {}
    const candidateDir = path.dirname(candidate)
    getRequires(candidate)
      .map(raw => {
        mapping[candidate][raw] =
          path.resolve(path.join(candidateDir, `${raw}.js`))
        return mapping[candidate][raw]
      })
      .filter(cooked => cooked !== null)
      .forEach(cooked => pending.push(cooked))
  }
  return mapping
}

export default transitiveClosure
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
import transitiveClosure from './transitive-closure.js'

const result = transitiveClosure(process.argv[2])
console.log(JSON.stringify(result, null, 2))
\end{lstlisting}



\begin{lstlisting}[frame=shadowbox]
node test-transitive-closure.js full/main.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{
  "/u/stjs/module-bundler/full/main.js": {
    "./top-left": "/u/stjs/module-bundler/full/top-left.js",
    "./top-right": "/u/stjs/module-bundler/full/top-right.js",
    "./subdir/bottom-left": \
    "/u/stjs/module-bundler/full/subdir/bottom-left.js",
    "./subdir/bottom-right": \
    "/u/stjs/module-bundler/full/subdir/bottom-right.js"
  },
  "/u/stjs/module-bundler/full/subdir/bottom-right.js": {},
  "/u/stjs/module-bundler/full/subdir/bottom-left.js": {
    "../top-left": "/u/stjs/module-bundler/full/top-left.js",
    "./bottom-right": \
    "/u/stjs/module-bundler/full/subdir/bottom-right.js"
  },
  "/u/stjs/module-bundler/full/top-left.js": {},
  "/u/stjs/module-bundler/full/top-right.js": {
    "./top-left": "/u/stjs/module-bundler/full/top-left.js",
    "./subdir/bottom-right": \
    "/u/stjs/module-bundler/full/subdir/bottom-right.js"
  }
}
\end{lstlisting}



\noindent The real cost, though, is the extra complexity of the data structure:
it took a couple of tries to get it right,
and it will be harder for the next person to understand than the original.
Comprehension and maintenance would be a little easier
if we could draw diagrams directly in our source code,
but as long as we insist that our programs be stored in a punchcard-compatible format
(i.e., as lines of text),
that will remain a dream.

\section{How can we safely combine several files into one?}\label{module-bundler-combine}


We now need to combine the files we have found into one
while keeping each in its own namespace.
We do this using the same method we used in \chapref{module-loader}:
wrap the source code in an IIFE\index{immediately-invoked function expression},
giving that IIFE a \texttt{module} object to fill in
and an implementation of \texttt{require} to resolve dependencies \emph{within the bundle}.
For example, suppose we have this file:


\begin{lstlisting}[frame=tblr]
const main = () => {
  console.log('in main')
}

module.exports = main
\end{lstlisting}



\noindent The wrapped version will look like this:


\begin{lstlisting}[frame=tblr]
const wrapper = (module, require) => {
  const main = () => {
    console.log('in main')
  }

  module.exports = main
}
\end{lstlisting}



\noindent And we can test it like this:


\begin{lstlisting}[frame=tblr]
const wrapper = (module, require) => {
  const main = () => {
    console.log('in main')
  }

  module.exports = main
}

const _require = (name) => null
const temp = {}
wrapper(temp, _require)
temp.exports()
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
in main
\end{lstlisting}



We need to do this for multiple files,
so we will put these IIFEs in a lookup table
that uses the files’ absolute paths as its keys.
We will also wrap loading in a function
so that we don’t accidentally step on anyone else’s toys:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'
import path from 'path'

const HEAD = `const initialize = (creators) => {
`

const TAIL = `
}
`

const combineFiles = (allFilenames) => {
  const body = allFilenames
    .map(filename => {
      const key = path.resolve(filename)
      const source = fs.readFileSync(filename, 'utf-8')
      const func = `(module, require) => {${source}}`
      const entry = `creators.set('${key}',\n${func})`
      return `// ${key}\n${entry}\n`
    })
    .join('\n')
  const func = `${HEAD}\n${body}\n${TAIL}`
  return func
}

export default combineFiles
\end{lstlisting}



Breaking this down,
the code in \texttt{HEAD} creates a function of no arguments
while the code in \texttt{TAIL} returns the lookup table from that function.
In between,
\texttt{combineFiles} adds an entry to the lookup table for each file
(\figref{module-bundler-head-tail}).

\figpdf{module-bundler-head-tail}{./module-bundler/head-tail.pdf}{Assembling fragments and modules to create a bundle.}{0.6}


We can test that this works in our two-file case:


\begin{lstlisting}[frame=tblr]
import combineFiles from './combine-files.js'

console.log(combineFiles(process.argv.slice(2)))
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
const initialize = (creators) => {

// /u/stjs/stjs/module-bundler/simple/main.js
creators.set('/u/stjs/stjs/module-bundler/simple/main.js',
(module, require) => {const other = require('./other')

const main = () => {
  console.log(other('main'))
}

module.exports = main
})

// /u/stjs/stjs/module-bundler/simple/other.js
creators.set('/u/stjs/stjs/module-bundler/simple/other.js',
(module, require) => {const other = (caller) => {
  return `other called from ${caller}`
}

module.exports = other
})


}
\end{lstlisting}



\noindent and then load the result and call \texttt{initialize}:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
Map(2) {
  '/u/stjs/module-bundler/simple/main.js' => [Function (anonymous)],
  '/u/stjs/module-bundler/simple/other.js' => [Function (anonymous)]
}
\end{lstlisting}


\section{How can files access each other?}\label{module-bundler-access}


The code we have built so far has not created our exports;
instead,
it has build a lookup table of functions that can create what we asked for.
More specifically we have:

\begin{itemize}

\item 

a lookup table from absolute filenames
    to functions that create the exports for those modules;



\item 

a lookup table from the importer’s absolute filename
    to pairs storing the name of the required file as it was written
    and the required file’s absolute filename;
    and



\item 

an entry point.



\end{itemize}


\noindent To turn this into what we want,
we must look up the function associated with the entry point and run it,
giving it an empty module object and a \texttt{require} function that we will describe below,
then get the \texttt{exports} it has added to that module object.
Our replacement for \texttt{require} is only allowed to take one argument
(because that’s all that JavaScript’s \texttt{require} takes).
However,
it actually needs four things:
the argument to the user’s \texttt{require} call,
the absolute path of the file making the call,
and the two lookup tables described above.
Those two tables can’t be global variables because of possible name collisions:
no matter what we call them,
the user might have given a variable the same name.


As in \chapref{module-loader} we solve this problem using closures.
The result is probably the most difficult code in this book to understand
because of its many levels of abstraction.
First, we write a function that takes the two tables as arguments
and returns a function that takes an absolute path identifying this module.
When that function is called,
it creates and returns a function that takes a local path inside a module and returns the exports.
Each of these wrapping layers remembers more information for us
(\figref{module-bundler-returning-functions}),
but we won’t pretend that it’s easy to trace.

\figpdf{module-bundler-returning-functions}{./module-bundler/returning-functions.pdf}{A function that returns functions that return functions.}{0.6}


We also need a third structure:
a cache for the modules we’ve already loaded.
Putting it all together we have:


\begin{lstlisting}[frame=tblr]
import fs from 'fs'
import path from 'path'

import transitiveClosure from './transitive-closure.js'

const HEAD = `const creators = new Map()
const cache = new Map()

const makeRequire = (absPath) => {
  return (localPath) => {
    const actualKey = translate[absPath][localPath]
    if (!cache.has(actualKey)) {
      const m = {}
      creators.get(actualKey)(m)
      cache.set(actualKey, m.exports)
    }
    return cache.get(actualKey)
  }
}

const initialize = (creators) => {
`

const TAIL = `
}

initialize(creators)
`

const makeProof = (entryPoint) => `
const start = creators.get('${entryPoint}')
const m = {}
start(m)
m.exports()
`

const createBundle = (entryPoint) => {
  entryPoint = path.resolve(entryPoint)
  const table = transitiveClosure(entryPoint)
  const translate = `const translate = ${JSON.stringify(table, null, 2)}`
  const creators = Object.keys(table).map(filename => makeCreator(filename))
  const proof = makeProof(entryPoint)
  return [
    translate,
    HEAD,
    ...creators,
    TAIL,
    proof
  ].join('\n')
}

const makeCreator = (filename) => {
  const key = path.resolve(filename)
  const source = fs.readFileSync(filename, 'utf-8')
  const func = `(module, require = makeRequire('${key}')) =>\n{${source}}`
  const entry = `creators.set('${key}',\n${func})`
  return `// ${key}\n${entry}\n`
}

export default createBundle
\end{lstlisting}



This code is hard to read
because we have to distinguish what is being printed in the output versus what is being executed right now
and because of the levels of nesting needed to capture variables safely.
Getting this right took much more time per line of finished code than anything we have seen so far
except the promises in \chapref{async-programming}.
However,
it is all intrinsic complexity\index{intrinsic complexity}:
anything that does what \texttt{require} does is going to be equally convoluted.


To prove that our code works,
we will look up the function \texttt{main} in the first file and call it.
(If we were loading in the browser,
we’d capture the exports in a variable for later use.)
First, we create the bundled file:


\begin{lstlisting}[frame=shadowbox]
echo '
node test-create-bundle.js single/main.js >> bundle-single.js
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
const translate = {
  "/u/stjs/stjs/module-bundler/single/main.js": {}
}
const creators = new Map()
const cache = new Map()

const makeRequire = (absPath) => {
  return (localPath) => {
    const actualKey = translate[absPath][localPath]
    if (!cache.has(actualKey)) {
      const m = {}
      creators.get(actualKey)(m)
      cache.set(actualKey, m.exports)
    }
    return cache.get(actualKey)
  }
}

const initialize = (creators) => {

// /u/stjs/stjs/module-bundler/single/main.js
creators.set('/u/stjs/stjs/module-bundler/single/main.js',
(module, require =
makeRequire('/u/stjs/stjs/module-bundler/single/main.js')) =>
{const main = () => {
  console.log('in main')
}

module.exports = main
})


}

initialize(creators)


const start = creators.get('/u/stjs/stjs/module-bundler/single/main.js')
const m = {}
start(m)
m.exports()
\end{lstlisting}



\noindent and then we run it:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
in main
\end{lstlisting}



That was a lot of work to print one line,
but what we have should work for other files.
The two-file case with \texttt{main} and \texttt{other} works:


\begin{lstlisting}[frame=tblr]
const translate = {
  "/u/stjs/stjs/module-bundler/simple/main.js": {
    "./other": "/u/stjs/stjs/module-bundler/simple/other.js"
  },
  "/u/stjs/stjs/module-bundler/simple/other.js": {}
}
const creators = new Map()
const cache = new Map()

const makeRequire = (absPath) => {
  return (localPath) => {
    const actualKey = translate[absPath][localPath]
    if (!cache.has(actualKey)) {
      const m = {}
      creators.get(actualKey)(m)
      cache.set(actualKey, m.exports)
    }
    return cache.get(actualKey)
  }
}

const initialize = (creators) => {

// /u/stjs/stjs/module-bundler/simple/main.js
creators.set('/u/stjs/stjs/module-bundler/simple/main.js',
(module, require =
makeRequire('/u/stjs/stjs/module-bundler/simple/main.js')) =>
{const other = require('./other')

const main = () => {
  console.log(other('main'))
}

module.exports = main
})

// /u/stjs/stjs/module-bundler/simple/other.js
creators.set('/u/stjs/stjs/module-bundler/simple/other.js',
(module, require =
makeRequire('/u/stjs/stjs/module-bundler/simple/other.js')) =>
{const other = (caller) => {
  return `other called from ${caller}`
}

module.exports = other
})


}

initialize(creators)


const start = creators.get('/u/stjs/stjs/module-bundler/simple/main.js')
const m = {}
start(m)
m.exports()
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
other called from main
\end{lstlisting}



\noindent and so does our most complicated test with \texttt{main} and four other files:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
topLeft from main
topRight from main with topLeft from topRight and bottomRight from \
topRight
bottomLeft from main with topLeft from bottomLeft and bottomRight from \
bottomLeft
bottomRight from main
\end{lstlisting}


\section{Exercises}\label{module-bundler-exercises}

\subsection*{Using test-driven development}


Suppose we wanted to compress the files being stored by the file backup system in \chapref{file-backup}
instead of copying them as-is.
What tests would you write before adding this feature in order to ensure that it worked correctly
once it was implemented?

\subsection*{Finding \texttt{import} dependencies}


Modify the dependency finder to work with \texttt{import} statements instead of \texttt{require} calls.

\subsection*{Track files using hashes}


Modify the dependency finder to track files by hashing them instead of relying on paths,
so that if exactly the same file is being required from two locations,
only one copy is loaded.

\subsection*{Using asynchronous file operations}


Modify the dependency finder to use \texttt{async} and \texttt{await} instead of synchronous file operations.

\subsection*{Unit testing transitive closure}


Write unit tests for the tool that finds the transitive closure of files’ requirements
using Mocha and \texttt{mock-fs}.
(Rather than parsing JavaScript files in the mock filesystem,
have each file contain only a list of the names of the files it depends on.)

\subsection*{Exporting multiple functions}


Create test cases for the module bundler in which files export more than one function
and fix any bugs in the module bundler that they uncover.

\subsection*{Checking integrity}


Write a function that checks the integrity of the data structure returned by the transitive closure routine,
i.e.,
that makes sure every cross-reference resolves correctly.

\subsection*{Logging module loading}

\begin{enumerate}

\item 

Write a function called \texttt{logLoad} that takes a module name as an argument
    and prints a message using \texttt{console.error} saying that the module has been loaded.



\item 

Modify the bundle generator to insert calls to this function
    to report when modules are actually loaded.



\end{enumerate}

\subsection*{Tracing execution}


Trace the execution of every function called
when the \texttt{main} function in the full bundle is called.

\subsection*{Making bundles more readable}


Modify the bundle creator to make its output more readable,
e.g.,
by adding comments and indentation.
(This does not matter to the computer,
but can help debugging.)

\chapter{Package Manager}\label{package-manager}


\noindent 
  Terms defined: \glossref{backward-compatible}, \glossref{combinatorial explosion}, \glossref{heuristic}, \glossref{manifest}, \glossref{patch}, \glossref{prune}, \glossref{SAT solver}, \glossref{scoring function}, \glossref{seed}, \glossref{semantic versioning}



There is no point building software if you can’t install it.
Inspired by the Comprehensive TeX Archive Network\index{Comprehensive TeX Archive Network} \hreffoot{CTAN}{https://www.ctan.org/},
most languages now have an online archive from which developers can download packages.
Each package typically has a name and one or more version(s);
each version may have a list of dependencies,
and the package may specify a version or range of versions for each dependency.


Downloading files requires some web programming that is out of scope for this book,
while installing those files in the right places
uses the systems programming skills of \chapref{systems-programming}.
The piece we are missing is a way to figure out exactly what versions of different packages to install
in order to create a consistent setup.
If packages A and B require different versions of C,
it might not be possible to use A and B together.
On the other hand,
if each one requires a range of versions of C and those ranges overlap,
we might be able to find a combination that works—at least,
until we try to install packages D and E.


We \emph{could} install every package’s dependencies separately with it;
the disk space wouldn’t be much of an obstacle,
but loading dozens of copies of the same package into the browser
would slow applications down.
This chapter therefore explores how to find a workable installation or prove that there isn’t one.
It is based in part on \hreffoot{this tutorial}{https://classic.yarnpkg.com/blog/2017/07/11/lets-dev-a-package-manager/} by \hreffoot{Maël Nison}{https://arcanis.github.io/}\index{Nison, Maël}.

\begin{callout}


\subsubsection*{Satisfiability}


What we are trying to do is find a version for each package
that makes the assertion “P is compatible with all its dependencies” true
for every package P.
The general-purpose tools for doing this are called \glossref{SAT solvers}\index{satisfiability}\index{SAT solver}
because they determine whether there is some assignment of values
that satisfies the claim (i.e., makes it true).
Finding a solution can be extremely hard in the general case,
so most SAT solvers use heuristics to try to reduce the work.

\end{callout}

\section{What is semantic versioning?}\label{package-manager-semver}


Most software projects use \glossref{semantic versioning}\index{semantic versioning} for software releases.
Each version number consists of three integers X.Y.Z,
where X is the major version,
Y is the minor version,
and Z is the \glossref{patch}\index{patch number}\index{semantic versioning!patch number} number.
(The \hreffoot{full specification}{https://semver.org/} allows for more fields,
but we will ignore them in this tutorial.)


A package’s authors increment its major version number
every time something changes in a way that makes the package incompatible with previous versions
For example,
if they add a required parameter to a function,
then code built for the old version will fail or behave unpredictably with the new one.
The minor version number is incremented when new functionality
is \glossref{backward-compatible}\index{backward compatibility}—i.e.,
it won’t break any existing code—and the patch number is changed
for backward-compatible bug fixes that don’t add any new features.


The notation for specifying a project’s dependencies looks a lot like arithmetic:
\texttt{>= 1.2.3} means “any version from 1.2.3 onward”,
\texttt{< 4} means “any version before 4.anything”,
and \texttt{1.0 - 3.1} means “any version in the specified range (including patches)”.
Note that version 2.1 is greater than version 1.99:
no matter how large a minor version number becomes,
it never spills over into the major version number
in the way that minutes add up to hours or months add up to years.


It isn’t hard to write a few simple comparisons for semantic version identifiers,
but getting all the different cases right is almost as tricky as handling dates and times correctly,
so we will rely on the \hreffoot{\texttt{semver}}{https://www.npmjs.com/package/semver} module.
\texttt{semver.valid({\textquotesingle}1.2.3{\textquotesingle})} checks that \texttt{1.2.3} is a valid version identifier,
while \texttt{semver.satisfies({\textquotesingle}2.2{\textquotesingle}, {\textquotesingle}1.0 - 3.1{\textquotesingle})} checks that its first argument
is compatible with the range specified in its second.

\section{How can we find a consistent set of packages?}\label{package-manager-consistent}


Imagine that each package we need is represented as an axis on a multi-dimensional grid,
with its versions as the tick marks
(\figref{package-manager-allowable}).
Each point on the grid is a possible combination of package versions.
We can block out regions of this grid using the constraints on the package versions;
whatever points are left when we’re done represent legal combinations.

\figpdf{package-manager-allowable}{./package-manager/allowable.pdf}{Finding allowable combinations of package versions.}{0.6}


For example,
suppose we have the set of requirements shown in \tblref{package-manager-example-dependencies}.
There are 18 possible configurations
(2 for X × 3 for Y × 3 for Z)
but 16 are excluded by various incompatibilities.
Of the two remaining possibilities,
X/2 + Y/3 + Z/3 is strictly greater than X/2 + Y/2 + Z/2,
so we would probably choose the former
(\tblref{package-manager-example-result}).
If we wound up with A/1 + B/2 versus A/2 + B/1,
we would need to add rules for resolving ties.

\begin{callout}


\subsubsection*{Reproducibility}


No matter what kind of software you build,
a given set of inputs should always produce the same output;
if they don’t,
testing is much more difficult (or impossible) \cite{Taschuk2017}.
There may not be a strong reason to prefer one mutually-compatible set of packages over another,
but a package manager should still resolve the ambiguity the same way every time.
It may not be what everyone wants,
but at least they will be unhappy for the same reasons everywhere.
This is why NPM has both \texttt{package.json} and a \texttt{package-lock.json} files:
the former is written by the user and specifies what they \emph{want},
while the latter is created by the package manager and specifies exactly what they \emph{got}.
If you want to reproduce someone else’s setup for debugging purposes,
you should install what is described in the latter file.

\end{callout}

\begin{table}[h]
\begin{tabular}{ll}
\textbf{\underline{Package}} & \textbf{\underline{Requires}} \\
X/1 & Y/1-2 \\
X/1 & Z/1 \\
X/2 & Y/2-3 \\
X/2 & Z/1-2 \\
Y/1 & Z/2 \\
Y/2 & Z/2-3 \\
Y/3 & Z/3 \\
Z/1 &  \\
Z/2 &  \\
Z/3 &  \\
\end{tabular}
\caption{Example package dependencies.}
\label{package-manager-example-dependencies}
\end{table}


\begin{table}[h]
\begin{tabular}{llll}
\textbf{\underline{X}} & \textbf{\underline{Y}} & \textbf{\underline{Z}} & \textbf{\underline{Excluded}} \\
1 & 1 & 1 & Y/1 - Z/1 \\
1 & 1 & 2 & X/1 - Z/2 \\
1 & 1 & 3 & X/1 - Z/3 \\
1 & 2 & 1 & Y/2 - Z/1 \\
1 & 2 & 2 & X/1 - Z/2 \\
1 & 2 & 3 & X/1 - Z/3 \\
1 & 3 & 1 & X/1 - Y/3 \\
1 & 3 & 2 & X/1 - Y/3 \\
1 & 3 & 3 & X/1 - Y/3 \\
2 & 1 & 1 & X/2 - Y/1 \\
2 & 1 & 2 & X/2 - Y/1 \\
2 & 1 & 3 & X/2 - Y/1 \\
2 & 2 & 1 & Y/2 - Z/1 \\
2 & 2 & 2 &  \\
2 & 2 & 3 & X/2 - Z/3 \\
2 & 3 & 1 & Y/3 - Z/1 \\
2 & 3 & 2 & Y/3 - Z/2 \\
2 & 3 & 3 & X/2 - Z/3 \\
\end{tabular}
\caption{Result for example package dependencies.}
\label{package-manager-example-result}
\end{table}



To construct \tblref{package-manager-example-dependencies}
we find the transitive closure\index{transitive closure} of all packages plus all of their dependencies.
We then pick two packages and create a list of their valid pairs.
Choosing a third package,
we cross off pairs that can’t be satisfied
to leave triples of legal combinations.
We repeat this until all packages are included in our table.


In the worst case this procedure will create
a \glossref{combinatorial explosion}\index{combinatorial explosion} of possibilities.
Smart algorithms will try to add packages to the mix
in an order that minimize the number of new possibilities at each stage,
or create pairs and then combine them to create pairs of pairs and so on.
Our algorithm will be simpler (and therefore slower),
but illustrates the key idea.

\section{How can we satisfy constraints?}\label{package-manager-constraints}


To avoid messing around with parsers,
our programs reads a JSON data structure describing the problem;
a real package manager would read the \glossref{manifests}\index{manifest (of package)}\index{package manifest} of the packages in question
and construct a similar data structure.
We will stick to single-digit version numbers for readability,
and will use this as our first test case:


\begin{lstlisting}[frame=tblr]
{
  "X": {
    "1": {
      "Y": ["1"]
    },
    "2": {
      "Y": ["2"]
    }
  },
  "Y": {
    "1": {},
    "2": {}
  }
}
\end{lstlisting}


\begin{callout}


\subsubsection*{Comments}


If you ever design a data format,
please include a standard way for people to add comments,
because they will always want to.
YAML has this,
but JSON and CSV don’t.

\end{callout}


To check if a combination of specific versions of packages is compatible with a manifest,
we add each package to our active list in turn and look for violations.
If there aren’t any more packages to add and we haven’t found a violation,
then what we have must be a legal configuration.


\begin{lstlisting}[frame=tblr]
import configStr from './config-str.js'

const sweep = (manifest) => {
  const names = Object.keys(manifest)
  const result = []
  recurse(manifest, names, {}, result)
}

const recurse = (manifest, names, config, result) => {
  if (names.length === 0) {
    if (allows(manifest, config)) {
      result.push({ ...config })
    }
  } else {
    const next = names[0]
    const rest = names.slice(1)
    for (const version in manifest[next]) {
      config[next] = version
      recurse(manifest, rest, config, result)
    }
  }
}


export default sweep
\end{lstlisting}



The simplest way to find configurations is to sweep over all possibilities.
For debugging purposes,
our function prints possibilities as it goes:


\begin{lstlisting}[frame=tblr]
const allows = (manifest, config) => {
  for (const [leftN, leftV] of Object.entries(config)) {
    const requirements = manifest[leftN][leftV]
    for (const [rightN, rightVAll] of Object.entries(requirements)) {
      if (!rightVAll.includes(config[rightN])) {
        const title = configStr(config)
        const missing = config[rightN]
        console.log(`${title} @ ${leftN}/${leftV} ${rightN}/${missing}`)
        return false
      }
    }
  }
  console.log(configStr(config))
  return true
}
\end{lstlisting}



If we run this program on the two-package example shown earlier, we get this output:


\begin{lstlisting}[frame=shadowbox]
node driver.js ./sweep.js double-chained.json
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{X:1 Y:1}
{X:1 Y:2} @ X/1 Y/2
{X:2 Y:1} @ X/2 Y/1
{X:2 Y:2}
\end{lstlisting}



When we run it on our triple-package example, we get this:


\begin{lstlisting}[frame=shadowbox]
node driver.js ./sweep.js triple.json
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{X:1 Y:1 Z:1} @ Y/1 Z/1
{X:1 Y:1 Z:2} @ X/1 Z/2
{X:1 Y:1 Z:3} @ X/1 Z/3
{X:1 Y:2 Z:1} @ Y/2 Z/1
{X:1 Y:2 Z:2} @ X/1 Z/2
{X:1 Y:2 Z:3} @ X/1 Z/3
{X:1 Y:3 Z:1} @ X/1 Y/3
{X:1 Y:3 Z:2} @ X/1 Y/3
{X:1 Y:3 Z:3} @ X/1 Y/3
{X:2 Y:1 Z:1} @ X/2 Y/1
{X:2 Y:1 Z:2} @ X/2 Y/1
{X:2 Y:1 Z:3} @ X/2 Y/1
{X:2 Y:2 Z:1} @ Y/2 Z/1
{X:2 Y:2 Z:2}
{X:2 Y:2 Z:3} @ X/2 Z/3
{X:2 Y:3 Z:1} @ Y/3 Z/1
{X:2 Y:3 Z:2} @ Y/3 Z/2
{X:2 Y:3 Z:3} @ X/2 Z/3
\end{lstlisting}



This works,
but it is doing a lot of unnecessary work.
If we sort the output by the case that caught the exclusion,
it turns out that 9 of the 17 exclusions are redundant rediscovery of a previously known problem
(\tblref{package-manager-exclusions}).

\begin{table}
\begin{tabular}{llll}
\textbf{\underline{Excluded}} & \textbf{\underline{X}} & \textbf{\underline{Y}} & \textbf{\underline{Z}} \\
X/1 - Y/3 & 1 & 3 & 1 \\
... & 1 & 3 & 2 \\
... & 1 & 3 & 3 \\
X/1 - Z/2 & 1 & 1 & 2 \\
... & 1 & 2 & 2 \\
X/1 - Z/3 & 1 & 1 & 3 \\
... & 1 & 2 & 3 \\
X/2 - Y/1 & 2 & 1 & 1 \\
... & 2 & 1 & 2 \\
... & 2 & 1 & 3 \\
X/2 - Z/3 & 2 & 2 & 3 \\
... & 2 & 3 & 3 \\
Y/1 - Z/1 & 1 & 1 & 1 \\
Y/2 - Z/1 & 1 & 2 & 1 \\
... & 2 & 2 & 1 \\
Y/3 - Z/1 & 2 & 3 & 1 \\
... & 2 & 3 & 2 \\
 & 2 & 2 & 2 \\
\end{tabular}
\caption{Package exclusions.}
\label{package-manager-exclusions}
\end{table}


\section{How can we do less work?}\label{package-manager-optimize}


We can make this more efficient by \glossref{pruning}\index{prune (a search tree)} the search tree
as we go along
(\figref{package-manager-pruning}).
After all,
if we know that X and Y are incompatible,
there is no need to check Z as well.

\figpdf{package-manager-pruning}{./package-manager/pruning.pdf}{Pruning options in the search tree to reduce work.}{0.6}


This version of the program collects possible solutions and displays them at the end.
It only keeps checking a partial solution if what it has found so far looks good:


\begin{lstlisting}[frame=tblr]
import configStr from './config-str.js'

const prune = (manifest) => {
  const names = Object.keys(manifest)
  const result = []
  recurse(manifest, names, {}, result)
  for (const config of result) {
    console.log(configStr(config))
  }
}

const recurse = (manifest, names, config, result) => {
  if (names.length === 0) {
    result.push({ ...config })
  } else {
    const next = names[0]
    const rest = names.slice(1)
    for (const version in manifest[next]) {
      config[next] = version
      if (compatible(manifest, config)) {
        recurse(manifest, rest, config, result)
      }
      delete config[next]
    }
  }
}


const report = (config, leftN, leftV, rightN, rightV) => {
  const title = configStr(config)
  console.log(`${title} @ ${leftN}/${leftV} ${rightN}/${rightV}`)
}

export default prune
\end{lstlisting}



The \texttt{compatible} function checks to see if adding something will leave us with a consistent configuration:


\begin{lstlisting}[frame=tblr]
const compatible = (manifest, config) => {
  for (const [leftN, leftV] of Object.entries(config)) {
    const leftR = manifest[leftN][leftV]
    for (const [rightN, rightV] of Object.entries(config)) {
      if ((rightN in leftR) && (!leftR[rightN].includes(rightV))) {
        report(config, leftN, leftV, rightN, rightV)
        return false
      }
      const rightR = manifest[rightN][rightV]
      if ((leftN in rightR) && (!rightR[leftN].includes(leftV))) {
        report(config, leftN, leftV, rightN, rightV)
        return false
      }
    }
  }
  return true
}
\end{lstlisting}



Checking as we go gets us from 18 complete solutions to 11.
One is workable
and two are incomplete—they represent 6 possible complete solutions that we didn’t need to finish:


\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{X:1 Y:1 Z:1} @ Y/1 Z/1
{X:1 Y:1 Z:2} @ X/1 Z/2
{X:1 Y:1 Z:3} @ X/1 Z/3
{X:1 Y:2 Z:1} @ Y/2 Z/1
{X:1 Y:2 Z:2} @ X/1 Z/2
{X:1 Y:2 Z:3} @ X/1 Z/3
{X:1 Y:3} @ X/1 Y/3
{X:2 Y:1} @ X/2 Y/1
{X:2 Y:2 Z:1} @ Y/2 Z/1
{X:2 Y:2 Z:3} @ X/2 Z/3
{X:2 Y:3 Z:1} @ Y/3 Z/1
{X:2 Y:3 Z:2} @ Y/3 Z/2
{X:2 Y:3 Z:3} @ X/2 Z/3
{X:2 Y:2 Z:2}
\end{lstlisting}



Another way to look at the work is the number of steps in the search.
The full search had 18×3 = 54 steps.
Pruning leaves us with (12×3) + (2×2) = 40 steps
so we have eliminated roughly 1/4 of the work.


What if we searched in the reverse order?


\begin{lstlisting}[frame=tblr]
import configStr from './config-str.js'

// [reverse]
const reverse = (manifest) => {
  const names = Object.keys(manifest)
  names.reverse()
  const result = []
  recurse(manifest, names, {}, result)
  for (const config of result) {
    console.log(configStr(config))
  }
}
// [/reverse]

const recurse = (manifest, names, config, result) => {
  if (names.length === 0) {
    result.push({ ...config })
  } else {
    const next = names[0]
    const rest = names.slice(1)
    for (const version in manifest[next]) {
      config[next] = version
      if (compatible(manifest, config)) {
        recurse(manifest, rest, config, result)
      }
      delete config[next]
    }
  }
}

const compatible = (manifest, config) => {
  for (const [leftN, leftV] of Object.entries(config)) {
    const leftR = manifest[leftN][leftV]
    for (const [rightN, rightV] of Object.entries(config)) {
      if ((rightN in leftR) && (!leftR[rightN].includes(rightV))) {
        report(config, leftN, leftV, rightN, rightV)
        return false
      }
      const rightR = manifest[rightN][rightV]
      if ((leftN in rightR) && (!rightR[leftN].includes(leftV))) {
        report(config, leftN, leftV, rightN, rightV)
        return false
      }
    }
  }
  return true
}

const report = (config, leftN, leftV, rightN, rightV) => {
  const title = configStr(config)
  console.log(`${title} @ ${leftN}/${leftV} ${rightN}/${rightV}`)
}

export default reverse
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
{Z:1 Y:1} @ Z/1 Y/1
{Z:1 Y:2} @ Z/1 Y/2
{Z:1 Y:3} @ Z/1 Y/3
{Z:2 Y:1 X:1} @ Z/2 X/1
{Z:2 Y:1 X:2} @ Y/1 X/2
{Z:2 Y:2 X:1} @ Z/2 X/1
{Z:2 Y:3} @ Z/2 Y/3
{Z:3 Y:1} @ Z/3 Y/1
{Z:3 Y:2 X:1} @ Z/3 X/1
{Z:3 Y:2 X:2} @ Z/3 X/2
{Z:3 Y:3 X:1} @ Z/3 X/1
{Z:3 Y:3 X:2} @ Z/3 X/2
{Z:2 Y:2 X:2}
\end{lstlisting}



Now we have (8×3) + (5×2) = 34 steps,
i.e.,
we have eliminated roughly 1/3 of the work.
That may not seem like a big difference,
but if we go five levels deep at the same rate,
it cuts the work in half.
There are lots of \glossref{heuristics} for searching trees;
none are guaranteed to give better performance in every case,
but most give better performance in most cases.

\begin{callout}


\subsubsection*{What research is for}


SAT solvers\index{SAT solver} are like regular expression libraries and random number generators:
it is the work of many lifetimes to create ones that are both fast and correct.
A lot of computer science researchers devote their careers to highly-specialized topics like this.
The debates often seem esoteric to outsiders,
and most ideas turn out to be dead ends,
but even small improvements in fundamental tools can have a profound impact.

\end{callout}

\section{Exercises}\label{package-manager-exercises}

\subsection*{Comparing semantic versions}


Write a function that takes an array of semantic version specifiers
and sorts them in ascending order.
Remember that \texttt{2.1} is greater than \texttt{1.99}.

\subsection*{Parsing semantic versions}


Using the techniques of \chapref{regex-parser},
write a parser for a subset of the \hreffoot{semantic versioning specification}{https://semver.org/}.

\subsection*{Using scoring functions}


Many different combinations of package versions can be mutually compatible.
One way to decide which actual combination to install
is to create a \glossref{scoring function}
that measures how good or bad a particular combination is.
For example,
a function could measure the “distance” between two versions as:

\begin{lstlisting}[frame=tblr]
const score (X, Y) => {
  if (X.major !== Y.major) {
    return 100 * abs(X.major - Y.major)
  } else if (X.minor !== Y.minor) {
    return 10 * abs(X.minor - Y.minor)
  } else {
    return abs(X.patch - Y.patch)
  }
}
\end{lstlisting}

\begin{enumerate}

\item 

Implement a working version of this function
    and use it to measure the total distance between
    the set of packages found by the solver
    and the set containing the most recent version of each package.



\item 

Explain why this doesn’t actually solve the original problem.



\end{enumerate}

\subsection*{Using full semantic versions}


Modify the constraint solver to use full semantic versions instead of single digits.

\subsection*{Regular releases}


Some packages release new versions on a regular cycle,
e.g.,
Version 2021.1 is released on March 1 of 2021,
Version 2021.2 is released on September 1 of that year,
version 2022.1 is released on March 1 of the following year,
and so on.

\begin{enumerate}

\item 

How does this make package management easier?



\item 

How does it make it more difficult?



\end{enumerate}

\subsection*{Writing unit tests}


Write unit tests for the constraint solver using Mocha.

\subsection*{Generating test fixtures}


Write a function that creates fixtures for testing the constraint solver:

\begin{enumerate}

\item 

Its first argument is an object whose keys are (fake) package names
    and whose values are integers indicating the number of versions of that package
    to include in the test set,
    such as \texttt{\{{\textquotesingle}left{\textquotesingle}: 3, {\textquotesingle}middle{\textquotesingle}: 2, {\textquotesingle}right{\textquotesingle}: 15\}}.
    Its second argument is a \glossref{seed} for random number generation.



\item 

It generates one valid configuration,
    such as \texttt{\{{\textquotesingle}left{\textquotesingle}: 2, {\textquotesingle}middle{\textquotesingle}: 2, {\textquotesingle}right{\textquotesingle}: 9\}}.
    (This is to ensure that there is at least one installable set of packages.)



\item 

It then generates random constraints between the packages.
    (These may or may not result in other installable combinations.)
    When this is done,
    it adds constraints so that the valid configuration from the previous step is included.



\end{enumerate}

\subsection*{Searching least first}


Rewrite the constraint solver so that it searches packages
by looking at those with the fewest available versions first.
Does this reduce the amount of work done for the small examples in this chapter?
Does it reduce the amount of work done for larger examples?

\subsection*{Using generators}


Rewrite the constraint solver to use generators.

\subsection*{Using exclusions}

\begin{enumerate}

\item 

Modify the constraint solver so that
    it uses a list of package exclusions instead of a list of package requirements,
    i.e.,
    its input tells it that version 1.2 of package Red
    can \emph{not} work with versions 3.1 and 3.2 of package Green
    (which implies that Red 1.2 can work with any other versions of Green).



\item 

Explain why package managers aren’t built this way.



\end{enumerate}

\chapter{Virtual Machine}\label{virtual-machine}


\noindent 
  Terms defined: \glossref{Application Binary Interface}, \glossref{assembler}, \glossref{assembly code}, \glossref{bitwise operation}, \glossref{disassembler}, \glossref{instruction pointer}, \glossref{instruction set}, \glossref{label (address in memory)}, \glossref{op code}, \glossref{register}, \glossref{virtual machine}, \glossref{word (of memory)}



Computers don’t execute JavaScript directly.
Instead,
each processor has its own \glossref{instruction set}\index{instruction set},
and a compiler translates high-level languages into those instructions.
Compilers often use an intermediate representation called \glossref{assembly code}\index{assembly code}
that gives instructions names instead of numbers.
To understand more about how JavaScript actually runs,
we will simulate a very simple processor with a little bit of memory.
If you want to dive deeper,
have a look at \hreffoot{Bob Nystrom’s}{http://journal.stuffwithstuff.com/}\index{Nystrom, Bob} \emph{\hreffoot{Crafting Interpreters}{https://craftinginterpreters.com/}}.
You may also enjoy \hreffoot{Human Resource Machine}{https://tomorrowcorporation.com/humanresourcemachine}[\index{Human Resource Machine},
which asks you to solve puzzles of increasing difficulty
using a processor almost as simple as ours.

\section{What is the architecture of our virtual machine?}\label{virtual-machine-arch}


Our \glossref{virtual machine}\index{virtual machine} has three parts,
which are shown in \figref{virtual-machine-architecture}
for a program made up of 110 instructions:

\begin{enumerate}

\item 

An \glossref{instruction pointer}\index{instruction pointer} (IP)
    that holds the memory address of the next instruction to execute.
    It is automatically initialized to point at address 0,
    which is where every program must start.
    This rule is part of the \glossref{Application Binary Interface}\index{Application Binary Interface} (ABI)
    for our virtual machine.



\item 

Four \glossref{registers}\index{register (in computer)} named R0 to R3 that instructions can access directly.
    There are no memory-to-memory operations in our VM:
    everything  happens in or through registers.



\item 

256 \glossref{words} of memory, each of which can store a single value.
    Both the program and its data live in this single block of memory;
    we chose the size 256 so that each address will fit in a single byte.



\end{enumerate}


The instructions for our VM are 3 bytes long.
The \glossref{op code}\index{op code}\index{virtual machine!op code} fits into one byte,
and each instruction may optionally include one or two single-byte operands.
Each operand is a register identifier,
a constant,
or an address
(which is just a constant that identifies a location in memory);
since constants have to fit in one byte,
the largest number we can represent directly is 256.
\tblref{virtual-machine-op-codes} uses the letters \texttt{r}, \texttt{c}, and \texttt{a}
to indicate instruction format,
where \texttt{r} indicates a register identifier,
\texttt{c} indicates a constant,
and \texttt{a} indicates an address.

\figpdfhere{virtual-machine-architecture}{./virtual-machine/architecture.pdf}{Architecture of the virtual machine.}{0.6}

\begin{table}[h]
\begin{tabular}{llllll}
\textbf{\underline{Instruction}} & \textbf{\underline{Code}} & \textbf{\underline{Format}} & \textbf{\underline{Action}} & \textbf{\underline{Example}} & \textbf{\underline{Equivalent}} \\
\texttt{hlt} & 1 & \texttt{--} & Halt program & \texttt{hlt} & \texttt{process.exit(0)} \\
\texttt{ldc} & 2 & \texttt{rc} & Load immediate & \texttt{ldc R0 123} & \texttt{R0 := 123} \\
\texttt{ldr} & 3 & \texttt{rr} & Load register & \texttt{ldr R0 R1} & \texttt{R0 := RAM[R1]} \\
\texttt{cpy} & 4 & \texttt{rr} & Copy register & \texttt{cpy R0 R1} & \texttt{R0 := R1} \\
\texttt{str} & 5 & \texttt{rr} & Store register & \texttt{str R0 R1} & \texttt{RAM[R1] := R0} \\
\texttt{add} & 6 & \texttt{rr} & Add & \texttt{add R0 R1} & \texttt{R0 := R0 + R1} \\
\texttt{sub} & 7 & \texttt{rr} & Subtract & \texttt{sub R0 R1} & \texttt{R0 := R0 - R1} \\
\texttt{beq} & 8 & \texttt{ra} & Branch if equal & \texttt{beq R0 123} & \texttt{if (R0 === 0) PC := 123} \\
\texttt{bne} & 9 & \texttt{ra} & Branch if not equal & \texttt{bne R0 123} & \texttt{if (R0 !== 0) PC := 123} \\
\texttt{prr} & 10 & \texttt{r-} & Print register & \texttt{prr R0} & \texttt{console.log(R0)} \\
\texttt{prm} & 11 & \texttt{r-} & Print memory & \texttt{prm R0} & \texttt{console.log(RAM[R0])} \\
\end{tabular}
\caption{Virtual machine op codes.}
\label{virtual-machine-op-codes}
\end{table}



\newpage


We put our VM’s architectural details in a file
that can be shared by other components:


\begin{lstlisting}[frame=tblr]
const OPS = {
  hlt: { code:  1, fmt: '--' }, // Halt program
  ldc: { code:  2, fmt: 'rv' }, // Load immediate
  ldr: { code:  3, fmt: 'rr' }, // Load register
  cpy: { code:  4, fmt: 'rr' }, // Copy register
  str: { code:  5, fmt: 'rr' }, // Store register
  add: { code:  6, fmt: 'rr' }, // Add
  sub: { code:  7, fmt: 'rr' }, // Subtract
  beq: { code:  8, fmt: 'rv' }, // Branch if equal
  bne: { code:  9, fmt: 'rv' }, // Branch if not equal
  prr: { code: 10, fmt: 'r-' }, // Print register
  prm: { code: 11, fmt: 'r-' }  // Print memory
}

const OP_MASK = 0xFF // select a single byte
const OP_SHIFT = 8   // shift up by one byte
const OP_WIDTH = 6   // op width in characters when printing

const NUM_REG = 4    // number of registers
const RAM_LEN = 256  // number of words in RAM

export {
  OPS,
  OP_MASK,
  OP_SHIFT,
  OP_WIDTH,
  NUM_REG,
  RAM_LEN
}
\end{lstlisting}



\noindent While there isn’t a name for this design pattern,
putting all the constants that define a system in one file
instead of scattering them across multiple files
makes them easier to find as well as ensuring consistency.

\section{How can we execute these instructions?}\label{virtual-machine-execute}


As in previous chapters,
we will split a class that would normally be written in one piece into several parts for exposition.
We start by defining a class with an instruction pointer, some registers, and some memory
along with a prompt for output:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import {
  OP_MASK,
  OP_SHIFT,
  NUM_REG,
  RAM_LEN
} from './architecture.js'

const COLUMNS = 4
const DIGITS = 8

class VirtualMachineBase {
  constructor () {
    this.ip = 0
    this.reg = Array(NUM_REG)
    this.ram = Array(RAM_LEN)
    this.prompt = '>>'
  }

}

export default VirtualMachineBase
\end{lstlisting}



A program is just an array of numbers representing instructions.
To load one,
we copy those numbers into memory and reset the instruction pointer and registers:


\begin{lstlisting}[frame=tblr]
  initialize (program) {
    assert(program.length <= this.ram.length,
      'Program is too long for memory')
    for (let i = 0; i < this.ram.length; i += 1) {
      if (i < program.length) {
        this.ram[i] = program[i]
      } else {
        this.ram[i] = 0
      }
    }
    this.ip = 0
    this.reg.fill(0)
  }
\end{lstlisting}



In order to handle the next instruction,
the VM gets the value in memory that the instruction pointer currently refers to
and moves the instruction pointer on by one address.
It then uses \glossref{bitwise operations}\index{bitwise operation}
to extract the op code and operands from the instruction
(\figref{virtual-machine-unpacking}):


\begin{lstlisting}[frame=tblr]
  fetch () {
    assert((0 <= this.ip) && (this.ip < RAM_LEN),
      `Program counter ${this.ip} out of range 0..${RAM_LEN}`)
    let instruction = this.ram[this.ip]
    this.ip += 1
    const op = instruction & OP_MASK
    instruction >>= OP_SHIFT
    const arg0 = instruction & OP_MASK
    instruction >>= OP_SHIFT
    const arg1 = instruction & OP_MASK
    return [op, arg0, arg1]
  }
\end{lstlisting}


\figpdfhere{virtual-machine-unpacking}{./virtual-machine/unpacking.pdf}{Using bitwise operations to unpack instructions.}{0.6}

\begin{callout}


\subsubsection*{Semi-realistic}


We always unpack two operands regardless of whether the instructions has them or not,
since this is what a hardware implementation would be.
We have also included assertions in our VM
to simulate the way that real hardware includes logic
to detect illegal instructions and out-of-bound memory addresses.

\end{callout}


The next step is to extend our base class with one that has a \texttt{run} method.
As its name suggests,
this runs the program by fetching instructions and executing them until told to stop:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import {
  OPS
} from './architecture.js'

import VirtualMachineBase from './vm-base.js'

class VirtualMachine extends VirtualMachineBase {
  run () {
    let running = true
    while (running) {
      const [op, arg0, arg1] = this.fetch()
      switch (op) {
        case OPS.hlt.code:
          running = false
          break

        case OPS.ldc.code:
          this.assertIsRegister(arg0, op)
          this.reg[arg0] = arg1
          break


        default:
          assert(false, `Unknown op ${op}`)
          break
      }
    }
  }

  assertIsRegister (reg) {
    assert((0 <= reg) && (reg < this.reg.length),
      `Invalid register ${reg}`)
  }

  assertIsAddress (addr) {
    assert((0 <= addr) && (addr < this.ram.length),
      `Invalid register ${addr}`)
  }
}

export default VirtualMachine
\end{lstlisting}



Some instructions are very similar to others,
so we will only look at three here.
The first stores the value of one register in the address held by another register:


\begin{lstlisting}[frame=tblr]
        case OPS.str.code:
          this.assertIsRegister(arg0, op)
          this.assertIsRegister(arg1, op)
          this.assertIsAddress(this.reg[arg1], op)
          this.ram[this.reg[arg1]] = this.reg[arg0]
          break
\end{lstlisting}



\noindent The first three lines check that the operation is legal;
the fourth one uses the value in one register as an address,
which is why it has nested array indexing.


Adding the value in one register to the value in another register is simpler:


\begin{lstlisting}[frame=tblr]
        case OPS.add.code:
          this.assertIsRegister(arg0, op)
          this.assertIsRegister(arg1, op)
          this.reg[arg0] += this.reg[arg1]
          break
\end{lstlisting}



\noindent as is jumping to a fixed address if the value in a register is zero:


\begin{lstlisting}[frame=tblr]
        case OPS.beq.code:
          this.assertIsRegister(arg0, op)
          this.assertIsAddress(arg1, op)
          if (this.reg[arg0] === 0) {
            this.ip = arg1
          }
          break
\end{lstlisting}


\section{What do assembly programs look like?}\label{virtual-machine-assembly}


We could figure out numerical op codes by hand,
and in fact that’s what \hreffoot{the first programmers}{http://eniacprogrammers.org/} did.
However,
it is much easier to use an \glossref{assembler}\index{assembler},
which is just a small compiler for a language that very closely represents actual machine instructions.


Each command in our assembly languages matches an instruction in the VM.
Here’s an assembly language program to print the value stored in R1 and then halt:


\begin{lstlisting}[frame=tblr]
# Print initial contents of R1.
prr R1
hlt
\end{lstlisting}



\noindent Its numeric representation is:


\begin{lstlisting}[frame=tblr]
00010a
000001
\end{lstlisting}



One thing the assembly language has that the instruction set doesn’t
is \glossref{labels on addresses}\index{label (on address)}.
The label \texttt{loop} doesn’t take up any space;
instead,
it tells the assembler to give the address of the next instruction a name
so that we can refer to that address as \texttt{@loop} in jump instructions.
For example,
this program prints the numbers from 0 to 2
(\figref{virtual-machine-count-up}):


\begin{lstlisting}[frame=tblr]
# Count up to 3.
# - R0: loop index.
# - R1: loop limit.
ldc R0 0
ldc R1 3
loop:
prr R0
ldc R2 1
add R0 R2
cpy R2 R1
sub R2 R0
bne R2 @loop
hlt
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
000002
030102
00000a
010202
020006
010204
000207
020209
000001
\end{lstlisting}


\figpdf{virtual-machine-count-up}{./virtual-machine/count-up.pdf}{Flowchart of assembly language program to count up from 0 to 2.}{0.6}


Let’s trace this program’s execution
(\figref{virtual-machine-trace-counter}):

\begin{enumerate}

\item R0 holds the current loop index.

\item R1 holds the loop’s upper bound (in this case 3).

\item The loop prints the value of R0 (one instruction).

\item The program adds 1 to R0.
    This takes two instructions because we can only add register-to-register.

\item It checks to see if we should loop again,
    which takes three instructions.

\item If the program \emph{doesn’t} jump back, it halts.

\end{enumerate}

\figpdf{virtual-machine-trace-counter}{./virtual-machine/trace-counter.pdf}{Tracing registers and memory values for a simple counting program.}{0.6}


\noindent The implementation of the assembler mirrors the simplicity of assembly language.
The main method gets interesting lines,
finds the addresses of labels,
and turns each remaining line into an instruction:


\begin{lstlisting}[frame=tblr]
  assemble (lines) {
    lines = this.cleanLines(lines)
    const labels = this.findLabels(lines)
    const instructions = lines.filter(line => !this.isLabel(line))
    const compiled = instructions.map(instr => this.compile(instr, labels))
    const program = this.instructionsToText(compiled)
    return program
  }

  cleanLines (lines) {
    return lines
      .map(line => line.trim())
      .filter(line => line.length > 0)
      .filter(line => !this.isComment(line))
  }

  isComment (line) {
    return line.startsWith('#')
  }
\end{lstlisting}



To find labels,
we go through the lines one by one
and either save the label \emph{or} increment the current address
(because labels don’t take up space):


\begin{lstlisting}[frame=tblr]
  findLabels (lines) {
    const result = {}
    let index = 0
    lines.forEach(line => {
      if (this.isLabel(line)) {
        const label = line.slice(0, -1)
        assert(!(label in result),
          `Duplicate label ${label}`)
        result[label] = index
      } else {
        index += 1
      }
    })
    return result
  }

  isLabel (line) {
    return line.endsWith(':')
  }
\end{lstlisting}



To compile a single instruction we break the line into tokens,
look up the format for the operands,
and pack them into a single value:


\begin{lstlisting}[frame=tblr]
  compile (instruction, labels) {
    const [op, ...args] = instruction.split(/\s+/)
    assert(op in OPS,
      `Unknown operation "${op}"`)
    let result = 0
    switch (OPS[op].fmt) {
      case '--':
        result = this.combine(
          OPS[op].code
        )
        break
      case 'r-':
        result = this.combine(
          this.register(args[0]),
          OPS[op].code
        )
        break
      case 'rr':
        result = this.combine(
          this.register(args[1]),
          this.register(args[0]),
          OPS[op].code
        )
        break
      case 'rv':
        result = this.combine(
          this.value(args[1], labels),
          this.register(args[0]),
          OPS[op].code
        )
        break
      default:
        assert(false,
          `Unknown instruction format ${OPS[op].fmt}`)
    }
    return result
  }
\end{lstlisting}



Combining op codes and operands into a single value
is the reverse of the unpacking done by the virtual machine:


\begin{lstlisting}[frame=tblr]
  combine (...args) {
    assert(args.length > 0,
      'Cannot combine no arguments')
    let result = 0
    for (const a of args) {
      result <<= OP_SHIFT
      result |= a
    }
    return result
  }
\end{lstlisting}



Finally, we need few utility functions:


\begin{lstlisting}[frame=tblr]
  instructionsToText (program) {
    return program.map(op => op.toString(16).padStart(OP_WIDTH, '0'))
  }

  register (token) {
    assert(token[0] === 'R',
      `Register "${token}" does not start with 'R'`)
    const r = parseInt(token.slice(1))
    assert((0 <= r) && (r < NUM_REG),
      `Illegal register ${token}`)
    return r
  }

  value (token, labels) {
    if (token[0] !== '@') {
      return parseInt(token)
    }
    const labelName = token.slice(1)
    assert(labelName in labels,
      `Unknown label "${token}"`)
    return labels[labelName]
  }
\end{lstlisting}



Let’s try assembling a program and display its output,
the registers,
and the interesting contents of memory.
As a test,
this program counts up to three:


\begin{lstlisting}[frame=tblr]
# Count up to 3.
# - R0: loop index.
# - R1: loop limit.
ldc R0 0
ldc R1 3
loop:
prr R0
ldc R2 1
add R0 R2
cpy R2 R1
sub R2 R0
bne R2 @loop
hlt
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
>> 0
>> 1
>> 2
R0 = 3
R1 = 3
R2 = 0
R3 = 0
0:   00000002  00030102  0000000a  00010202
4:   00020006  00010204  00000207  00020209
8:   00000001  00000000  00000000  00000000
\end{lstlisting}


\section{How can we store data?}\label{virtual-machine-data}


It is tedious to write interesting programs when each value needs a unique name.
We can do a lot more once we have collections like arrays\index{array!implementation of},
so let’s add those to our assembler.
We don’t have to make any changes to the virtual machine,
which doesn’t care if we think of a bunch of numbers as individuals or elements of an array,
but we do need a way to create arrays and refer to them.


We will allocate storage for arrays at the end of the program
by using \texttt{.data} on a line of its own to mark the start of the data section
and then \texttt{label: number} to give a region a name and allocate some storage space
(\figref{virtual-machine-storage-allocation}).


This enhancement only requires a few changes to the assembler.
First,
we need to split the lines into instructions and data allocations:


\begin{lstlisting}[frame=tblr]
  assemble (lines) {
    lines = this.cleanLines(lines)
    const [toCompile, toAllocate] = this.splitAllocations(lines)
    const labels = this.findLabels(lines)
    const instructions = toCompile.filter(line => !this.isLabel(line))
    const baseOfData = instructions.length
    this.addAllocations(baseOfData, labels, toAllocate)
    const compiled = instructions.map(instr => this.compile(instr, labels))
    const program = this.instructionsToText(compiled)
    return program
  }
\end{lstlisting}



\begin{lstlisting}[frame=tblr]
  splitAllocations (lines) {
    const split = lines.indexOf(DIVIDER)
    if (split === -1) {
      return [lines, []]
    } else {
      return [lines.slice(0, split), lines.slice(split + 1)]
    }
  }
\end{lstlisting}


\figpdfhere{virtual-machine-storage-allocation}{./virtual-machine/storage-allocation.pdf}{Allocating storage for arrays in the virtual machine.}{0.6}


Second,
we need to figure out where each allocation lies and create a label accordingly:


\begin{lstlisting}[frame=tblr]
  addAllocations (baseOfData, labels, toAllocate) {
    toAllocate.forEach(alloc => {
      const fields = alloc.split(':').map(a => a.trim())
      assert(fields.length === 2,
        `Invalid allocation directive "${alloc}"`)
      const [label, numWordsText] = fields
      assert(!(label in labels),
        `Duplicate label "${label}" in data allocation`)
      const numWords = parseInt(numWordsText)
      assert((baseOfData + numWords) < RAM_LEN,
        `Allocation "${label}" requires too much memory`)
      labels[label] = baseOfData
      baseOfData += numWords
    })
  }
\end{lstlisting}



And that’s it:
no other changes are needed to either compilation or execution.
To test it,
let’s fill an array with the numbers from 0 to 3:


\begin{lstlisting}[frame=tblr]
# Count up to 3.
# - R0: loop index.
# - R1: loop limit.
# - R2: array index.
# - R3: temporary.
ldc R0 0
ldc R1 3
ldc R2 @array
loop:
str R0 R2
ldc R3 1
add R0 R3
add R2 R3
cpy R3 R1
sub R3 R0
bne R3 @loop
hlt
.data
array: 10
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
R0 = 3
R1 = 3
R2 = 14
R3 = 0
0:   00000002  00030102  000b0202  00020005
4:   00010302  00030006  00030206  00010304
8:   00000307  00030309  00000001  00000000
c:   00000001  00000002  00000000  00000000
\end{lstlisting}


\begin{callout}


\subsubsection*{How does it actually work?}


Our VM is just another program.
If you’d like to know what happens when instructions finally meet hardware,
and how electrical circuits are able to do arithmetic,
make decisions,
and talk to the world,
\cite{Patterson2017} has everything you want to know and more.

\end{callout}

\section{Exercises}\label{virtual-machine-exercises}

\subsection*{Swapping values}


Write an assembly language program that swaps the values in R1 and R2
without affecting the values in other registers.

\subsection*{Reversing an array}


Write an assembly language program that starts with:

\begin{itemize}

\item the base address of an array in one word

\item the length of the array N in the next word

\item N values immediately thereafter

\end{itemize}


\noindent and reverses the array in place.

\subsection*{Increment and decrement}

\begin{enumerate}

\item 

Add instructions \texttt{inc} and \texttt{dec} that add one to the value of a register
    and subtract one from the value of a register respectively.



\item 

Rewrite the examples to use these instructions.
    How much shorter do they make the programs?
    How much easier to read?



\end{enumerate}

\subsection*{Using long addresses}

\begin{enumerate}

\item 

Modify the virtual machine so that the \texttt{ldr} and \texttt{str} instructions
    contain 16-bit addresses rather than 8-bit addresses
    and increase the virtual machine’s memory to 64K words to match.



\item 

How does this complicate instruction interpretation?



\end{enumerate}

\subsection*{Operating on strings}


The C programming language stored character strings as non-zero bytes terminated by a byte containing zero.

\begin{enumerate}

\item 

Write a program that starts with the base address of a string in R1
    and finishes with the length of the string (not including the terminator) in the same register.



\item 

Write a program that starts with the base address of a string in R1
    and the base address of some other block of memory in R2
    and copies the string to that new location (including the terminator).



\item 

What happens in each case if the terminator is missing?



\end{enumerate}

\subsection*{Call and return}

\begin{enumerate}

\item 

Add another register to the virtual machine called SP (for “stack pointer”)
    that is automatically initialized to the \emph{last} address in memory.



\item 

Add an instruction \texttt{psh} (short for “push”) that copies a value from a register
    to the address stored in SP and then subtracts one from SP.



\item 

Add an instruction \texttt{pop} (short for “pop”) that adds one to SP
    and then copies a value from that address into a register.



\item 

Using these instructions,
    write a subroutine that evaluates \texttt{2x+1} for every value in an array.



\end{enumerate}

\subsection*{Disassembling instructions}


A \glossref{disassembler} turns machine instructions into assembly code.
Write a disassembler for the instruction set used by our virtual machine.
(Since the labels for addresses are not stored in machine instructions,
disassemblers typically generate labels like \texttt{@L001} and \texttt{@L002}.)

\subsection*{Linking multiple files}

\begin{enumerate}

\item 

Modify the assembler to handle \texttt{.include filename} directives.



\item 

What does your modified assembler do about duplicate label names?
    How does it prevent infinite includes
    (i.e., \texttt{A.as} includes \texttt{B.as} which includes \texttt{A.as} again)?



\end{enumerate}

\subsection*{Providing system calls}


Modify the virtual machine so that developers can add “system calls” to it.

\begin{enumerate}

\item 

On startup,
    the virtual machine loads an array of functions defined in a file called \texttt{syscalls.js}.



\item 

The \texttt{sys} instruction takes a one-byte constant argument.
    It looks up the corresponding function and calls it with the values of R0-R3 as parameters
    and places the result in R0.



\end{enumerate}

\subsection*{Unit testing}

\begin{enumerate}

\item 

Write unit tests for the assembler.



\item 

Once they are working,
    write unit tests for the virtual machine.



\end{enumerate}

\chapter{Debugger}\label{debugger}


\noindent 
  Terms defined: \glossref{breakpoint}, \glossref{source map}, \glossref{tab completion}, \glossref{watchpoint}



We have finally come to another of the topics that sparked this book:
how does a debugger\index{debugger} work?
Debuggers are as much a part of programmers’ lives as version control
but are taught far less often
(in part, we believe, because it’s harder to create homework questions for them).
In this chapter we will build a simple single-stepping debugger
and show one way to test interactive applications (\chapref{unit-test}).

\section{What is our starting point?}\label{debugger-start}


We would like to debug a higher-level language than the assembly code\index{assembly code} of \chapref{virtual-machine},
but we don’t want to have to write a parser
or wrestle with the ASTs\index{abstract syntax tree} of \chapref{style-checker}.
As a compromise,
we will represent programs as JSON data structures
whose elements have the form \texttt{[command ...args]}.
If the JavaScript version of our program is:


\begin{lstlisting}[frame=tblr]
const a = [-3, -5, -1, 0, -2, 1, 3, 1]
const b = Array()
let largest = a[0]
let i = 0
while (i < length(a)) {
  if (a[i] > largest) {
    b.push(a[i])
  }
  i += 1
}
i = 0
while (i < length(b)) {
  console.log(b[i])
  i += 1
}
\end{lstlisting}



\noindent then the JSON representation is:


\begin{lstlisting}[frame=tblr]
[
  ["defA", "a", ["data", -3, -5, -1, 0, -2, 1, 3, 1]],
  ["defA", "b", ["data"]],
  ["defV", "largest", ["getA", "a", ["num", 0]]],
  ["append", "b", ["getV", "largest"]],
  ["defV", "i", ["num", 0]],
  ["loop", ["lt", ["getV", "i"], ["len", "a"]],
    ["test", ["gt", ["getA", "a", ["getV", "i"]], ["getV", "largest"]],
      ["setV", "largest", ["getA", "a", ["getV", "i"]]],
      ["append", "b", ["getV", "largest"]]
    ],
    ["setV", "i", ["add", ["getV", "i"], ["num", 1]]]
  ],
  ["setV", "i", ["num", 0]],
  ["loop", ["lt", ["getV", "i"], ["len", "b"]],
    ["print", ["getA", "b", ["getV", "i"]]],
    ["setV", "i", ["add", ["getV", "i"], ["num", 1]]]
  ]
]
\end{lstlisting}



Our virtual machine\index{virtual machine} is structured like the one in \chapref{virtual-machine}.
A real system would parse a program to create JSON,
then translate JSON into assembly code,
then assemble that to create machine instructions.
Again,
to keep things simple we will execute a program by
removing comments and blank lines
and then running commands by looking up the command name’s and calling that method:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

class VirtualMachineBase {
  constructor (program) {
    this.program = this.compile(program)
    this.prefix = '>>'
  }

  compile (lines) {
    const text = lines
      .map(line => line.trim())
      .filter(line => (line.length > 0) && !line.startsWith('//'))
      .join('\n')
    return JSON.parse(text)
  }

  run () {
    this.env = {}
    this.runAll(this.program)
  }

  runAll (commands) {
    commands.forEach(command => this.exec(command))
  }

  exec (command) {
    const [op, ...args] = command
    assert(op in this,
      `Unknown op "${op}"`)
    return this[op](args)
  }

}

export default VirtualMachineBase
\end{lstlisting}



\noindent Remember, functions and methods are just another kind of data,
so if an object has a method called \texttt{"meth"},
the expression \texttt{this["meth"]} looks it up
and \texttt{this["meth"](args)} calls it.
If \texttt{"meth"} is stored in a variable called \texttt{name},
then \texttt{this[name](args)} will do exactly the same thing.


The method in our VM that defines a new variable with an initial value looks like this:


\begin{lstlisting}[frame=tblr]
  defV (args) {
    this.checkOp('defV', 2, args)
    const [name, value] = args
    this.env[name] = this.exec(value)
  }
\end{lstlisting}



\noindent while the one that adds two values looks like this:


\begin{lstlisting}[frame=tblr]
  add (args) {
    this.checkOp('add', 2, args)
    const left = this.exec(args[0])
    const right = this.exec(args[1])
    return left + right
  }
\end{lstlisting}



Running a \texttt{while} loop is:


\begin{lstlisting}[frame=tblr]
  loop (args) {
    this.checkBody('loop', 1, args)
    const body = args.slice(1)
    while (this.exec(args[0])) {
      this.runAll(body)
    }
  }
\end{lstlisting}



\noindent and checking that a variable name refers to an array is:


\begin{lstlisting}[frame=tblr]
  checkArray (op, name) {
    this.checkName(op, name)
    const array = this.env[name]
    assert(Array.isArray(array),
      `Variable "${name}" used in "${op}" is not array`)
  }
\end{lstlisting}



The other operations are similar to these.

\section{How can we make a tracing debugger?}\label{debugger-tracing}


The next thing we need in our debugger is
a \glossref{source map}\index{source map}\index{debugger!source map} that keeps track of
where in the source file each instruction came from.
Since JSON is a subset of JavaScript,
we could get line numbers by parsing our programs with \hreffoot{Acorn}{https://github.com/acornjs/acorn}\index{Acorn}.
However,
we would then have to scrape the information we want for this example out of the AST.
Since this chapter is supposed to be about debugging,
not parsing,
we will instead cheat and add a line number to each interesting statement by hand
so that our program looks like this:


\begin{lstlisting}[frame=tblr]
[
  [1, "defA", "a", ["data", -3, -5, -1, 0, -2, 1, 3, 1]],
  [2, "defA", "b", ["data"]],
  [3, "defV", "largest", ["getA", "a", ["num", 0]]],
  [4, "append", "b", ["getV", "largest"]],
  [5, "defV", "i", ["num", 0]],
  [6, "loop", ["lt", ["getV", "i"], ["len", "a"]],
   [7, "test", ["gt", ["getA", "a", ["getV", "i"]], ["getV", "largest"]],
    [8, "setV", "largest", ["getA", "a", ["getV", "i"]]],
    [9, "append", "b", ["getV", "largest"]]
   ],
   [11, "setV", "i", ["add", ["getV", "i"], ["num", 1]]]
  ],
  [13, "setV", "i", ["num", 0]],
  [14, "loop", ["lt", ["getV", "i"], ["len", "b"]],
   [15, "print", ["getA", "b", ["getV", "i"]]],
   [16, "setV", "i", ["add", ["getV", "i"], ["num", 1]]]
  ]
]
\end{lstlisting}



Building the source map from that is simple;
for now,
we just modify \texttt{exec} to ignore the line number:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import VirtualMachineBase from './vm-base.js'

class VirtualMachineSourceMap extends VirtualMachineBase {
  compile (lines) {
    const original = super.compile(lines)
    this.sourceMap = {}
    const result = original.map(command => this.transform(command))
    return result
  }

  transform (node) {
    if (!Array.isArray(node)) {
      return node
    }
    if (Array.length === 0) {
      return []
    }
    const [first, ...rest] = node
    if (typeof first !== 'number') {
      return [first, null, ...rest.map(arg => this.transform(arg))]
    }
    const [op, ...args] = rest
    this.sourceMap[first] =
      [op, first, ...args.map(arg => this.transform(arg))]
    return this.sourceMap[first]
  }

  exec (command) {
    const [op, lineNum, ...args] = command
    assert(op in this,
      `Unknown op "${op}"`)
    return this[op](args)
  }
}

export default VirtualMachineSourceMap
\end{lstlisting}


\begin{callout}


\subsubsection*{It’s not really cheating}


We said that adding line numbers by hand was cheating,
but it isn’t.
What we’re actually doing is deferring a problem until we’re sure we need to solve it.
If our approach is clumsy or fails outright because of some aspect of design we didn’t foresee,
there will have been no point handling line numbers the “right” way.
A good rule for software design\index{software design!deferring problems}
is to tackle the thing you’re least sure about first,
using temporary code in place of what you think you’ll eventually need.

\end{callout}


The next step is to modify the VM’s \texttt{exec} method
so that it executes a callback function for each significant operation
(where “significant” means “we bothered to record its line number”).
Since we’re not sure what our debugger is going to need,
we give this callback the environment holding the current set of variables,
the line number,
and the operation being performed:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import VirtualMachineSourceMap from './vm-source-map.js'

class VirtualMachineCallback extends VirtualMachineSourceMap {
  constructor (program, dbg) {
    super(program)
    this.dbg = dbg
    this.dbg.setVM(this)
  }

  exec (command) {
    const [op, lineNum, ...args] = command
    this.dbg.handle(this.env, lineNum, op)
    assert(op in this,
      `Unknown op "${op}"`)
    return this[op](args, lineNum)
  }

  message (prefix, val) {
    this.dbg.message(`${prefix} ${val}`)
  }
}

export default VirtualMachineCallback
\end{lstlisting}



We also modify the VM’s constructor to record the debugger and give it a reference to the virtual machine
(\figref{debugger-initialization}).
We have to connect the two objects explicitly\index{mutual references}
because each one needs a reference to the other,
but one of them has to be created first.
“A gets B then B tells A about itself” is a common pattern;
we will look at other ways to manage it in the exercises.

\figpdf{debugger-initialization}{./debugger/initialization.pdf}{Two-step initialization of mutually-dependent objects.}{0.6}


To run the program,
we create a debugger object and pass it to the VM’s constructor:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

import readSource from './read-source.js'

const main = () => {
  assert(process.argv.length === 5,
    'Usage: run-debugger.js ./vm ./debugger input|-')
  const VM = require(process.argv[2])
  const Debugger = require(process.argv[3])
  const inFile = process.argv[4]
  const lines = readSource(inFile)
  const dbg = new Debugger()
  const vm = new VM(lines, dbg)
  vm.run()
}

main()
\end{lstlisting}



A simple debugger just traces interesting statements as they run:


\begin{lstlisting}[frame=tblr]
import DebuggerBase from './debugger-base.js'

class DebuggerTrace extends DebuggerBase {
  handle (env, lineNum, op) {
    if (lineNum !== null) {
      console.log(`${lineNum} / ${op}: ${JSON.stringify(env)}`)
    }
  }
}

export default DebuggerTrace
\end{lstlisting}



Let’s try it on a program that adds the numbers in an array:


\begin{lstlisting}[frame=tblr]
// const a = [-5, 1, 3]
// const total = 0
// let i = 0
// while (i < length(a)) {
//   total += a[i]
//   i += 1
// }
// console.log(total)

[
  [1, "defA", "a", ["data", -5, 1, 3]],
  [2, "defV", "total", ["num", 0]],
  [3, "defV", "i", ["num", 0]],
  [4, "loop", ["lt", ["getV", "i"], ["len", "a"]],
    [5, "setV", "total",
      ["add", ["getV", "total"], ["getA", "a", ["getV", "i"]]]
    ],
    [8, "setV", "i", ["add", ["getV", "i"], ["num", 1]]]
  ],
  [10, "print", ["getV", "total"]]
]
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
1 / defA: {}
2 / defV: {"a":[-5,1,3]}
3 / defV: {"a":[-5,1,3],"total":0}
4 / loop: {"a":[-5,1,3],"total":0,"i":0}
5 / setV: {"a":[-5,1,3],"total":0,"i":0}
8 / setV: {"a":[-5,1,3],"total":-5,"i":0}
5 / setV: {"a":[-5,1,3],"total":-5,"i":1}
8 / setV: {"a":[-5,1,3],"total":-4,"i":1}
5 / setV: {"a":[-5,1,3],"total":-4,"i":2}
8 / setV: {"a":[-5,1,3],"total":-1,"i":2}
10 / print: {"a":[-5,1,3],"total":-1,"i":3}
>> -1
\end{lstlisting}


\section{How can we make the debugger interactive?}\label{debugger-interactive}


What we have built so far is an always-on \texttt{print} statement.
To turn it into an interactive debugger,
we will use the \hreffoot{\texttt{prompt-sync}}{https://www.npmjs.com/package/prompt-sync} module to manage user input
with the following set of commands:

\begin{itemize}

\item 

\texttt{?} or \texttt{help} to list commands.



\item 

\texttt{clear \#} to clear a \glossref{breakpoint} at a numbered line.



\item 

\texttt{list} to list lines and breakpoints.



\item 

\texttt{next} to go forward one line.



\item 

\texttt{print name} to show a variable while at a breakpoint.



\item 

\texttt{run} to run to the next breakpoint.



\item 

\texttt{stop \#} to break at a numbered line.



\item 

\texttt{variables} to list all variable names.



\item 

\texttt{exit} to exit immediately.



\end{itemize}


When the virtual machine calls the debugger,
the debugger first checks if it is on a numbered line.
If it isn’t,
it hands control back to the VM.
Otherwise,
if we are single-stepping or this line is a breakpoint,
the debugger takes over.
Its overall structure is:


\begin{lstlisting}[frame=tblr]
import prompt from 'prompt-sync'

import DebuggerBase from './debugger-base.js'

const PROMPT_OPTIONS = { sigint: true }

class DebuggerInteractive extends DebuggerBase {
  constructor () {
    super()
    this.singleStep = true
    this.breakpoints = new Set()
    this.lookup = {
      '?': 'help',
      c: 'clear',
      l: 'list',
      n: 'next',
      p: 'print',
      r: 'run',
      s: 'stop',
      v: 'variables',
      x: 'exit'
    }
  }

  handle (env, lineNum, op) {
    if (lineNum === null) {
      return
    }
    if (this.singleStep) {
      this.singleStep = false
      this.interact(env, lineNum, op)
    } else if (this.breakpoints.has(lineNum)) {
      this.interact(env, lineNum, op)
    }
  }

}

export default DebuggerInteractive
\end{lstlisting}



\noindent It interacts with users by lookup up a command and invoking the corresponding method,
just as the VM does:


\begin{lstlisting}[frame=tblr]
  interact (env, lineNum, op) {
    let interacting = true
    while (interacting) {
      const command = this.getCommand(env, lineNum, op)
      if (command.length === 0) {
        continue
      }
      const [cmd, ...args] = command
      if (cmd in this) {
        interacting = this[cmd](env, lineNum, op, args)
      } else if (cmd in this.lookup) {
        interacting = this[this.lookup[cmd]](env, lineNum, op, args)
      } else {
        this.message(`unknown command ${command} (use '?' for help)`)
      }
    }
  }

  getCommand (env, lineNum, op) {
    const options = Object.keys(this.lookup).sort().join('')
    const display = `[${lineNum} ${options}] `
    return this.input(display)
      .split(/\s+/)
      .map(s => s.trim())
      .filter(s => s.length > 0)
  }

  input (display) {
    return prompt(PROMPT_OPTIONS)(display)
  }
\end{lstlisting}


\begin{callout}


\subsubsection*{Learning as we go}


We didn’t originally put the input and output in methods that could be overridden,
but realized later we needed to do this to make the debugger testable.
Rather than coming back and rewriting this,
we have done it here.

\end{callout}


With this structure in place,
the command handlers are pretty straightforward.
For example,
this method moves us to the next line:


\begin{lstlisting}[frame=tblr]
  next (env, lineNum, op, args) {
    this.singleStep = true
    return false
  }
\end{lstlisting}



\noindent while this one prints the value of a variable:


\begin{lstlisting}[frame=tblr]
  print (env, lineNum, op, args) {
    if (args.length !== 1) {
      this.message('p[rint] requires one variable name')
    } else if (!(args[0] in env)) {
      this.message(`unknown variable name "${args[0]}"`)
    } else {
      this.message(JSON.stringify(env[args[0]]))
    }
    return true
  }
\end{lstlisting}



After using this for a few moments,
though
we realized that we needed to change the signature of the \texttt{loop} method.
We want to stop the loop each time it runs,
and need to know where we are.
We didn’t allow for this in the base class,
and we don’t want to have to change every method,
so we take advantage of the fact that JavaScript ignores any extra arguments passed to a method:


\begin{lstlisting}[frame=tblr]
import VirtualMachineCallback from './vm-callback.js'

class VirtualMachineInteractive extends VirtualMachineCallback {
  loop (args, lineNum) {
    this.checkBody('loop', 1, args)
    const body = args.slice(1)
    while (this.exec(args[0])) {
      this.dbg.handle(this.env, lineNum, 'loop')
      this.runAll(body)
    }
  }
}

export default VirtualMachineInteractive
\end{lstlisting}



\noindent This is sloppy, but it works;
we will tidy it up in the exercises.

\section{How can we test an interactive application?}\label{debugger-test}


How can we test\index{unit test!interactive application} an interactive application like a debugger?
The answer is, “By making it non-interactive.”
Like many tools over the past 30 years,
our approach is based on a program called \hreffoot{Expect}{https://en.wikipedia.org/wiki/Expect}\index{Expect}.
Our library replaces the input and output functions of the application being tested with callbacks,
then provides input when asked and checks output when it is given
(\figref{debugger-test-interact}).

\figpdf{debugger-test-interact}{./debugger/test-interact.pdf}{Replacing input and output to test interactive applications.}{0.6}


The results look like this:


\begin{lstlisting}[frame=tblr]
describe('interactive debugger', () => {
  it('runs and prints', (done) => {
    setup('print-0.json')
      .get('[1 ?clnprsvx] ')
      .send('r')
      .get('>> 0')
      .run()
    done()
  })

  it('breaks and resumes', (done) => {
    setup('print-3.json')
      .get('[1 ?clnprsvx] ')
      .send('s 3')
      .get('[1 ?clnprsvx] ')
      .send('r')
      .get('>> 0')
      .get('>> 1')
      .get('[3 ?clnprsvx] ')
      .send('x')
      .run()
    done()
  })
})
\end{lstlisting}



Our \texttt{Expect} class may be short,
but it is hard to understand because it is so abstract:


\begin{lstlisting}[frame=tblr]
import assert from 'assert'

class Expect {
  constructor (subject, start) {
    this.start = start
    this.steps = []
    subject.setTester(this)
  }

  send (text) {
    this.steps.push({ op: 'toSystem', arg: text })
    return this
  }

  get (text) {
    this.steps.push({ op: 'fromSystem', arg: text })
    return this
  }

  run () {
    this.start()
    assert.strictEqual(this.steps.length, 0,
      'Extra steps at end of test')
  }

  toSystem () {
    return this.next('toSystem')
  }

  fromSystem (actual) {
    const expected = this.next('fromSystem')
    assert.strictEqual(expected, actual,
      `Expected "${expected}" got "${actual}"`)
  }

  next (kind) {
    assert(this.steps.length > 0,
      'Unexpected end of steps')
    assert.strictEqual(this.steps[0].op, kind,
      `Expected ${kind}, got "${this.steps[0].op}"`)
    const text = this.steps[0].arg
    this.steps = this.steps.slice(1)
    return text
  }
}

export default Expect
\end{lstlisting}



\noindent Piece-by-piece:

\begin{itemize}

\item \texttt{subject} is the thing being tested.

\item \texttt{start} is a callback to start the system running.
    It gives control to the subject,
    which then calls back into the test framework for input and output.

\item \texttt{get} and \texttt{send} store things to be given to the subject
    and to be checked against its output.
    Both methods return \texttt{this} so that we can chain calls together.

\item \texttt{run} starts the system
    and checks that all expected interactions have been used up when testing is done.

\item \texttt{toSystem} and \texttt{fromSystem} use \texttt{next} to get the next test record,
    check its type,
    and return the string.

\end{itemize}


Let’s modify the debugger to use the tester,
keeping in mind that the prompt counts as an output
(and yes, we forgot this in the first version):


\begin{lstlisting}[frame=tblr]
import DebuggerInteractive from './debugger-interactive.js'

class DebuggerTest extends DebuggerInteractive {
  constructor () {
    super()
    this.tester = null
  }

  setTester (tester) {
    this.tester = tester
  }

  input (display) {
    this.tester.fromSystem(display)
    return this.tester.toSystem()
  }

  message (m) {
    this.tester.fromSystem(m)
  }
}

export default DebuggerTest
\end{lstlisting}



Again,
we can’t pass the tester as a constructor parameter because of initialization order,
so we write a \texttt{setup} function to make sure everything is connected the right way:


\begin{lstlisting}[frame=tblr]
import Expect from '../expect.js'
import VM from '../vm-interactive.js'
import Debugger from '../debugger-test.js'
import readSource from '../read-source.js'

const setup = (filename) => {
  const lines = readSource(path.join('debugger/test', filename))
  const dbg = new Debugger()
  const vm = new VM(lines, dbg)
  return new Expect(dbg, () => vm.run())
}
\end{lstlisting}



Let’s try running our tests:


\begin{lstlisting}[frame=shadowbox]
npm run test -- -g 'interactive debugger'
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test /u/stjs
> mocha */test/test-*.js "-g" "interactive debugger"



  interactive debugger
    ✓ runs and prints
\end{lstlisting}



That works—or does it?
Why is only one test shown,
and doesn’t the summary appear?
After a bit of digging,
we realize that the debugger’s \texttt{exit} command calls \texttt{process.exit} when the simulated program ends,
so the whole program including the VM, debugger, and test framework stops immediately
\emph{before} the promises that contain the tests have run.


We could fix this by modifying the debugger callback
to return an indication of whether or not execution should continue,
then modify the VM to pay attention to that flag.
However,
this approach becomes very complicated when we have deeply-nested calls to \texttt{exec},
which will happen with loops and conditionals.


A better alternative is to use an exception for control flow\index{exception!for control flow}.
We can define our own kind of exception as an empty class:
it doesn’t need any data
because we are only using it to get a typed object:


\begin{lstlisting}[frame=tblr]
class HaltException {
}

export default HaltException
\end{lstlisting}



\noindent Next,
we modify the debugger to throw this exception when asked to exit:


\begin{lstlisting}[frame=tblr]
import HaltException from './halt-exception.js'
import DebuggerTest from './debugger-test.js'

class DebuggerExit extends DebuggerTest {
  exit (env, lineNum, op, args) {
    throw new HaltException()
  }
}

export default DebuggerExit
\end{lstlisting}



\noindent And finally
we modify the VM to finish cleanly if this exception is thrown,
but re-throw any other kind of exception:


\begin{lstlisting}[frame=tblr]
import HaltException from './halt-exception.js'
import VirtualMachineInteractive from './vm-interactive.js'

class VirtualMachineExit extends VirtualMachineInteractive {
  run () {
    this.env = {}
    try {
      this.runAll(this.program)
    } catch (exc) {
      if (exc instanceof HaltException) {
        return
      }
      throw exc
    }
  }
}

export default VirtualMachineExit
\end{lstlisting}



With these changes in place,
we are finally able to test our interactive debugger:


\begin{lstlisting}[frame=shadowbox]
npm run test -- -g 'exitable debugger'
\end{lstlisting}



\begin{lstlisting}[frame=tblr,backgroundcolor=\color{black!5}]
> stjs@1.0.0 test /u/stjs
> mocha */test/test-*.js "-g" "exitable debugger"



  exitable debugger
    ✓ runs and prints
    ✓ breaks and resumes


  2 passing (7ms)
\end{lstlisting}


\section{Exercises}\label{debugger-exercises}

\subsection*{Implementing tab completion}


Read the documentation for \texttt{prompt-sync}
and then implement \glossref{tab completion}
for the debugger.

\subsection*{Modifying variables while running}


Add a \texttt{set} command that sets the value of a variable to a new value in a running program.
How do you handle setting array elements?

\subsection*{Making output more readable}


Modify the tracing debugger so that
the statements inside loops and conditionals are indented for easier reading.

\subsection*{Better loops}


Our solution for handling loops is sloppy; fix it.

\subsection*{Using a flag to continue execution}


Modify the debugger and virtual machine to use a “continue executing” flag
rather than throwing an exception when execution should end.
Which approach is easier to understand?
Which will be easier to extend in future?

\subsection*{Numbering lines}


Write a tool that takes a JSON program representation \emph{without} statement numbers
and produces one that numbers all of the interesting statements for debugging purposes.
Use whatever definition of “interesting” you think would be most useful.

\subsection*{Looping around again}


Implement a “next loop iteration” command that runs the program
until it reaches the current point in the next iteration of the current loop.

\subsection*{Looking up objects}


Rather than having some objects call \texttt{setXYZ} methods in other objects,
it is common practice to use a lookup table for mutual dependencies:

\begin{enumerate}

\item 

Every object initializes calls \texttt{table.set(name, this)} in its constructor.



\item 

Whenever object A needs the instance of object B,
    it calls \texttt{table.lookup({\textquotesingle}B{\textquotesingle})}.
    It does \emph{not} store the result in a member variable.



\end{enumerate}


Modify the virtual machine and debugger to use this pattern.

\subsection*{Watching for variable changes}


Modify the debugger and virtual machine to implement \glossref{watchpoints}
that halt the program whenever the value of a variable changes.

\subsection*{Translating JSON to assembler}


Write a tool that translates the JSON program representation
into the assembly code of \chapref{virtual-machine}.
To simplify things,
increase the number of registers so that
there is always storage for intermediate results
when doing arithmetic.

\chapter{Conclusion}\label{conclusion}


We have come a long way since we listed the contents of a directory in \chapref{systems-programming}.
Saving files in version control,
making sure code meets style rules,
debugging it and bundling it (hopefully in that order)—programmers do these things every day,
and we hope that understanding how they work will help you do them better.


We also hope that your journey won’t stop here.
If you would like to add a chapter to this book
or translate it into another programming language,
human language,
or both,
your help would be very welcome:
please see the introduction in \chapref{introduction}
and the contributors’ guide in \appref{contributing} for more information.

\begin{quotation}


We shape our tools, and thereafter our tools shape us.


— Marshall McLuhan

\end{quotation}


\appendix
\chapter{License}\label{license}
\markboth{\thechapter\  License}{\thechapter\  License}

All of the written material is made available under the Creative
Commons - Attribution - NonCommercial 4.0 International license (CC-BY-NC-4.0),
while the software is made available under the Hippocratic License.

\section*{Writing}


\emph{This is a human-readable summary of (and not a substitute for) the license.
For the full legal text of this license, please see
\hreffoot{https://creativecommons.org/licenses/by-nc/4.0/legalcode}{https://creativecommons.org/licenses/by-nc/4.0/legalcode}.}


All of this site is made available under the terms of the Creative Commons
Attribution - NonCommercial 4.0 license. You are free to:

\begin{itemize}

\item 

\textbf{Share} — copy and redistribute the material in any medium or format



\item 

\textbf{Adapt} — remix, transform, and build upon the material



\item 

The licensor cannot revoke these freedoms as long as you follow the license
    terms.



\end{itemize}


\noindent Under the following terms:

\begin{itemize}

\item 

\textbf{Attribution} — You must give appropriate credit, provide a link to the
    license, and indicate if changes were made. You may do so in any reasonable
    manner, but not in any way that suggests the licensor endorses you or your
    use.



\item 

\textbf{NonCommercial} — You may not use the material for commercial purposes.



\item 

\textbf{No additional restrictions} — You may not apply legal terms or technological
    measures that legally restrict others from doing anything the license
    permits.



\end{itemize}


\noindent \textbf{Notices:}


You do not have to comply with the license for elements of the material in the
public domain or where your use is permitted by an applicable exception or
limitation.


No warranties are given. The license may not give you all of the permissions
necessary for your intended use. For example, other rights such as publicity,
privacy, or moral rights may limit how you use the material.

\section*{Software}


Licensor hereby grants permission by this license (“License”), free of charge,
to any person or entity (the “Licensee”) obtaining a copy of this software and
associated documentation files (the “Software”), to deal in the Software without
restriction, including without limitation the rights to use, copy, modify,
merge, publish, distribute, sublicense, and/or sell copies of the Software, and
to permit persons to whom the Software is furnished to do so, subject to the
following conditions:

\begin{itemize}

\item 

The above copyright notice and this License or a subsequent version published
    on the [Hippocratic License Website][hippocratic-license] shall be
    included in all copies or substantial portions of the Software. Licensee has
    the option of following the terms and conditions either of the above
    numbered version of this License or of any subsequent version published on
    the Hippocratic License Website.



\item 

Compliance with Human Rights Laws and Human Rights Principles:

\begin{enumerate}

\item 

Human Rights Laws. The Software shall not be used by any person or
    entity for any systems, activities, or other uses that violate any
    applicable laws, regulations, or rules that protect human, civil, labor,
    privacy, political, environmental, security, economic, due process, or
    similar rights (the “Human Rights Laws”). Where the Human Rights Laws of
    more than one jurisdiction are applicable to the use of the Software,
    the Human Rights Laws that are most protective of the individuals or
    groups harmed shall apply.



\item 

Human Rights Principles. Licensee is advised to consult the articles of
    the \hreffoot{United Nations Universal Declaration of Human Rights}{https://www.un.org/en/universal-declaration-human-rights/} and the
    \hreffoot{United Nations Global Compact}{https://www.unglobalcompact.org/what-is-gc/mission/principles} that define recognized principles
    of international human rights (the “Human Rights Principles”). It is
    Licensor’s express intent that all use of the Software be consistent
    with Human Rights Principles. If Licensor receives notification or
    otherwise learns of an alleged violation of any Human Rights Principles
    relating to Licensee’s use of the Software, Licensor may in its
    discretion and without obligation (i) (a) notify Licensee of such
    allegation and (b) allow Licensee 90 days from notification under (i)(a)
    to investigate and respond to Licensor regarding the allegation and (ii)
    (a) after the earlier of 90 days from notification under (i)(a), or
    Licensee’s response under (i)(b), notify Licensee of License termination
    and (b) allow Licensee an additional 90 days from notification under
    (ii)(a) to cease use of the Software.



\item 

Indemnity. Licensee shall hold harmless and indemnify Licensor against
    all losses, damages, liabilities, deficiencies, claims, actions,
    judgments, settlements, interest, awards, penalties, fines, costs, or
    expenses of whatever kind, including Licensor’s reasonable attorneys’
    fees, arising out of or relating to Licensee’s non-compliance with this
    License or use of the Software in violation of Human Rights Laws or
    Human Rights Principles.



\end{enumerate}



\item 

Enforceability: If any portion or provision of this License is determined to
     be invalid, illegal, or unenforceable by a court of competent jurisdiction,
     then such invalidity, illegality, or unenforceability shall not affect any
     other term or provision of this License or invalidate or render
     unenforceable such term or provision in any other jurisdiction. Upon a
     determination that any term or provision is invalid, illegal, or
     unenforceable, to the extent permitted by applicable law, the court may
     modify this License to affect the original intent of the parties as closely
     as possible. The section headings are for convenience only and are not
     intended to affect the construction or interpretation of this License. Any
     rule of construction to the effect that ambiguities are to be resolved
     against the drafting party shall not apply in interpreting this
     License. The language in this License shall be interpreted as to its fair
     meaning and not strictly for or against any party.



\end{itemize}


THE SOFTWARE IS PROVIDED “AS IS”, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS
FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR
COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER
IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.


\emph{The Hippocratic License is an \hreffoot{Ethical Source license}{https://ethicalsource.dev}.}

\chapter{Code of Conduct}\label{conduct}
\markboth{\thechapter\  Code of Conduct}{\thechapter\  Code of Conduct}

In the interest of fostering an open and welcoming environment, we as
contributors and maintainers pledge to making participation in our project and
our community a harassment-free experience for everyone, regardless of age, body
size, disability, ethnicity, gender identity and expression, level of
experience, education, socioeconomic status, nationality, personal appearance,
race, religion, or sexual identity and orientation.

\section*{Our Standards}


Examples of behavior that contributes to creating a positive environment
include:

\begin{itemize}

\item using welcoming and inclusive language,

\item being respectful of differing viewpoints and experiences,

\item gracefully accepting constructive criticism,

\item focusing on what is best for the community, and

\item showing empathy towards other community members.

\end{itemize}


\noindent Examples of unacceptable behavior by participants include:

\begin{itemize}

\item the use of sexualized language or imagery and unwelcome sexual
  attention or advances,

\item trolling, insulting/derogatory comments, and personal or political
  attacks,

\item public or private harassment,

\item publishing others’ private information, such as a physical or
  electronic address, without explicit permission, and

\item other conduct which could reasonably be considered inappropriate in
  a professional setting

\end{itemize}

\section*{Our Responsibilities}


Project maintainers are responsible for clarifying the standards of acceptable
behavior and are expected to take appropriate and fair corrective action in
response to any instances of unacceptable behavior.


Project maintainers have the right and responsibility to remove, edit, or reject
comments, commits, code, wiki edits, issues, and other contributions that are
not aligned to this Code of Conduct, or to ban temporarily or permanently any
contributor for other behaviors that they deem inappropriate, threatening,
offensive, or harmful.

\section*{Scope}


This Code of Conduct applies both within project spaces and in public spaces
when an individual is representing the project or its community. Examples of
representing a project or community include using an official project email
address, posting via an official social media account, or acting as an appointed
representative at an online or offline event. Representation of a project may be
further defined and clarified by project maintainers.

\section*{Enforcement}


Instances of abusive, harassing, or otherwise unacceptable behavior may be
reported by emailing the project team. All complaints will be reviewed
and investigated and will result in a response that is deemed necessary and
appropriate to the circumstances. The project team is obligated to maintain
confidentiality with regard to the reporter of an incident.  Further details of
specific enforcement policies may be posted separately.


Project maintainers who do not follow or enforce the Code of Conduct in good
faith may face temporary or permanent repercussions as determined by other
members of the project’s leadership.

\section*{Attribution}


This Code of Conduct is adapted from the \hreffoot{Contributor Covenant}{https://www.contributor-covenant.org/}
version 1.4.

\chapter{Contributing}\label{contributing}
\markboth{\thechapter\  Contributing}{\thechapter\  Contributing}

Contributions are very welcome;
please contact us by email or by filing an issue on this site.
All contributors must abide by our Code of Conduct.

\section*{Making Decisions}


This project uses \hreffoot{Martha’s Rules}{https://journals.sagepub.com/doi/10.1177/088610998600100206} for consensus decision making:

\begin{enumerate}

\item 

Before each meeting, anyone who wishes may sponsor a proposal by filing an
    issue in the GitHub repository tagged “proposal”.  People must file proposals
    at least 24 hours before a meeting in order for them to be considered at that
    meeting, and must include:

\begin{itemize}

\item a one-line summary (the subject line of the issue)

\item the full text of the proposal

\item any required background information

\item pros and cons

\item possible alternatives

\end{itemize}



\item 

A quorum is established in a meeting if half or more of voting members are
    present.



\item 

Once a person has sponsored a proposal, they are responsible for it.  The
    group may not discuss or vote on the issue unless the sponsor or their
    delegate is present.  The sponsor is also responsible for presenting the
    item to the group.



\item 

After the sponsor presents the proposal, a “sense” vote is cast for the
    proposal before any discussion:

\begin{itemize}

\item Who likes the proposal?

\item Who can live with the proposal?

\item Who is uncomfortable with the proposal?

\end{itemize}



\item 

If everyone likes or can live with the proposal, it passes immediately.



\item 

If most of the group is uncomfortable with the proposal, it is postponed for
    further rework by the sponsor.



\item 

Otherwise, members who are uncomfortable can briefly state their objections.
    A timer is then set for a brief discussion moderated by the facilitator.
    After 10 minutes or when no one has anything further to add (whichever comes
    first), the facilitator calls for a yes-or-no vote on the question: “Should
    we implement this decision over the stated objections?”  If a majority votes
    “yes” the proposal is implemented.  Otherwise, the proposal is returned to
    the sponsor for further work.



\end{enumerate}

\section*{Formatting}


This material uses \hreffoot{Ivy}{https://www.dmulholl.com/docs/ivy/dev/} with some custom extensions.
Run \texttt{make} in the root directory to get a list of available commands.
Some of these rely on scripts in the \texttt{./bin/} directory.
Please see our Git repository for up-to-date instructions.

\chapter{Bibliography}\label{bibliography}
\markboth{\thechapter\  Bibliography}{\thechapter\  Bibliography}
\printbibliography[heading=none]

\chapter{Glossary}\label{glossary}
\markboth{\thechapter\  Glossary}{\thechapter\  Glossary}


\noindent \textbf{{\newline}\glosskey{absolute error}}: 
The absolute value of the difference between the observed and the correct value. Absolute error is usually less useful than \glosskey{relative error}.


\noindent \textbf{{\newline}\glosskey{absolute path}}: 
A path that points to the same location in the \glosskey{filesystem} regardless of where it is evaluated. An absolute path is the equivalent of latitude and longitude in geography.


\noindent \textbf{{\newline}\glosskey{abstract method}}: 
In \glosskey{object-oriented programming}, a \glosskey{method} that is defined but not implemented. Programmers will define an abstract method in a \glosskey{parent class} to specify operations that \glosskey{child classes} must provide.


\noindent \textbf{{\newline}\glosskey{abstract syntax tree} (AST)}: 
A deeply nested data structure, or \glosskey{tree}, that represents the structure of a program. For example, the AST might have a \glosskey{node} representing a \texttt{while} loop with one \glosskey{child} representing the loop condition and another representing the \glosskey{loop body}.


\noindent \textbf{{\newline}\glosskey{accidental complexity}}: 
The extra (avoidable) complexity introduced by poor design choices. The term is used in contrast with \glosskey{intrinsic complexity}.


\noindent \textbf{{\newline}\glosskey{accumulator}}: 
A variable that collects and/or combines many values.  For example, if a program sums the values in an array by adding them all to a variable called \texttt{result}, then \texttt{result} is the accumulator.


\noindent \textbf{{\newline}\glosskey{actual result (of test)}}: 
The value generated by running code in a test. If this matches the \glosskey{expected result}, the test \glosskey{passes}; if the two are different, the test \glosskey{fails}.


\noindent \textbf{{\newline}\glosskey{Adapter pattern}}: 
A \glosskey{design pattern} that rearranges parameters, provides extra values, or does other work so that one function can be called by another.


\noindent \textbf{{\newline}\glosskey{alias}}: 
A second or subsequent reference to the same object. Aliases are useful, but increase the \glosskey{cognitive load} on readers who have to remember that all these names refer to the same thing.


\noindent \textbf{{\newline}\glosskey{anonymous function}}: 
A function that has not been assigned a name. Anonymous functions are usually quite short, and are usually defined where they are used, e.g., as callbacks. In Python, these are called lambda functions and are created through use of the lambda reserved word.


\noindent \textbf{{\newline}\glosskey{Application Binary Interface} (ABI)}: 
The low-level layout that a piece of software must have to work on a particular kind of machine.


\noindent \textbf{{\newline}\glosskey{Application Programming Interface} (API)}: 
A set of functions provided by a software library or web service that other software can call.


\noindent \textbf{{\newline}\glosskey{argument}}: 
A value passed to a function when it is called.


\noindent \textbf{{\newline}\glosskey{ASCII}}: 
A standard way to represent the characters commonly used in the Western European languages as 7- or 8-bit integers, now largely superceded by \glosskey{Unicode}.


\noindent \textbf{{\newline}\glosskey{assembler}}: 
A \glosskey{compiler} that translates software written in \glosskey{assembly code} into machine instructions.


\noindent \textbf{{\newline}\glosskey{assembly code}}: 
A low-level programming language whose statements correspond closely to the actual \glosskey{instruction set} of a particular kind of processor.


\noindent \textbf{{\newline}\glosskey{assertion}}: 
A \glosskey{Boolean} expression that must be true at a certain point in a program. Assertions may be built into the language (e.g., Python’s \texttt{assert} statement) or provided as functions (as with Node’s \texttt{assert} library).


\noindent \textbf{{\newline}\glosskey{associative array}}: 
See \glosskey{dictionary}.


\noindent \textbf{{\newline}\glosskey{asynchronous}}: 
Not happening at the same time. In programming, an asynchronous operation is one that runs independently of another, or that starts at one time and ends at another.


\noindent \textbf{{\newline}\glosskey{attribute}}: 
A name-value pair associated with an object, used to store metadata about the object such as an array’s dimensions.


\noindent \textbf{{\newline}\glosskey{automatic variable}}: 
A variable that is automatically given a value in a \glosskey{build rule}. For example, Make automatically assigns the name of a rule’s \glosskey{target} to the automatic variable \texttt{\$@}. Automatic variables are frequently used when writing \glosskey{pattern rules}.


\noindent \textbf{{\newline}\glosskey{backward-compatible}}: 
A property of a system that enables interoperability with an older legacy system, or with input designed for such a system.


\noindent \textbf{{\newline}\glosskey{bare object}}: 
An object that isn’t an instance of any particular class.


\noindent \textbf{{\newline}\glosskey{base class}}: 
In \glosskey{object-oriented programming}, a \glosskey{class} from which other classes are derived.


\noindent \textbf{{\newline}\glosskey{binary}}: 
A system which can have one of two possible states, often represented as 0 and 1 or true and false.


\noindent \textbf{{\newline}\glosskey{bit}}: 
A single binary digit (0 or 1).


\noindent \textbf{{\newline}\glosskey{bitwise operation}}: 
An operation that manipulates individual bits in memory. Common bitwise operations include \texttt{and}, \texttt{or}, \texttt{not}, and \texttt{xor}.


\noindent \textbf{{\newline}\glosskey{block comment}}: 
A \glosskey{comment} that spans multiple lines. Block comments may be marked with special start and end symbols, like \texttt{/*} and \texttt{*/} in C and its descendents, or each line may be prefixed with a marker like \texttt{\#}.


\noindent \textbf{{\newline}\glosskey{Boolean}}: 
Relating to a variable or data type that can have either a logical value of true or false. Named for George Boole, a 19th century mathematician.


\noindent \textbf{{\newline}\glosskey{breadth first}}: 
To go through a nested data structure such as a \glosskey{tree} by exploring all of one level, then going on to the next level and so on, or to explore a problem by examining the first step of each possible solution, and then trying the next step for each.


\noindent \textbf{{\newline}\glosskey{breakpoint}}: 
An instruction to a debugger telling it to suspend execution whenever a specific point in the program (such as a particular line) is reached.


\noindent \textbf{{\newline}\glosskey{bug}}: 
A missing or undesirable \glosskey{feature} of a piece of software.


\noindent \textbf{{\newline}\glosskey{build manager}}: 
A program that keeps track of how files depend on one another and runs commands to update any files that are out-of-date. Build managers were invented to \glosskey{compile} only those parts of programs that had changed, but are now often used to implement workflows in which plots depend on results files, which in turn depend on raw data files or configuration files.


\noindent \textbf{{\newline}\glosskey{build recipe}}: 
The part of a \glosskey{build rule} that describes how to update something that has fallen out-of-date.


\noindent \textbf{{\newline}\glosskey{build rule}}: 
A specification for a \glosskey{build manager} that describes how some files depend on others and what to do if those files are out-of-date.


\noindent \textbf{{\newline}\glosskey{build target}}: 
The file(s) that a \glosskey{build rule} will update if they are out-of-date compared to their \glosskey{dependencies}.


\noindent \textbf{{\newline}\glosskey{byte code}}: 
A set of instructions designed to be executed efficiently by an \glosskey{interpreter}.


\noindent \textbf{{\newline}\glosskey{cache}}: 
Something that stores copies of data so that future requests for it can be satisfied more quickly. The CPU in a computer uses a hardware cache to hold recently-accessed values; many programs rely on a software cache to reduce network traffic and latency. Figuring out when something in a cache is out-of-date and should be replaced is one of the \glosskey{two hard problems in computer science}.


\noindent \textbf{{\newline}\glosskey{caching}}: 
To save a copy of some data in a local \glosskey{cache} to make future access faster.


\noindent \textbf{{\newline}\glosskey{call stack}}: 
A data structure that stores information about the active subroutines executed.


\noindent \textbf{{\newline}\glosskey{callback function}}: 
A function A that is passed to another function B so that B can call it at some later point. Callbacks can be used \glosskey{synchronously}, as in generic functions like \texttt{map} that invoke a callback function once for each element in a collection, or \glosskey{asynchronously}, as in a \glosskey{client} that runs a callback when a \glosskey{response} is received in answer to a \glosskey{request}.


\noindent \textbf{{\newline}\glosskey{Cascading Style Sheets} (CSS)}: 
A way to control the appearance of HTML. CSS is typically used to specify fonts, colors, and layout.


\noindent \textbf{{\newline}\glosskey{catch (an exception)}}: 
To handle an error or other unexpected event represented by an \glosskey{exception}.


\noindent \textbf{{\newline}\glosskey{Chain of Responsibility pattern}}: 
A \glosskey{design pattern} in which each \glosskey{object} either handles a request or passes it on to another object.


\noindent \textbf{{\newline}\glosskey{character encoding}}: 
A specification of how characters are stored as bytes. The most commonly-used encoding today is \glosskey{UTF-8}.


\noindent \textbf{{\newline}\glosskey{child (in a tree)}}: 
A \glosskey{node} in a \glosskey{tree} that is below another node (call the \glosskey{parent}).


\noindent \textbf{{\newline}\glosskey{child class}}: 
In \glosskey{object-oriented programming}, a \glosskey{class} derived from another class (called the \glosskey{parent class}).


\noindent \textbf{{\newline}\glosskey{circular dependency}}: 
A situation in which X depends on Y and Y depends on X, either directly or indirectly. If there is a circular dependency, then the \glosskey{dependency graph} is not \glosskey{acyclic}.


\noindent \textbf{{\newline}\glosskey{class}}: 
In \glosskey{object-oriented programming}, a structure that combines data and operations (called \glosskey{methods}). The program then uses a \glosskey{constructor} to create an \glosskey{object} with those properties and methods. Programmers generally put generic or reusable behavior in \glosskey{parent classes}, and more detailed or specific behavior in \glosskey{child classes}.


\noindent \textbf{{\newline}\glosskey{client}}: 
A program such as a web browser that gets data from a \glosskey{server} and displays it to, or interacts with, users. The term is used more generally to refer to any program A that makes requests of another program B. A single program can be both a client and a server.


\noindent \textbf{{\newline}\glosskey{closure}}: 
A set of variables defined in the same \glosskey{scope} whose existence has been preserved after that scope has ended.


\noindent \textbf{{\newline}\glosskey{code coverage (in testing)}}: 
How much of a \glosskey{library} or program is executed when tests run. This is normally reported as a percentage of lines of code.


\noindent \textbf{{\newline}\glosskey{cognitive load}}: 
The amount of working memory needed to accomplish a set of simultaneous tasks.


\noindent \textbf{{\newline}\glosskey{collision}}: 
A situation in which a program tries to store two items in the same location in memory. For example, a collision occurs when a \glosskey{hash function} generates the same \glosskey{hash code} for two different items.


\noindent \textbf{{\newline}\glosskey{column-major storage}}: 
Storing each column of a two-dimensional array as one block of memory so that elements in the same row are far apart.


\noindent \textbf{{\newline}\glosskey{combinatorial explosion}}: 
The exponential growth in the size of a problem or the time required to solve it that arises when all possible combinations of a set of items must be searched.


\noindent \textbf{{\newline}\glosskey{comma-separated values} (CSV)}: 
A text format for tabular data in which each \glosskey{record} is one row and \glosskey{fields} are separated by commas. There are many minor variations, particularly around quoting of \glosskey{strings}.


\noindent \textbf{{\newline}\glosskey{command-line argument}}: 
A filename or control flag given to a command-line program when it is run.


\noindent \textbf{{\newline}\glosskey{command-line interface} (CLI)}: 
A user interface that relies solely on text for commands and output, typically running in a \glosskey{shell}.


\noindent \textbf{{\newline}\glosskey{comment}}: 
Text written in a script that is not treated as code to be run, but rather as text that describes what the code is doing. These are usually short notes, often beginning with a \texttt{\#} (in many programming languages).


\noindent \textbf{{\newline}\glosskey{compile}}: 
To translate textual source into another form. Programs in \glosskey{compiled languages} are translated into machine instructions for a computer to run, and \glosskey{Markdown} is usually translated into \glosskey{HTML} for display.


\noindent \textbf{{\newline}\glosskey{compiled language}}: 
Originally, a language such as C or Fortran that is translated into machine instructions for execution. Languages such as Java are also compiled before execution, but into \glosskey{byte code} instead of machine instructions, while \glosskey{interpreted languages} like JavaScript are compiled to byte code on the fly.


\noindent \textbf{{\newline}\glosskey{compiler}}: 
An application that translates programs written in some languages into machine instructions or \glosskey{byte code}.


\noindent \textbf{{\newline}\glosskey{confirmation bias}}: 
The tendency for someone to look for evidence that they are right rather than searching for reasons why they might be wrong.


\noindent \textbf{{\newline}\glosskey{console}}: 
A computer terminal where a user may enter commands, or a program, such as a shell that simulates such a device.


\noindent \textbf{{\newline}\glosskey{constructor}}: 
A function that creates an \glosskey{object} of a particular \glosskey{class}.


\noindent \textbf{{\newline}\glosskey{Coordinated Universal Time} (UTC)}: 
The standard time against which all others are defined. UTC is the time at longitude 0°, and is not adjusted for daylight savings. \glosskey{Timestamps} are often reported in UTC so that they will be the same no matter what timezone the computer is in.


\noindent \textbf{{\newline}\glosskey{corner case}}: 
Another name for an \glosskey{edge case}.


\noindent \textbf{{\newline}\glosskey{coupling}}: 
The degree of interaction between two \glosskey{classes}, \glosskey{modules}, or other software components. If a system’s components are \glosskey{loosely coupled}, changes to one are unlikely to affect others.  If they are \glosskey{tightly coupled}, then any change requires other changes elsewhere, which complicates maintenance and evolution.


\noindent \textbf{{\newline}\glosskey{cryptographic hash function}}: 
A \glosskey{hash function} that produces an apparently-random value for any input.


\noindent \textbf{{\newline}\glosskey{current working directory}}: 
The \glosskey{folder} or \glosskey{directory} location in which the program operates. Any action taken by the program occurs relative to this directory.


\noindent \textbf{{\newline}\glosskey{cycle (in a graph)}}: 
A set of links in a graph that leads from a node back to itself.


\noindent \textbf{{\newline}\glosskey{data frame}}: 
A two-dimensional data structure for storing tabular data in memory. Rows represent \glosskey{records} and columns represent \glosskey{fields}.


\noindent \textbf{{\newline}\glosskey{data migration}}: 
Moving data from one location or format to another. The term refers to translating data from an old format to a newer one.


\noindent \textbf{{\newline}\glosskey{Decorator pattern}}: 
A \glosskey{design pattern} in which a function adds additional features to another function or a \glosskey{class} after its initial definition. Decorators are a feature of Python and can be implemented in most other languages as well.


\noindent \textbf{{\newline}\glosskey{defensive programming}}: 
A set of programming practices that assumes mistakes will happen and either reports or corrects them, such as inserting \glosskey{assertions} to report situations that are not ever supposed to occur.


\noindent \textbf{{\newline}\glosskey{dependency}}: 
See \glosskey{prerequisite}.


\noindent \textbf{{\newline}\glosskey{dependency graph}}: 
A \glosskey{directed graph} showing how things depend on one another, such as the files to be updated by a \glosskey{build manager}. If the dependency graph is not \glosskey{acyclic}, the dependencies cannot be resolved.


\noindent \textbf{{\newline}\glosskey{deprecation}}: 
To indicate that while a function, method, or class exists, its use is no longer recommended (for example, because it is going to be phased out in a future release).


\noindent \textbf{{\newline}\glosskey{depth-first}}: 
A search algorithm that explores one possibility all the way to its conclusion before moving on to the next.


\noindent \textbf{{\newline}\glosskey{derived class}}: 
In \glosskey{object-oriented programming}, a class that is a direct or indirect extension of a \glosskey{base class}.


\noindent \textbf{{\newline}\glosskey{design by contract}}: 
A style of designing software in which functions specify the \glosskey{pre-conditions} that must be true in order for them to run and the \glosskey{post-conditions} they guarantee will be true when they return. A function can then be replaced by one with weaker pre-conditions (i.e., it accepts a wider set of input) and/or stronger post-conditions (i.e., it produces a smaller range of output) without breaking anything else.


\noindent \textbf{{\newline}\glosskey{design pattern}}: 
A recurring pattern in software design that is specific enough to be worth naming, but not so specific that a single best implementation can be provided by a \glosskey{library}.


\noindent \textbf{{\newline}\glosskey{destructuring assignment}}: 
Unpacking values from data structures and assigning them to multiple variables in a single statement.


\noindent \textbf{{\newline}\glosskey{dictionary}}: 
A data structure that allows items to be looked up by value, sometimes called an \glosskey{associative array}. Dictionaries are often implemented using \glosskey{hash tables}.


\noindent \textbf{{\newline}\glosskey{directed acyclic graph} (DAG)}: 
A \glosskey{directed graph} which does not contain any loops (i.e., it is not possible to reach a \glosskey{node} from itself by following edges).


\noindent \textbf{{\newline}\glosskey{directed graph}}: 
A \glosskey{graph} whose \glosskey{edges} have directions.


\noindent \textbf{{\newline}\glosskey{directory}}: 
A structure in a \glosskey{filesystem} that contains references to other structures, such as files and other directories.


\noindent \textbf{{\newline}\glosskey{disassembler}}: 
A program that translates machine instructions into \glosskey{assembly code} or some other higher-level language.


\noindent \textbf{{\newline}\glosskey{doc comment}}: 
A documentation comment (“doc comment” for short) is a specially-formatted comment containing documentation about a piece of code that is embedded in the code itself.


\noindent \textbf{{\newline}\glosskey{Document Object Model} (DOM)}: 
A standard, in-memory representation of \glosskey{HTML} and \glosskey{XML}. Each \glosskey{element} is stored as a \glosskey{node} in a \glosskey{tree} with a set of named \glosskey{attributes}; contained elements are \glosskey{child nodes}.


\noindent \textbf{{\newline}\glosskey{driver}}: 
A program that runs other programs, or a function that drives all of the other functions in a program.


\noindent \textbf{{\newline}\glosskey{dynamic loading}}: 
To \glosskey{import} a \glosskey{module} into the memory of a program while it is already running. Most \glosskey{interpreted languages} use dynamic loading, and provide tools so that programs can find and load modules dynamically to configure themselves.


\noindent \textbf{{\newline}\glosskey{dynamic lookup}}: 
To find a function or a property of an \glosskey{object} by name while a program is running. For example, instead of getting a specific property of an object using \texttt{obj.name}, a program might use \texttt{obj[someVariable]}, where \texttt{someVariable} could hold \texttt{"name"} or some other property name.


\noindent \textbf{{\newline}\glosskey{dynamic scoping}}: 
To find the value of a variable by looking at what is on the \glosskey{call stack} at the moment the lookup is done. Almost all programming languages use \glosskey{lexical-scoping} instead, since it is more predictable.


\noindent \textbf{{\newline}\glosskey{eager matching}}: 
Matching as much as possible, as early as possible.


\noindent \textbf{{\newline}\glosskey{easy mode}}: 
A term borrowed from gaming meaning to do something with obstacles or difficulties simplified or removed, often for practice purposes.


\noindent \textbf{{\newline}\glosskey{edge}}: 
A connection between two \glosskey{nodes} in a \glosskey{graph}. An edge may have data associated with it, such as a name or distance.


\noindent \textbf{{\newline}\glosskey{edge case}}: 
A problem that only comes up under unusual circumstances or when a system is pushed to its limits; also sometimes called a \glosskey{corner case}. Programs intended for widespread use have to handle edge cases, but doing so can make them much more complicated.


\noindent \textbf{{\newline}\glosskey{element}}: 
A named component in an \glosskey{HTML} or \glosskey{XML} document. Elements are usually written \texttt{<name>}...\texttt{</name>}, where “...” represents the content of the element. Elements often have \glosskey{attributes}.


\noindent \textbf{{\newline}\glosskey{encapsulate}}: 
To store data inside some kind of structure so that it is only accessible through that structure.


\noindent \textbf{{\newline}\glosskey{entry point}}: 
Where a program begins executing.


\noindent \textbf{{\newline}\glosskey{environment}}: 
A structure that stores a set of variable names and the values they refer to.


\noindent \textbf{{\newline}\glosskey{error (in a test)}}: 
Signalled when something goes wrong in a \glosskey{unit test} itself rather than in the system being tested. In this case, we do not know anything about the correctness of the system.


\noindent \textbf{{\newline}\glosskey{error handling}}: 
What a program does to detect and correct for errors. Examples include printing a message and using a default configuration if the user-specified configuration cannot be found.


\noindent \textbf{{\newline}\glosskey{event loop}}: 
A mechanism for managing concurrent activities in a program. Tasks are represented as items in a queue; the event loop repeatedly takes an item from the front of the queue and runs it, adding any other tasks it generates to the back of the queue to run later.


\noindent \textbf{{\newline}\glosskey{exception}}: 
An object that stores information about an error or other unusual event in a program. One part of a program will create and \glosskey{raise an exception} to signal that something unexpected has happened; another part will \glosskey{catch} it.


\noindent \textbf{{\newline}\glosskey{exception handler}}: 
A piece of code that deals with an \glosskey{exception} after it is \glosskey{caught}, e.g., by recording a message, retrying the operation that failed, or performing an alternate operation.


\noindent \textbf{{\newline}\glosskey{expected result (of test)}}: 
The value that a piece of software is supposed to produce when tested in a certain way, or the state in which it is supposed to leave the system.


\noindent \textbf{{\newline}\glosskey{exploratory programming}}: 
A software development methodology in which requirements emerge or change as the software is being written, often in response to results from early runs.


\noindent \textbf{{\newline}\glosskey{export}}: 
To make something visible outside a \glosskey{module} so that other parts of a program can \glosskey{import} it. In most languages a module must export things explicitly in an attempt to avoid \glosskey{name collision}.


\noindent \textbf{{\newline}\glosskey{fail (a test)}}: 
A test fails if the \glosskey{actual result} does not match the \glosskey{expected result}.


\noindent \textbf{{\newline}\glosskey{feature (in software)}}: 
Some aspect of software that was deliberately designed or built. A \glosskey{bug} is an undesired feature.


\noindent \textbf{{\newline}\glosskey{field}}: 
A component of a \glosskey{record} containing a single value. Every record in a database \glosskey{table} has the same fields.


\noindent \textbf{{\newline}\glosskey{filename extension}}: 
The last part of a filename, usually following the ‘.’ symbol. Filename extensions are commonly used to indicate the type of content in the file, though there is no guarantee that this is correct.


\noindent \textbf{{\newline}\glosskey{filesystem}}: 
The part of the \glosskey{operating system} that manages how files are stored and retrieved. Also used to refer to all of those files and \glosskey{directories} or the specific way they are stored (as in “the Unix filesystem”).


\noindent \textbf{{\newline}\glosskey{filter}}: 
As a verb, to choose a set of \glosskey{records} (i.e., rows of a table) based on the values they contain. As a noun, a command-line program that reads lines of text from files or \glosskey{standard input}, performs some operation on them (such as filtering), and writes to a file or \glosskey{stdout}.


\noindent \textbf{{\newline}\glosskey{finite state machine} (FSM)}: 
A theoretical model of computing consisting of a directed graph whose nodes represent the states of the computation and whose arcs show how to move from one state to another. Every \glosskey{regular expression} corresponds to a finite state machine.


\noindent \textbf{{\newline}\glosskey{fixed-width (of strings)}}: 
A set of character strings that have the same length. Databases often used fixed-width strings to make storage and access more efficient; short strings are \glosskey{padded} up to the required length and long strings are truncated.


\noindent \textbf{{\newline}\glosskey{fixture}}: 
The thing on which a test is run, such as the \glosskey{parameters} to the function being tested or the file being processed.


\noindent \textbf{{\newline}\glosskey{fluent interface}}: 
A style of object-oriented programming in which methods return objects so that other methods can immediately be called.


\noindent \textbf{{\newline}\glosskey{folder}}: 
Another term for a \glosskey{directory}.


\noindent \textbf{{\newline}\glosskey{garbage collection}}: 
The process of identifying memory that has been allocated but is no longer in use and reclaiming it to be re-used.


\noindent \textbf{{\newline}\glosskey{generator function}}: 
A function whose state is automatically saved when it returns a value so that execution can be restarted from that point the next time it is called. One example of generator functions use is to produce streams of values that can be processed by \texttt{for} loops.


\noindent \textbf{{\newline}\glosskey{generic function}}: 
A collection of functions with similar purpose, each operating on a different class of data.


\noindent \textbf{{\newline}\glosskey{global variable}}: 
A variable defined outside any particular function or \glosskey{package} namespace, which is therefore visible to all functions.


\noindent \textbf{{\newline}\glosskey{globbing}}: 
To specify a set of filenames using a simplified form of \glosskey{regular expressions}, such as \texttt{*.dat} to mean “all files whose names end in \texttt{.dat}”. The name is derived from “global”.


\noindent \textbf{{\newline}\glosskey{graph}}: 
A plot or a chart that displays data, or a data structure in which \glosskey{nodes} are connected to one another by \glosskey{edges}.


\noindent \textbf{{\newline}\glosskey{greedy algorithm}}: 
An algorithm that consumes as much input as possible, as early as possible.


\noindent \textbf{{\newline}\glosskey{handler}}: 
A \glosskey{callback function} responsible for handling some particular event, such as the user clicking on a button or new data being receiving from a file.


\noindent \textbf{{\newline}\glosskey{hash code}}: 
A value generated by a \glosskey{hash function}. Good hash codes have the same properties as random numbers in order to reduce the frequency of \glosskey{collisions}.


\noindent \textbf{{\newline}\glosskey{hash function}}: 
A function that turns arbitrary data into a bit array, or a \glosskey{key}, of a fixed size. Hash functions are used to determine where data should be stored in a \glosskey{hash table}.


\noindent \textbf{{\newline}\glosskey{hash table}}: 
A data structure that calculates a pseudo-random key (location) for each value passed to it and stores the value in that location. Hash tables enable fast lookup for arbitrary data. This occurs at the cost of extra memory because hash tables must always be larger than the amount of information they need to store, to avoid the possibility of data collisions, when the hash function returns the same key for two different values.


\noindent \textbf{{\newline}\glosskey{header file}}: 
In C and C++, a file that defines constants and function \glosskey{signatures} but does not contain runnable code. Header files tell the including file what is defined in other files so that the compiler can generate correct code.


\noindent \textbf{{\newline}\glosskey{heterogeneous}}: 
Containing mixed data types. For example, an array in Javascript can contain a mix of numbers, character strings, and values of other types.


\noindent \textbf{{\newline}\glosskey{heuristic}}: 
A rule or guideline that isn’t guaranteed to produce the desired result, but usually does.


\noindent \textbf{{\newline}\glosskey{homogeneous}}: 
Containing a single data type. For example, a \glosskey{vector} must be homogeneous: its values must all be numeric, logical, etc.


\noindent \textbf{{\newline}\glosskey{HTTP request}}: 
A message sent from a \glosskey{client} to a \glosskey{server} using the \glosskey{HTTP} \glosskey{protocol} asking for data. A request usually asks for a web page, image, or other data.


\noindent \textbf{{\newline}\glosskey{HTTP response}}: 
A reply sent from a \glosskey{server} to a \glosskey{client} using the \glosskey{HTTP} \glosskey{protocol} in response to a \glosskey{request}. The response usually contains a web page, image, or data.


\noindent \textbf{{\newline}\glosskey{HyperText Markup Language} (HTML)}: 
The standard \glosskey{markup language} used for web pages. HTML is represented in memory using \glosskey{DOM} (Digital Object Model).


\noindent \textbf{{\newline}\glosskey{HyperText Transfer Protocol} (HTTP)}: 
The standard \glosskey{protocol} for data transfer on the World-Wide Web. HTTP defines the format of \glosskey{requests} and \glosskey{responses}, the meanings of standard error codes, and other features.


\noindent \textbf{{\newline}\glosskey{idiomatic}}: 
To use a language in the same way as a fluent or native speaker. Programs are called idiomatic if they use the language the way that proficient programmers use it.


\noindent \textbf{{\newline}\glosskey{immediately-invoked function expression} (IIFE)}: 
A function that is invoked once at the point where it is defined.  IIFEs are typically used to create a \glosskey{scope} to hide some function or variable definitions.


\noindent \textbf{{\newline}\glosskey{immutable}}: 
Data that cannot be changed after being created. Immutable data is easier to think about, particularly if data structures are shared between several tasks, but may result in higher memory requirements.


\noindent \textbf{{\newline}\glosskey{import}}: 
To bring things from a \glosskey{module} into a program for use. In most languages a program can only import things that the module explicitly \glosskey{exports}.


\noindent \textbf{{\newline}\glosskey{index (in a database)}}: 
An auxiliary data structure in a database used to speed up search for some entries. An index increases memory and disk requirements but reduces search time.


\noindent \textbf{{\newline}\glosskey{inner function}}: 
A function defined inside another (outer) function.  Creating and returning inner functions is a way to create \glosskey{closures}.


\noindent \textbf{{\newline}\glosskey{instance}}: 
An \glosskey{object} of a particular \glosskey{class}.


\noindent \textbf{{\newline}\glosskey{instruction pointer}}: 
A special \glosskey{register} in a processor that stores the address of the next instruction to execute.


\noindent \textbf{{\newline}\glosskey{instruction set}}: 
The basic operations that a particular processor can execute directly.


\noindent \textbf{{\newline}\glosskey{interpreted language}}: 
A high-level language that is not executed directly by the computer, but instead is run by an \glosskey{interpreter} that translates program instructions into machine commands on the fly.


\noindent \textbf{{\newline}\glosskey{interpreter}}: 
A program whose job it is to run programs written in a high-level \glosskey{interpreted language}. Interpreters can run interactively, but may also execute commands saved in a file.


\noindent \textbf{{\newline}\glosskey{intrinsic complexity}}: 
The unavoidable complexity inherent in a problem that any solution must deal with. The term is used in contrast with \glosskey{accidental complexity}.


\noindent \textbf{{\newline}\glosskey{introspection}}: 
Having a program examine itself as it is running; common examples are to determine the specific class of a generic object or to get the fields of an object when they are not known in advance.


\noindent \textbf{{\newline}\glosskey{ISO date format}}: 
An international for formatting dates. While the full standard is complex, the most common form is \texttt{YYYY-MM-DD}, i.e., a four-digit year, a two-digit month, and a two-digit day, separated by hyphens.


\noindent \textbf{{\newline}\glosskey{Iterator pattern}}: 
A \glosskey{design pattern} in which a temporary \glosskey{object} or \glosskey{generator function} produces each value from a collection in turn for processing. This pattern hides the differences between different kinds of data structures so that everything can be processed using loops.


\noindent \textbf{{\newline}\glosskey{JavaScript Object Notation} (JSON)}: 
A way to represent data by combining basic values like numbers and character strings in \glosskey{lists} and \glosskey{key/value} structures. The acronym stands for “JavaScript Object Notation”; unlike better-defined standards like \glosskey{XML}, it is unencumbered by a syntax for comments or ways to define a \glosskey{schema}.


\noindent \textbf{{\newline}\glosskey{join}}: 
An operation that combines two \glosskey{tables}, typically by matching \glosskey{keys} from one with keys from another.


\noindent \textbf{{\newline}\glosskey{key}}: 
A \glosskey{field} or combination of fields whose value(s) uniquely identify a \glosskey{record} within a \glosskey{table} or dataset. Keys are often used to select specific records and in \glosskey{joins}.


\noindent \textbf{{\newline}\glosskey{label (address in memory)}}: 
A human-readable name given to a particular location in memory when writing programs in \glosskey{assembly code}.


\noindent \textbf{{\newline}\glosskey{layout engine}}: 
A piece of software that decides where to place text, images, and other elements on a page.


\noindent \textbf{{\newline}\glosskey{lazy matching}}: 
Matching as little as possible while still finding a valid match.


\noindent \textbf{{\newline}\glosskey{Least Recently Used cache} (LRU cache)}: 
A \glosskey{cache} that discards items that have not been used recently in order to limit memory requirements.


\noindent \textbf{{\newline}\glosskey{lexical scoping}}: 
To look up the value associated with a name according to the textual structure of a program. Most programming languages use lexical scoping instead of \glosskey{dynamic scoping} because the latter is less predictable.


\noindent \textbf{{\newline}\glosskey{library}}: 
An installable collection of software, also often called a \glosskey{module} or \glosskey{package}.


\noindent \textbf{{\newline}\glosskey{lifecycle}}: 
The steps that something is allowed or required to go through. The lifecycle of an \glosskey{object} runs from its \glosskey{construction} through the operations it can or must perform before it is destroyed.


\noindent \textbf{{\newline}\glosskey{line comment}}: 
A \glosskey{comment} in a program that spans part of a single line, as opposed to a \glosskey{block comment} that may span multiple lines.


\noindent \textbf{{\newline}\glosskey{link (a program)}}: 
To combine separately \glosskey{compiled} modules into a single runnable program.


\noindent \textbf{{\newline}\glosskey{linter}}: 
A program that checks for common problems in software, such as violations of indentation rules or variable naming conventions. The name comes from the first tool of its kind, called \texttt{lint}.


\noindent \textbf{{\newline}\glosskey{Liskov Substitution Principle}}: 
A design rule stating that it should be possible to replace objects in a program with objects of derived classes without breaking the program. \glosskey{Design by contract} is intended to enforce this rule.


\noindent \textbf{{\newline}\glosskey{list}}: 
A \glosskey{vector} that can contain values of many different (\glosskey{heterogeneous}) types.


\noindent \textbf{{\newline}\glosskey{literal}}: 
A representation of a fixed value in a program, such as the digits \texttt{123} for the number 123 or the characters \texttt{"abc"} for the string containing those three letters.


\noindent \textbf{{\newline}\glosskey{literate programming}}: 
A programming paradigm that mixes prose and code so that explanations and instructions are side by side.


\noindent \textbf{{\newline}\glosskey{loader}}: 
A function whose job is to read files containing runnable code into memory and make that code available to the calling program.


\noindent \textbf{{\newline}\glosskey{local variable}}: 
A variable defined inside a function which is only visible within that function.


\noindent \textbf{{\newline}\glosskey{log message}}: 
A status report or error message written to a file as a program runs.


\noindent \textbf{{\newline}\glosskey{loop body}}: 
The statement or statements executed by a loop.


\noindent \textbf{{\newline}\glosskey{loosely coupled}}: 
Components in a software system are said to be loosely coupled if they are relatively independent of one another, i.e., if any one of them can be changed or replaced without others having to be altered as well.


\noindent \textbf{{\newline}\glosskey{macro}}: 
Originally short for “macro-instruction”, an instruction to translate some of the text into a program into other text before using it.


\noindent \textbf{{\newline}\glosskey{Makefile}}: 
A configuration file for the original \glosskey{build manager}.


\noindent \textbf{{\newline}\glosskey{manifest}}: 
A list that specifies the precise versions of a complete set of libraries or other software components.


\noindent \textbf{{\newline}\glosskey{Markdown}}: 
A \glosskey{markup language} with a simple syntax intended as a replacement for \glosskey{HTML}.


\noindent \textbf{{\newline}\glosskey{markup language}}: 
A set of rules for annotating text to define its meaning or how it should be displayed. The markup is usually not displayed, but instead controls how the underlying text is interpreted or shown. \glosskey{Markdown} and \glosskey{HTML} are widely-used markup languages for web pages.


\noindent \textbf{{\newline}\glosskey{method}}: 
An implementation of a \glosskey{generic function} that handles objects of a specific class.


\noindent \textbf{{\newline}\glosskey{method chaining}}: 
A style of object-oriented programming in which an object’s methods return that object as their result so that another method can immediately be called, as in \texttt{obj.a().b().c()}.


\noindent \textbf{{\newline}\glosskey{mock object}}: 
A simplified replacement for part of a program whose behavior is easy to control and predict. Mock objects are used in \glosskey{unit tests} to simulate databases, web services, and other complex systems.


\noindent \textbf{{\newline}\glosskey{module}}: 
A reusable software \glosskey{package}, also often called a \glosskey{library}.


\noindent \textbf{{\newline}\glosskey{module bundler}}: 
A program that finds all the dependencies of a set of source files and combines them into a single loadable file.


\noindent \textbf{{\newline}\glosskey{multi-threaded}}: 
Capable of performing several operations simultaneously. Multi-threaded programs are usually more efficient than \glosskey{single-threaded} ones, but also harder to understand and debug.


\noindent \textbf{{\newline}\glosskey{name collision}}: 
The ambiguity that arises when two or more things in a program that have the same name are active at the same time. Most languages use \glosskey{namespaces} to prevent such collisions.


\noindent \textbf{{\newline}\glosskey{namespace}}: 
A collection of names in a program that exists in isolation from other namespaces. Each function, \glosskey{object}, \glosskey{class}, or \glosskey{module} in a program typically has its own namespace so that references to “X” in one part of a program do not accidentally refer to something called “X” in another part of the program. Scope is a distinct, but related, concept.


\noindent \textbf{{\newline}\glosskey{nested function}}: 
A function that is defined inside another function.


\noindent \textbf{{\newline}\glosskey{node}}: 
An element of a \glosskey{graph} that is connected to other nodes by \glosskey{edges}. Nodes typically have data associated with them, such as names or weights.


\noindent \textbf{{\newline}\glosskey{non-blocking execution}}: 
To allow a program to continue running while an operation is in progress. For example, many systems support non-blocking execution for file I/O so that the program can continue doing work while it waits for data to be read from or written to the \glosskey{filesystem} (which is typically much slower than the CPU).


\noindent \textbf{{\newline}\glosskey{object}}: 
In \glosskey{object-oriented programming}, a structure that contains the data for a specific instance of a \glosskey{class}. The operations the object is capable of are defined by the class’s \glosskey{methods}.


\noindent \textbf{{\newline}\glosskey{object-oriented programming} (OOP)}: 
A style of programming in which functions and data are bound together in \glosskey{objects} that only interact with each other through well-defined interfaces.


\noindent \textbf{{\newline}\glosskey{off-by-one error}}: 
A common error in programming in which the program refers to element \texttt{i} of a structure when it should refer to element \texttt{i-1} or \texttt{i+1}, or processes \texttt{N} elements when it should process \texttt{N-1} or \texttt{N+1}.


\noindent \textbf{{\newline}\glosskey{op code}}: 
The numerical code for a particular instruction that a processor can execute.


\noindent \textbf{{\newline}\glosskey{Open-Closed Principle}}: 
A design rule stating that software should be open for extension but closed for modification, i.e., it should be possible to extend functionality without having to rewrite existing code.


\noindent \textbf{{\newline}\glosskey{operating system}}: 
A program that provides a standard interface to whatever hardware it is running on. Theoretically, any program that only interacts with the operating system should run on any computer that operating system runs on.


\noindent \textbf{{\newline}\glosskey{package}}: 
A collection of code, data, and documentation that can be distributed and re-used. Also referred to in some languages as a \glosskey{library} or \glosskey{module}.


\noindent \textbf{{\newline}\glosskey{pad (a string)}}: 
To add extra characters to a string to make it a required length.


\noindent \textbf{{\newline}\glosskey{parameter}}: 
A variable specified in a function definition that is assigned a value when the function is called.


\noindent \textbf{{\newline}\glosskey{parent (in a tree)}}: 
A \glosskey{node} in a \glosskey{tree} that is above another node (called a \glosskey{child}). Every node in a tree except the \glosskey{root node} has a single parent.


\noindent \textbf{{\newline}\glosskey{parent class}}: 
In \glosskey{object-oriented programming}, the \glosskey{class} from which a sub class (called the \glosskey{child class}) is derived.


\noindent \textbf{{\newline}\glosskey{parser}}: 
A piece of software that translates a textual representation of something into a data structure. For example, a \glosskey{YAML} parser reads indented text and produces nested lists and objects.


\noindent \textbf{{\newline}\glosskey{pass (a test)}}: 
A test passes if the \glosskey{actual result} matches the \glosskey{expected result}.


\noindent \textbf{{\newline}\glosskey{patch}}: 
A single file containing a set of changes to a set of files, separated by markers that indicate where each individual change should be applied.


\noindent \textbf{{\newline}\glosskey{path (in filesystem)}}: 
A \glosskey{string} that specifies a location in a \glosskey{filesystem}. In Unix, the \glosskey{directories} in a path are joined using \texttt{/}.


\noindent \textbf{{\newline}\glosskey{pattern rule}}: 
A generic \glosskey{build rule} that describes how to update any file whose name matches a pattern. Pattern rules often use \glosskey{automatic variables} to represent the actual filenames.


\noindent \textbf{{\newline}\glosskey{pipe}}: 
To use the output of one computation as the input for the next, or the connection between the two computations responsible for the data transfer. Pipes were popularized by the \glosskey{Unix shell}, and are now used in many different programming languages and systems.


\noindent \textbf{{\newline}\glosskey{pipe (in the Unix shell)}}: 
The \texttt{|} used to make the output of one command the input of the next.


\noindent \textbf{{\newline}\glosskey{plugin architecture}}: 
A style of application design in which the main program loads and runs small independent modules that do the bulk of the work.


\noindent \textbf{{\newline}\glosskey{polymorphism}}: 
Having many different implementations of the same interface. If a set of functions or objects are polymorphic, they can be called interchangeably.


\noindent \textbf{{\newline}\glosskey{post-condition}}: 
Something that is guaranteed to be true after a function runs successfully. Post-conditions are often expressed as \glosskey{assertions} that are guaranteed to be be true of a function’s results.


\noindent \textbf{{\newline}\glosskey{pre-condition}}: 
Something that must be true before a function runs in order for it to work correctly. Pre-conditions are often expressed as as \glosskey{assertions} that must be true of a function’s inputs in order for it to run successfully.


\noindent \textbf{{\newline}\glosskey{precedence}}: 
The priority of an operation. For example, multiplication has a higher precedence than addition, so \texttt{a+b*c} is read as “the sum of \texttt{a} with the product of \texttt{b} and \texttt{c}”.


\noindent \textbf{{\newline}\glosskey{prerequisite}}: 
Something that a \glosskey{build target} depends on.


\noindent \textbf{{\newline}\glosskey{process}}: 
An \glosskey{operating system}‘s representation of a running program. A process typically has some memory, the identity of the user who is running it, and a set of connections to open files.


\noindent \textbf{{\newline}\glosskey{promise}}: 
A way to represent the result of a delayed or \glosskey{asynchronous} computation. A promise is a placeholder for a value that will eventually be computed; any attempt to read the value before it is available blocks, while any such attempt after the computation finishes acts like a normal read.


\noindent \textbf{{\newline}\glosskey{promisification}}: 
In JavaScript, the act of wrapping a callback function in a \glosskey{promise} for uniform asynchronous execution.


\noindent \textbf{{\newline}\glosskey{protocol}}: 
Any standard specifying how two pieces of software interact. A network protocol such as \glosskey{HTTP} defines the messages that \glosskey{clients} and \glosskey{servers} exchange on the World-Wide Web; \glosskey{object-oriented} programs often define protocols for interactions between \glosskey{objects} of different \glosskey{classes}.


\noindent \textbf{{\newline}\glosskey{prune}}: 
To remove branches and nodes from a tree, or to rule out partially-complete solutions when searching for an overall solution in order to reduce work.


\noindent \textbf{{\newline}\glosskey{pseudo-random number}}: 
A value generated in a repeatable way that resembles the true randomness of the universe well enough to fool observers.


\noindent \textbf{{\newline}\glosskey{pseudo-random number generator} (PRNG)}: 
A function that can generate \glosskey{pseudo-random numbers}.


\noindent \textbf{{\newline}\glosskey{query selector}}: 
A pattern that specifies a set of \glosskey{DOM} nodes.  Query selectors are used in \glosskey{CSS} to specify the elements that rules apply to, or by JavaScript programs to manipulate web pages.


\noindent \textbf{{\newline}\glosskey{query string}}: 
The portion of a \glosskey{URL} after the question mark \texttt{?} that specifies extra parameters for the \glosskey{HTTP request} as name-value pairs.


\noindent \textbf{{\newline}\glosskey{race condition}}: 
A situation in which a result depends on the order in which two or more concurrent operations are carried out.


\noindent \textbf{{\newline}\glosskey{raise (an exception)}}: 
To signal that something unexpected or unusual has happened in a program by creating an \glosskey{exception} and handing it to the \glosskey{error-handling} system, which then tries to find a point in the program that will \glosskey{catch} it.


\noindent \textbf{{\newline}\glosskey{read-eval-print loop} (REPL)}: 
An interactive program that reads a command typed in by a user, executes it, prints the result, and then waits patiently for the next command. REPLs are often used to explore new ideas, or for debugging.


\noindent \textbf{{\newline}\glosskey{record}}: 
A group of related values that are stored together. A record may be represented as a \glosskey{tuple} or as a row in a \glosskey{table}; in the latter case, every record in the table has the same \glosskey{fields}.


\noindent \textbf{{\newline}\glosskey{register}}: 
A small piece of memory (typically one \glosskey{word} long) built into a processor that operations can refer to directly.


\noindent \textbf{{\newline}\glosskey{regular expression}}: 
A pattern for matching text, written as text itself. Regular expressions are sometimes called “regexp”, “regex”, or “RE”, and are powerful tools for working with text.


\noindent \textbf{{\newline}\glosskey{relational database}}: 
A database that organizes information into \glosskey{tables}, each of which has a fixed set of named \glosskey{fields} (shown as columns) and a variable number of \glosskey{records} (shown as rows).


\noindent \textbf{{\newline}\glosskey{relative error}}: 
The absolute value of the difference between the actual and correct value divided by the correct value. For example, if the actual value is 9 and the correct value is 10, the relative error is 0.1. Relative error is usually more useful than \glosskey{absolute error}.


\noindent \textbf{{\newline}\glosskey{relative path}}: 
A path that is interpreted relative to some other location, such as the \glosskey{current working directory}. A relative path is the equivalent of giving directions using terms like “straight” and “left”.


\noindent \textbf{{\newline}\glosskey{root (in a tree)}}: 
The \glosskey{node} in a \glosskey{tree} of which all other nodes are direct or indirect \glosskey{children}, or equivalently the only node in the tree that has no \glosskey{parent}.


\noindent \textbf{{\newline}\glosskey{row-major storage}}: 
Storing each row of a two-dimensional array as one block of memory so that elements in the same column are far apart.


\noindent \textbf{{\newline}\glosskey{runnable documentation}}: 
Statements about code that can be executed to check their correctness, such as \glosskey{assertions} or \glosskey{type declarations}.


\noindent \textbf{{\newline}\glosskey{sandbox}}: 
A testing environment that is separate from the production system, or an environment that is only allowed to perform a restricted set of operations for security reasons.


\noindent \textbf{{\newline}\glosskey{SAT solver}}: 
A library or application that determines whether there is an assignment of true and false to a set of \glosskey{Boolean} variables that makes an expression true (i.e., that satisfies the expression).


\noindent \textbf{{\newline}\glosskey{schema}}: 
A specification of the format of a dataset, including the name, format, and content of each \glosskey{table}.


\noindent \textbf{{\newline}\glosskey{scope}}: 
The portion of a program within which a definition can be seen and used.


\noindent \textbf{{\newline}\glosskey{scope creep}}: 
Slow but steady increase in a project’s goals after the project starts.


\noindent \textbf{{\newline}\glosskey{scoring function}}: 
A function that measures or estimates how good a solution to a problem is.


\noindent \textbf{{\newline}\glosskey{search path}}: 
The list of directories that a program searches to find something. For example, the Unix \glosskey{shell} uses the search path stored in the \texttt{PATH} variable when trying to find a program whose name it has been given.


\noindent \textbf{{\newline}\glosskey{seed}}: 
A value used to initialize a \glosskey{pseudo-random number generator}.


\noindent \textbf{{\newline}\glosskey{semantic versioning}}: 
A standard for identifying software releases. In the version identifier \texttt{major.minor.patch}, \texttt{major} changes when a new version of software is incompatible with old versions, \texttt{minor} changes when new features are added to an existing version, and \texttt{patch} changes when small \glosskey{bugs} are fixed.


\noindent \textbf{{\newline}\glosskey{server}}: 
Typically, a program such as a database manager or web server that provides data to a \glosskey{client} upon request.


\noindent \textbf{{\newline}\glosskey{SHA-1 hash}}: 
A \glosskey{cryptographic hash function} that produces a 160-bit output.


\noindent \textbf{{\newline}\glosskey{shell}}: 
A \glosskey{command-line interface} that allows a user to interact with the \glosskey{operating system}, such as Bash (for Unix and MacOS) or PowerShell (for Windows).


\noindent \textbf{{\newline}\glosskey{shell variable}}: 
A variable set and used in the \glosskey{Unix shell}. Commonly-used shell variables include \texttt{HOME} (the user’s home directory) and \texttt{PATH} (their \glosskey{search path}).


\noindent \textbf{{\newline}\glosskey{side effect}}: 
A change made by a function while it runs that is visible after the function finishes, such as modifying a \glosskey{global variable} or writing to a file. Side effects make programs harder for people to understand, since the effects are not necessarily clear at the point in the program where the function is called.


\noindent \textbf{{\newline}\glosskey{signature}}: 
The set of parameters (with types or meaning) that characterize the calling interface of a function or set of functions. Two functions with the same signature can be called interchangeably.


\noindent \textbf{{\newline}\glosskey{single-threaded}}: 
A model of program execution in which only one thing can happen at a time. Single-threaded execution is easier for people to understand, but less efficient than \glosskey{multi-threaded} execution.


\noindent \textbf{{\newline}\glosskey{singleton}}: 
A set with only one element, or a \glosskey{class} with only one \glosskey{instance}.


\noindent \textbf{{\newline}\glosskey{Singleton pattern}}: 
A \glosskey{design pattern} that creates a \glosskey{singleton} \glosskey{object} to manage some resource or service, such as a database or \glosskey{cache}. In \glosskey{object-oriented programming}, the pattern is usually implemented by hiding the \glosskey{constructor} of the \glosskey{class} in some way so that it can only be called once.


\noindent \textbf{{\newline}\glosskey{slug}}: 
An abbreviated portion of a page’s URL that uniquely identifies it. In the example \texttt{https://www.mysite.com/category/post-name}, the slug is \texttt{post-name}.


\noindent \textbf{{\newline}\glosskey{source map}}: 
A table used to translate a piece of code back to the lines in the original source.


\noindent \textbf{{\newline}\glosskey{sparse matrix}}: 
A matrix in which most of the values are zero (or some other value). Rather than storing many copies of the same values, programs will often use a special data structure that only stores the “interesting” values.


\noindent \textbf{{\newline}\glosskey{SQL}}: 
The language used for writing queries for a \glosskey{relational database}. The term was originally an acronym for Structured Query Language.


\noindent \textbf{{\newline}\glosskey{stack frame}}: 
A section of the \glosskey{call stack} that records details of a single call to a specific function.


\noindent \textbf{{\newline}\glosskey{stale (in build)}}: 
To be out-of-date compared to a \glosskey{prerequisite}. A \glosskey{build manager}‘s job is to find and update things that are stale.


\noindent \textbf{{\newline}\glosskey{standard error}}: 
A predefined communication channel for a \glosskey{process} typically used to report errors.


\noindent \textbf{{\newline}\glosskey{standard input}}: 
A predefined communication channel for a \glosskey{process}, typically used to read input from the keyboard or from the previous process in a \glosskey{pipe}.


\noindent \textbf{{\newline}\glosskey{standard output}}: 
A predefined communication channel for a \glosskey{process}, typically used to send output to the screen or to the next process in a \glosskey{pipe}.


\noindent \textbf{{\newline}\glosskey{static site generator}}: 
A software tool that creates HTML pages from templates and content.


\noindent \textbf{{\newline}\glosskey{stream}}: 
A sequential flow of data, such as the \glosskey{bits} arriving across a network connection or the bytes read from a file.


\noindent \textbf{{\newline}\glosskey{streaming API}}: 
An \glosskey{API} that processes data in chunks rather than needing to have all of it in memory at once. Streaming APIs usually require \glosskey{handlers} for events such as “start of data”, “next block”, and “end of data”.


\noindent \textbf{{\newline}\glosskey{string}}: 
A block of text in a program. The term is short for “character string”.


\noindent \textbf{{\newline}\glosskey{string interpolation}}: 
The process of inserting text corresponding to specified values into a \glosskey{string}, usually to make output human-readable.


\noindent \textbf{{\newline}\glosskey{synchronous}}: 
To happen at the same time. In programming, synchronous operations are ones that have to run simultaneously, or complete at the same time.


\noindent \textbf{{\newline}\glosskey{tab completion}}: 
A technique implemented by most \glosskey{REPLs}, \glosskey{shells}, and programming editors that completes a command, variable name, filename, or other text when the TAB key is pressed.


\noindent \textbf{{\newline}\glosskey{table}}: 
A set of \glosskey{records} in a \glosskey{relational database} or \glosskey{data frame}.


\noindent \textbf{{\newline}\glosskey{tagged data}}: 
A technique for storing data in a two-part structure, where one part identifies the type and the other part stores the bits making up the value.


\noindent \textbf{{\newline}\glosskey{Template Method pattern}}: 
A \glosskey{design pattern} in which a \glosskey{parent class} defines an overall sequence of operations by calling \glosskey{abstract methods} that \glosskey{child classes} must then implement. Each child class then behaves in the same general way, but implements the steps differently.


\noindent \textbf{{\newline}\glosskey{test harness}}: 
A program written to test some other program or set of functions, typically to measure their performance.


\noindent \textbf{{\newline}\glosskey{test runner}}: 
A program that finds and runs software tests and reports their results.


\noindent \textbf{{\newline}\glosskey{test subject}}: 
The thing being tested, sometimes also called the system under test (SUT).


\noindent \textbf{{\newline}\glosskey{test-driven development} (TDD)}: 
A programming practice in which tests are written before a new feature is added or a \glosskey{bug} is fixed in order to clarify the goal.


\noindent \textbf{{\newline}\glosskey{throw (exception)}}: 
Another term for \glosskey{raising} an exception.


\noindent \textbf{{\newline}\glosskey{tightly coupled}}: 
Components in a software system are said to be tightly coupled if they depend on each other’s internals, so that if one is altered then others have to be altered as well.


\noindent \textbf{{\newline}\glosskey{Time of check/time of use} (ToCToU)}: 
A \glosskey{race condition} in which a process checks the state of something and then operates on it, but some other process might alter that state between the check and the operation.


\noindent \textbf{{\newline}\glosskey{timestamp}}: 
A digital identifier showing the time at which something was created or accessed. Timestamps should use \glosskey{ISO date format} for portability.


\noindent \textbf{{\newline}\glosskey{token}}: 
An indivisible unit of text for a parser, such as a variable name or a number. Exactly what constitutes a token depends on the language.


\noindent \textbf{{\newline}\glosskey{topological order}}: 
Any ordering of the \glosskey{nodes} in a \glosskey{graph} that respects the direction of its \glosskey{edges}, i.e., if there is an edge from node A to node B, A comes before B in the ordering. There may be many topological orderings of a particular graph.


\noindent \textbf{{\newline}\glosskey{transitive closure}}: 
The set of all \glosskey{nodes} in a \glosskey{graph} that are reachable from a starting node, either directly or indirectly.


\noindent \textbf{{\newline}\glosskey{tree}}: 
A \glosskey{graph} in which every node except the \glosskey{root} has exactly one \glosskey{parent}.


\noindent \textbf{{\newline}\glosskey{tuple}}: 
A value that has a fixed number of parts, such as the three color components of a red-green-blue color specification.


\noindent \textbf{{\newline}\glosskey{Turing Machine}}: 
A theoretical model of computation that manipulates symbols on an infinite tape according to a fixed table of rules. Any computation that can be expressed as an algorithm can be done by a Turing Machine.


\noindent \textbf{{\newline}\glosskey{two hard problems in computer science}}: 
Refers to a quote by Phil Karlton: “There are only two hard problems in computer science—cache invalidation and naming things.” Many variations add a third problem as a joke, such as \glosskey{off-by-one errors}.


\noindent \textbf{{\newline}\glosskey{type declaration}}: 
A statement in a program that a variable or value has a particular data type. Languages like Java require type declarations for all variables; they are optional in TypeScript and Python, and not allowed in pure JavaScript.


\noindent \textbf{{\newline}\glosskey{Unicode}}: 
A standard that defines numeric codes for many thousands of characters and symbols. Unicode does not define how those numbers are stored; that is done by standards like \glosskey{UTF-8}.


\noindent \textbf{{\newline}\glosskey{Uniform Resource Locator} (URL)}: 
A unique address on the World-Wide Web. URLs originally identified web pages, but may also represent datasets or database queries, particularly if they include a \glosskey{query string}.


\noindent \textbf{{\newline}\glosskey{unit test}}: 
A test that exercises one function or feature of a piece of software and produces \glosskey{pass}, \glosskey{fail}, or \glosskey{error}.


\noindent \textbf{{\newline}\glosskey{UTF-8}}: 
A way to store the numeric codes representing \glosskey{Unicode} characters in memory that is \glosskey{backward-compatible} with the older \glosskey{ASCII} standard.


\noindent \textbf{{\newline}\glosskey{vector}}: 
A sequence of values, usually of \glosskey{homogeneous} type.


\noindent \textbf{{\newline}\glosskey{version control system}}: 
A system for managing changes made to software during its development.


\noindent \textbf{{\newline}\glosskey{virtual machine}}: 
A program that pretends to be a computer. This may seem a bit redundant, but VMs are quick to create and start up, and changes made inside the virtual machine are contained within that VM so we can install new \glosskey{packages} or run a completely different operating system without affecting the underlying computer.


\noindent \textbf{{\newline}\glosskey{Visitor pattern}}: 
A \glosskey{design pattern} in which the operation to be done is taken to each element of a data structure in turn. It is usually implemented by having a generator “visitor” that knows how to reach the structure’s elements, which is given a function or method to call for each in turn, and that carries out the specific operation.


\noindent \textbf{{\newline}\glosskey{walk (a tree)}}: 
To visit each \glosskey{node} in a \glosskey{tree} in some order, typically \glosskey{depth-first} or \glosskey{breadth-first}.


\noindent \textbf{{\newline}\glosskey{watchpoint}}: 
An instruction for a debugger telling it to suspect execution whenever the value of a variable (or more generally an expression) changes.


\noindent \textbf{{\newline}\glosskey{well formed}}: 
A piece of text that obeys the rules of a formal grammar is said to be well formed.


\noindent \textbf{{\newline}\glosskey{word (of memory)}}: 
The unit of memory that a particular processor most naturally works with. While a byte is a fixed size (8 bits), a word may be 16, 32, or 64 bits long depending on the processor.


\noindent \textbf{{\newline}\glosskey{XML}}: 
A set of rules for defining \glosskey{HTML}-like tags and using them to format documents (typically data). XML was popular in the early 2000s, but its complexity led many programmers to adopt \glosskey{JSON}, instead.


\noindent \textbf{{\newline}\glosskey{YAML}}: 
Short for “YAML Ain’t Markup Language”, a way to represent nested data using indentation rather than the parentheses and commas of \glosskey{JSON}. YAML is often used in configuration files and to define \glosskey{parameters} for various flavors of \glosskey{Markdown} documents.


\noindent \textbf{{\newline}\glosskey{z-buffering}}: 
A drawing method that keeps track of the depth of what lies “under” each pixel so that it displays whatever is nearest to the observer.




\chapter{Credits}\label{credits}
\markboth{\thechapter\  Credits}{\thechapter\  Credits}

\textbf{Greg Wilson} has worked in industry and academia for 35 years, and is the author, co-author, or editor of several books, including \emph{Beautiful Code}, \emph{The Architecture of Open Source Applications}, \emph{JavaScript for Data Science}, \emph{Teaching Tech Together}, and \emph{Research Software Engineering with Python}. He was the co-founder and first Executive Director of Software Carpentry and received ACM SIGSOFT’s Influential Educator Award in 2020.


\cleardoublepage
\makeatletter
\renewcommand{\tocetcmark}[1]{%
  \@mkboth{{#1}}{{#1}}}
  \makeatother
\printindex

\end{document}

